{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-27T13:26:49.161348Z",
     "iopub.status.busy": "2024-04-27T13:26:49.160788Z",
     "iopub.status.idle": "2024-04-27T13:26:49.170273Z",
     "shell.execute_reply": "2024-04-27T13:26:49.168730Z",
     "shell.execute_reply.started": "2024-04-27T13:26:49.161318Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:26:49.839830Z",
     "iopub.status.busy": "2024-04-27T13:26:49.839431Z",
     "iopub.status.idle": "2024-04-27T13:26:49.848820Z",
     "shell.execute_reply": "2024-04-27T13:26:49.847084Z",
     "shell.execute_reply.started": "2024-04-27T13:26:49.839800Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, encoding='utf-8'):\n",
    "        self.data = pd.read_csv(csv_file, encoding=encoding, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        features = self.data.iloc[idx, 1:].values.astype(float)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:26:51.678194Z",
     "iopub.status.busy": "2024-04-27T13:26:51.676406Z",
     "iopub.status.idle": "2024-04-27T13:27:08.158563Z",
     "shell.execute_reply": "2024-04-27T13:27:08.157034Z",
     "shell.execute_reply.started": "2024-04-27T13:26:51.678149Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:08.160676Z",
     "iopub.status.busy": "2024-04-27T13:27:08.160353Z",
     "iopub.status.idle": "2024-04-27T13:27:08.176192Z",
     "shell.execute_reply": "2024-04-27T13:27:08.174570Z",
     "shell.execute_reply.started": "2024-04-27T13:27:08.160651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:08.178249Z",
     "iopub.status.busy": "2024-04-27T13:27:08.177848Z",
     "iopub.status.idle": "2024-04-27T13:27:08.187369Z",
     "shell.execute_reply": "2024-04-27T13:27:08.185820Z",
     "shell.execute_reply.started": "2024-04-27T13:27:08.178222Z"
    }
   },
   "outputs": [],
   "source": [
    "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changing value of target values as 0 and 1 ( from 0 and 4) because target =2 case not found in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:08.190511Z",
     "iopub.status.busy": "2024-04-27T13:27:08.190055Z",
     "iopub.status.idle": "2024-04-27T13:27:08.226978Z",
     "shell.execute_reply": "2024-04-27T13:27:08.225662Z",
     "shell.execute_reply.started": "2024-04-27T13:27:08.190473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your label column is named 'label' and your DataFrame is named 'data'\n",
    "data['label'] = data['label'].replace(4, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing urls and emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:15.522546Z",
     "iopub.status.busy": "2024-04-27T13:27:15.522267Z",
     "iopub.status.idle": "2024-04-27T13:27:22.814692Z",
     "shell.execute_reply": "2024-04-27T13:27:22.813615Z",
     "shell.execute_reply.started": "2024-04-27T13:27:15.522522Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emails(text):\n",
    "    # Define the pattern for matching emails\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "\n",
    "    # Replace emails with an empty string\n",
    "    clean_text = re.sub(email_pattern, 'email', text)\n",
    "\n",
    "    return clean_text\n",
    "data['text'] = data['text'].apply(remove_emails)\n",
    "def remove_urls(text):\n",
    "    # Define the pattern for matching URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "    # Replace URLs with an empty string\n",
    "    clean_text = re.sub(url_pattern, 'url', text)\n",
    "\n",
    "    return clean_text\n",
    "data['text'] = data['text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elaborating short forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:08.229052Z",
     "iopub.status.busy": "2024-04-27T13:27:08.228634Z",
     "iopub.status.idle": "2024-04-27T13:27:08.252491Z",
     "shell.execute_reply": "2024-04-27T13:27:08.251330Z",
     "shell.execute_reply.started": "2024-04-27T13:27:08.229018Z"
    }
   },
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\": \" dollar \",\n",
    "    \"€\": \" euro \",\n",
    "    \"4ao\": \"for adults only\",\n",
    "    \"a.m\": \"before midday\",\n",
    "    \"a3\": \"anytime anywhere anyplace\",\n",
    "    \"aamof\": \"as a matter of fact\",\n",
    "    \"acct\": \"account\",\n",
    "    \"adih\": \"another day in hell\",\n",
    "    \"afaic\": \"as far as I am concerned\",\n",
    "    \"afaict\": \"as far as I can tell\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"afair\": \"as far as I remember\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"app\": \"application\",\n",
    "    \"approx\": \"approximately\",\n",
    "    \"apps\": \"applications\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"asl\": \"age, sex, location\",\n",
    "    \"atk\": \"at the keyboard\",\n",
    "    \"ave.\": \"avenue\",\n",
    "    \"aymm\": \"are you my mother\",\n",
    "    \"ayor\": \"at your own risk\",\n",
    "    \"b&b\": \"bed and breakfast\",\n",
    "    \"b+b\": \"bed and breakfast\",\n",
    "    \"b.c\": \"before christ\",\n",
    "    \"b2b\": \"business to business\",\n",
    "    \"b2c\": \"business to customer\",\n",
    "    \"b4\": \"before\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"b@u\": \"back at you\",\n",
    "    \"bae\": \"before anyone else\",\n",
    "    \"bak\": \"back at keyboard\",\n",
    "    \"bbbg\": \"bye bye be good\",\n",
    "    \"bbc\": \"british broadcasting corporation\",\n",
    "    \"bbias\": \"be back in a second\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"bbs\": \"be back soon\",\n",
    "    \"be4\": \"before\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"blvd\": \"boulevard\",\n",
    "    \"bout\": \"about\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bros\": \"brothers\",\n",
    "    \"brt\": \"be right there\",\n",
    "    \"bsaaw\": \"big smile and a wink\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"bwl\": \"bursting with laughter\",\n",
    "    \"c/o\": \"care of\",\n",
    "    \"cet\": \"central european time\",\n",
    "    \"cf\": \"compare\",\n",
    "    \"cia\": \"central intelligence agency\",\n",
    "    \"csl\": \"can not stop laughing\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cv\": \"curriculum vitae\",\n",
    "    \"cwot\": \"complete waste of time\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"cyt\": \"see you tomorrow\",\n",
    "    \"dae\": \"does anyone else\",\n",
    "    \"dbmib\": \"do not bother me i am busy\",\n",
    "    \"diy\": \"do it yourself\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"dwh\": \"during work hours\",\n",
    "    \"e123\": \"easy as one two three\",\n",
    "    \"eet\": \"eastern european time\",\n",
    "    \"eg\": \"example\",\n",
    "    \"embm\": \"early morning business meeting\",\n",
    "    \"encl\": \"enclosed\",\n",
    "    \"etc\": \"and so on\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"fawc\": \"for anyone who cares\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"fc\": \"fingers crossed\",\n",
    "    \"fig\": \"figure\",\n",
    "    \"fimh\": \"forever in my heart\",\n",
    "    \"ft.\": \"feet\",\n",
    "    \"ft\": \"featuring\",\n",
    "    \"ftl\": \"for the loss\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"fwiw\": \"for what it is worth\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"g9\": \"genius\",\n",
    "    \"gahoy\": \"get a hold of yourself\",\n",
    "    \"gal\": \"get a life\",\n",
    "    \"gcse\": \"general certificate of secondary education\",\n",
    "    \"gfn\": \"gone for now\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"glhf\": \"good luck have fun\",\n",
    "    \"gmt\": \"greenwich mean time\",\n",
    "    \"gmta\": \"great minds think alike\",\n",
    "    \"gn\": \"good night\",\n",
    "    \"g.o.a.t\": \"greatest of all time\",\n",
    "    \"goat\": \"greatest of all time\",\n",
    "    \"goi\": \"get over it\",\n",
    "    \"gps\": \"global positioning system\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"gratz\": \"congratulations\",\n",
    "    \"gyal\": \"girl\",\n",
    "    \"h&c\": \"hot and cold\",\n",
    "    \"hp\": \"horsepower\",\n",
    "    \"hr\": \"hour\",\n",
    "    \"hrh\": \"his royal highness\",\n",
    "    \"ht\": \"height\",\n",
    "    \"ibrb\": \"i will be right back\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"icq\": \"i seek you\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"idc\": \"i do not care\",\n",
    "    \"idgadf\": \"i do not give a damn fuck\",\n",
    "    \"idgaf\": \"i do not give a fuck\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"ie\": \"that is\",\n",
    "    \"i.e\": \"that is\",\n",
    "    \"ifyp\": \"i feel your pain\",\n",
    "    \"IG\": \"instagram\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"ilu\": \"i love you\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imu\": \"i miss you\",\n",
    "    \"iow\": \"in other words\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"j4f\": \"just for fun\",\n",
    "    \"jic\": \"just in case\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"jsyk\": \"just so you know\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"lb\": \"pound\",\n",
    "    \"lbs\": \"pounds\",\n",
    "    \"ldr\": \"long distance relationship\",\n",
    "    \"lmao\": \"laugh my ass off\",\n",
    "    \"lmfao\": \"laugh my fucking ass off\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"ltd\": \"limited\",\n",
    "    \"ltns\": \"long time no see\",\n",
    "    \"m8\": \"mate\",\n",
    "    \"mf\": \"motherfucker\",\n",
    "    \"mfs\": \"motherfuckers\",\n",
    "    \"mfw\": \"my face when\",\n",
    "    \"mofo\": \"motherfucker\",\n",
    "    \"mph\": \"miles per hour\",\n",
    "    \"mr\": \"mister\",\n",
    "    \"mrw\": \"my reaction when\",\n",
    "    \"ms\": \"miss\",\n",
    "    \"mte\": \"my thoughts exactly\",\n",
    "    \"nagi\": \"not a good idea\",\n",
    "    \"nbc\": \"national broadcasting company\",\n",
    "    \"nbd\": \"not big deal\",\n",
    "    \"nfs\": \"not for sale\",\n",
    "    \"ngl\": \"not going to lie\",\n",
    "    \"nhs\": \"national health service\",\n",
    "    \"nrn\": \"no reply necessary\",\n",
    "    \"nsfl\": \"not safe for life\",\n",
    "    \"nsfw\": \"not safe for work\",\n",
    "    \"nth\": \"nice to have\",\n",
    "    \"nvr\": \"never\",\n",
    "    \"nyc\": \"new york city\",\n",
    "    \"oc\": \"original content\",\n",
    "    \"og\": \"original\",\n",
    "    \"ohp\": \"overhead projector\",\n",
    "    \"oic\": \"oh i see\",\n",
    "    \"omdb\": \"over my dead body\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"p.a\": \"per annum\",\n",
    "    \"p.m\": \"after midday\",\n",
    "    \"pm\": \"prime minister\",\n",
    "    \"poc\": \"people of color\",\n",
    "    \"pov\": \"point of view\",\n",
    "    \"pp\": \"pages\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"prw\": \"parents are watching\",\n",
    "    \"ps\": \"postscript\",\n",
    "    \"pt\": \"point\",\n",
    "    \"ptb\": \"please text back\",\n",
    "    \"pto\": \"please turn over\",\n",
    "    \"qpsa\": \"what happens\",\n",
    "    \"ratchet\": \"rude\",\n",
    "    \"rbtl\": \"read between the lines\",\n",
    "    \"rlrt\": \"real life retweet\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\": \"retweet\",\n",
    "    \"ruok\": \"are you ok\",\n",
    "    \"sfw\": \"safe for work\",\n",
    "    \"sk8\": \"skate\",\n",
    "    \"smh\": \"shake my head\",\n",
    "    \"sq\": \"square\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"ssdd\": \"same stuff different day\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"tbs\": \"tablespoonful\",\n",
    "    \"tbsp\": \"tablespoonful\",\n",
    "    \"tfw\": \"that feeling when\",\n",
    "    \"thks\": \"thank you\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thx\": \"thank you\",\n",
    "    \"tia\": \"thanks in advance\",\n",
    "    \"til\": \"today i learned\",\n",
    "    \"tl;dr\": \"too long i did not read\",\n",
    "    \"tldr\": \"too long i did not read\",\n",
    "    \"tmb\": \"tweet me back\",\n",
    "    \"tntl\": \"trying not to laugh\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"u\": \"you\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"u4e\": \"yours for ever\",\n",
    "    \"utc\": \"coordinated universal time\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"wassup\": \"what is up\",\n",
    "    \"wb\": \"welcome back\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wtpa\": \"where the party at\",\n",
    "    \"wuf\": \"where are you from\",\n",
    "    \"wuzup\": \"what is up\",\n",
    "    \"wywh\": \"wish you were here\",\n",
    "    \"yd\": \"yard\",\n",
    "    \"ygtr\": \"you got that right\",\n",
    "    \"ynk\": \"you never know\",\n",
    "    \"zzz\": \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:22.818003Z",
     "iopub.status.busy": "2024-04-27T13:27:22.817546Z",
     "iopub.status.idle": "2024-04-27T13:27:22.824654Z",
     "shell.execute_reply": "2024-04-27T13:27:22.823217Z",
     "shell.execute_reply.started": "2024-04-27T13:27:22.817965Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_abbreviations(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in abbreviations:\n",
    "            new_words.append(abbreviations[word.lower()])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing emoticons with words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:22.826421Z",
     "iopub.status.busy": "2024-04-27T13:27:22.826026Z",
     "iopub.status.idle": "2024-04-27T13:27:31.040423Z",
     "shell.execute_reply": "2024-04-27T13:27:31.038481Z",
     "shell.execute_reply.started": "2024-04-27T13:27:22.826384Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a dictionary to map emoticons to their corresponding emotions\n",
    "emoticon_emotion_mapping = {\n",
    "    ':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'\n",
    "}\n",
    "\n",
    "# Function to replace emoticons with their corresponding emotions\n",
    "def replace_emoticons_with_emotions(text):\n",
    "    for emoticon, emotion in emoticon_emotion_mapping.items():\n",
    "        text = text.replace(emoticon, emotion)\n",
    "    return text\n",
    "\n",
    "# Assuming your dataset is stored in a pandas DataFrame called 'df' and the text column is named 'text_column'\n",
    "data['text'] = data['text'].apply(replace_emoticons_with_emotions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing username tags and non ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:31.043115Z",
     "iopub.status.busy": "2024-04-27T13:27:31.042609Z",
     "iopub.status.idle": "2024-04-27T13:27:31.049987Z",
     "shell.execute_reply": "2024-04-27T13:27:31.048801Z",
     "shell.execute_reply.started": "2024-04-27T13:27:31.043075Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_username_tags(text):\n",
    "    # Define the pattern to match \"@username\" tags\n",
    "    pattern = r'@\\w+'\n",
    "    # Use re.sub() to replace all occurrences of the pattern with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# data['text'] = data['text'].apply(replace_abbreviations)\n",
    "import unicodedata\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    # Use unicodedata.normalize() to remove non-ASCII characters\n",
    "    cleaned_text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:27:41.450431Z",
     "iopub.status.busy": "2024-04-27T13:27:41.450030Z",
     "iopub.status.idle": "2024-04-27T13:28:06.287214Z",
     "shell.execute_reply": "2024-04-27T13:28:06.285490Z",
     "shell.execute_reply.started": "2024-04-27T13:27:41.450399Z"
    }
   },
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(remove_username_tags)\n",
    "data['text'] = data['text'].apply(remove_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:06.291110Z",
     "iopub.status.busy": "2024-04-27T13:28:06.289384Z",
     "iopub.status.idle": "2024-04-27T13:28:06.835582Z",
     "shell.execute_reply": "2024-04-27T13:28:06.834600Z",
     "shell.execute_reply.started": "2024-04-27T13:28:06.291040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  is upset that he can't update his facebook by ...\n",
      "1      0   i dived many times for the ball. managed to s...\n",
      "2      0    my whole body feels itchy and like its on fire \n",
      "3      0   no, it's not behaving at all. i'm mad. why am...\n",
      "4      0                                not the whole crew \n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is your DataFrame with a 'text' column\n",
    "\n",
    "# Convert text data to lowercase\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Example: Display the first few rows of the DataFrame\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:06.836960Z",
     "iopub.status.busy": "2024-04-27T13:28:06.836682Z",
     "iopub.status.idle": "2024-04-27T13:28:06.842085Z",
     "shell.execute_reply": "2024-04-27T13:28:06.840754Z",
     "shell.execute_reply.started": "2024-04-27T13:28:06.836938Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:06.844570Z",
     "iopub.status.busy": "2024-04-27T13:28:06.844277Z",
     "iopub.status.idle": "2024-04-27T13:28:08.160586Z",
     "shell.execute_reply": "2024-04-27T13:28:08.158296Z",
     "shell.execute_reply.started": "2024-04-27T13:28:06.844545Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['time', 'date', 'query', 'username'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[209], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musername\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['time', 'date', 'query', 'username'] not found in axis\""
     ]
    }
   ],
   "source": [
    "data.drop([\"time\", \"date\", \"query\", \"username\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.161619Z",
     "iopub.status.idle": "2024-04-27T13:28:08.162081Z",
     "shell.execute_reply": "2024-04-27T13:28:08.161881Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.161847Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame and it contains a 'text' column\n",
    "empty_text_rows = data[data['text'].isnull() | (data['text'] == \"\")]\n",
    "\n",
    "if not empty_text_rows.empty:\n",
    "    print(\"Rows where 'text' column is empty or equal to the value in the first row of 'text' column:\")\n",
    "    print(empty_text_rows)\n",
    "else:\n",
    "    print(\"No rows where 'text' column is empty or equal to the value in the first row of 'text' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.162936Z",
     "iopub.status.idle": "2024-04-27T13:28:08.163262Z",
     "shell.execute_reply": "2024-04-27T13:28:08.163125Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.163112Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.165124Z",
     "iopub.status.idle": "2024-04-27T13:28:08.166806Z",
     "shell.execute_reply": "2024-04-27T13:28:08.166636Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.166612Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'].values, data['label'].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.167560Z",
     "iopub.status.idle": "2024-04-27T13:28:08.167877Z",
     "shell.execute_reply": "2024-04-27T13:28:08.167730Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.167717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming X_train is a NumPy array or Pandas Series containing text data\n",
    "documents = [text.split() for text in X_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.170678Z",
     "iopub.status.idle": "2024-04-27T13:28:08.171199Z",
     "shell.execute_reply": "2024-04-27T13:28:08.170993Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.170974Z"
    }
   },
   "outputs": [],
   "source": [
    "test_documents=[text.split() for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.174302Z",
     "iopub.status.idle": "2024-04-27T13:28:08.174884Z",
     "shell.execute_reply": "2024-04-27T13:28:08.174633Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.174608Z"
    }
   },
   "outputs": [],
   "source": [
    "print(test_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.176348Z",
     "iopub.status.idle": "2024-04-27T13:28:08.176824Z",
     "shell.execute_reply": "2024-04-27T13:28:08.176616Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.176595Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(test_documents))\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.178333Z",
     "iopub.status.idle": "2024-04-27T13:28:08.178777Z",
     "shell.execute_reply": "2024-04-27T13:28:08.178581Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.178563Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.word2vec.Word2Vec(vector_size=300, \n",
    "                                            window=7, \n",
    "                                            min_count=10, \n",
    "                                            workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.180182Z",
     "iopub.status.idle": "2024-04-27T13:28:08.180644Z",
     "shell.execute_reply": "2024-04-27T13:28:08.180439Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.180420Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.183339Z",
     "iopub.status.idle": "2024-04-27T13:28:08.183813Z",
     "shell.execute_reply": "2024-04-27T13:28:08.183627Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.183609Z"
    }
   },
   "outputs": [],
   "source": [
    "words = w2v_model.wv.key_to_index.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.184665Z",
     "iopub.status.idle": "2024-04-27T13:28:08.185029Z",
     "shell.execute_reply": "2024-04-27T13:28:08.184874Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.184844Z"
    }
   },
   "outputs": [],
   "source": [
    "# epoch_logger = EpochLogger()\n",
    "\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.187258Z",
     "iopub.status.idle": "2024-04-27T13:28:08.187814Z",
     "shell.execute_reply": "2024-04-27T13:28:08.187574Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.187551Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.189018Z",
     "iopub.status.idle": "2024-04-27T13:28:08.189479Z",
     "shell.execute_reply": "2024-04-27T13:28:08.189280Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.189260Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(w2v_model.wv.key_to_index)\n",
    "embedding_size = w2v_model.wv.vector_size\n",
    "oov_vector = np.random.uniform(low=-0.05, high=0.05, size=(embedding_size,))\n",
    "\n",
    "# Extend the embedding matrix with the OOV vector\n",
    "new_vectors = np.vstack([w2v_model.wv.vectors, oov_vector.reshape(1, -1)])\n",
    "\n",
    "# Update the model with the new embedding matrix\n",
    "w2v_model.wv.vectors = new_vectors\n",
    "\n",
    "# Update the vocabulary to include the OOV vector\n",
    "w2v_model.wv.index_to_key.append('OOV_word')\n",
    "w2v_model.wv.key_to_index['OOV_word'] = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T13:28:08.191049Z",
     "iopub.status.idle": "2024-04-27T13:28:08.191510Z",
     "shell.execute_reply": "2024-04-27T13:28:08.191305Z",
     "shell.execute_reply.started": "2024-04-27T13:28:08.191286Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_word_embeddings(word_tokens, w2v_model, oov_vector):\n",
    "    # Initialize a list to store word embeddings\n",
    "    word_embeddings = []\n",
    "    \n",
    "    # Iterate over each word token in the document\n",
    "    for word in word_tokens:\n",
    "        # Check if the word is in the vocabulary of the Word2Vec model\n",
    "        if word in w2v_model.wv:\n",
    "            # If the word is in the vocabulary, get its embedding from the model\n",
    "            word_embedding = w2v_model.wv[word]\n",
    "        else:\n",
    "            # If the word is not in the vocabulary, use the OOV vector\n",
    "            word_embedding = oov_vector\n",
    "        # Append the word embedding to the list\n",
    "        word_embeddings.append(word_embedding)\n",
    "    \n",
    "    # Convert the list of word embeddings to a NumPy array\n",
    "    word_embeddings_np = np.array(word_embeddings)\n",
    "    \n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    return torch.tensor(word_embeddings_np, dtype=torch.float32)\n",
    "\n",
    "# Assuming 'documents' is a list of documents where each document is a list of word tokens\n",
    "# Assuming 'w2v_model' is your Word2Vec model\n",
    "# Assuming 'oov_vector' is your out-of-vocabulary vector\n",
    "\n",
    "# Create a list to store the document representations\n",
    "document_representations = []\n",
    "\n",
    "# Iterate over each document in the training data\n",
    "for doc in documents:\n",
    "    # Get word embeddings for the document's word tokens\n",
    "    word_embeddings = get_word_embeddings(doc, w2v_model, oov_vector)\n",
    "    \n",
    "    # Check if the list of word embeddings is empty\n",
    "    if len(word_embeddings) == 0:\n",
    "        # If the list is empty, assign the OOV vector as the document representation\n",
    "        doc_representation = torch.tensor(oov_vector, dtype=torch.float32)\n",
    "    else:\n",
    "        # Otherwise, aggregate word embeddings (e.g., by averaging)\n",
    "        doc_representation = torch.mean(word_embeddings, dim=0)  # Assuming average pooling\n",
    "    \n",
    "    # Append the document representation to the list\n",
    "    document_representations.append(doc_representation)\n",
    "\n",
    "# Convert the list of document representations into a tensor\n",
    "# document_tensor = torch.stack(document_representations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:11.070419Z",
     "iopub.status.busy": "2024-04-27T13:28:11.070033Z",
     "iopub.status.idle": "2024-04-27T13:28:12.648601Z",
     "shell.execute_reply": "2024-04-27T13:28:12.647165Z",
     "shell.execute_reply.started": "2024-04-27T13:28:11.070390Z"
    }
   },
   "outputs": [],
   "source": [
    "document_tensor = torch.stack(document_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:12.650206Z",
     "iopub.status.busy": "2024-04-27T13:28:12.649945Z",
     "iopub.status.idle": "2024-04-27T13:28:12.658492Z",
     "shell.execute_reply": "2024-04-27T13:28:12.656972Z",
     "shell.execute_reply.started": "2024-04-27T13:28:12.650182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279999\n"
     ]
    }
   ],
   "source": [
    "print(document_tensor.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:14.085438Z",
     "iopub.status.busy": "2024-04-27T13:28:14.085033Z",
     "iopub.status.idle": "2024-04-27T13:28:14.090779Z",
     "shell.execute_reply": "2024-04-27T13:28:14.089968Z",
     "shell.execute_reply.started": "2024-04-27T13:28:14.085408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(documents[405])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:17.173317Z",
     "iopub.status.busy": "2024-04-27T13:28:17.172645Z",
     "iopub.status.idle": "2024-04-27T13:28:17.182660Z",
     "shell.execute_reply": "2024-04-27T13:28:17.181480Z",
     "shell.execute_reply.started": "2024-04-27T13:28:17.173282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0274, -0.0101, -0.0089,  0.0445, -0.0500,  0.0313, -0.0157, -0.0246,\n",
      "        -0.0367, -0.0237, -0.0108,  0.0216, -0.0158,  0.0459, -0.0036,  0.0415,\n",
      "        -0.0329,  0.0345, -0.0459, -0.0309, -0.0244,  0.0081,  0.0206,  0.0230,\n",
      "         0.0486, -0.0483,  0.0408, -0.0174,  0.0208, -0.0392, -0.0042, -0.0044,\n",
      "         0.0445, -0.0359,  0.0015,  0.0316,  0.0447,  0.0487, -0.0235,  0.0077,\n",
      "         0.0280,  0.0334,  0.0249,  0.0108, -0.0106,  0.0336, -0.0173, -0.0174,\n",
      "        -0.0145, -0.0249, -0.0070, -0.0452, -0.0039, -0.0214, -0.0370, -0.0397,\n",
      "        -0.0013, -0.0487, -0.0361, -0.0294,  0.0365, -0.0353, -0.0397, -0.0117,\n",
      "        -0.0059, -0.0416, -0.0156,  0.0015, -0.0213,  0.0160, -0.0355,  0.0464,\n",
      "        -0.0309, -0.0272, -0.0055,  0.0286,  0.0014,  0.0355,  0.0261, -0.0250,\n",
      "         0.0256, -0.0353, -0.0116,  0.0165,  0.0219,  0.0189,  0.0448, -0.0064,\n",
      "        -0.0091, -0.0234,  0.0035, -0.0456,  0.0071, -0.0215, -0.0395,  0.0393,\n",
      "         0.0196,  0.0169, -0.0451, -0.0440, -0.0337,  0.0441, -0.0041, -0.0475,\n",
      "        -0.0337, -0.0227, -0.0126, -0.0082,  0.0088,  0.0428, -0.0324, -0.0184,\n",
      "        -0.0301,  0.0204, -0.0281, -0.0292, -0.0472,  0.0214, -0.0356, -0.0160,\n",
      "        -0.0400,  0.0122, -0.0033,  0.0094,  0.0031, -0.0158, -0.0485, -0.0176,\n",
      "         0.0454,  0.0002,  0.0368, -0.0279, -0.0386,  0.0232, -0.0288,  0.0301,\n",
      "         0.0488,  0.0188, -0.0353, -0.0415,  0.0291, -0.0027,  0.0226, -0.0224,\n",
      "        -0.0040, -0.0148,  0.0322,  0.0463,  0.0175,  0.0151, -0.0456, -0.0198,\n",
      "        -0.0402,  0.0194, -0.0368,  0.0060, -0.0474, -0.0034, -0.0128, -0.0185,\n",
      "         0.0420, -0.0004,  0.0174,  0.0443,  0.0343,  0.0448,  0.0481, -0.0077,\n",
      "         0.0159,  0.0357, -0.0231, -0.0078, -0.0269, -0.0256, -0.0174, -0.0328,\n",
      "         0.0136,  0.0458, -0.0463,  0.0333, -0.0246, -0.0341, -0.0004, -0.0287,\n",
      "        -0.0497, -0.0467,  0.0112, -0.0431, -0.0221,  0.0436,  0.0323,  0.0017,\n",
      "        -0.0282, -0.0423,  0.0351, -0.0028,  0.0123,  0.0247,  0.0307, -0.0234,\n",
      "         0.0077,  0.0370, -0.0316,  0.0221,  0.0185,  0.0163,  0.0040,  0.0146,\n",
      "         0.0112, -0.0061,  0.0203,  0.0220,  0.0182, -0.0118, -0.0331, -0.0403,\n",
      "        -0.0144, -0.0389, -0.0102, -0.0006,  0.0294,  0.0326, -0.0012,  0.0369,\n",
      "        -0.0392,  0.0372,  0.0272, -0.0046,  0.0331, -0.0026, -0.0252, -0.0464,\n",
      "        -0.0490,  0.0095, -0.0400, -0.0316, -0.0183, -0.0109, -0.0243,  0.0035,\n",
      "         0.0355,  0.0051, -0.0418,  0.0474, -0.0043, -0.0003,  0.0470, -0.0487,\n",
      "        -0.0057,  0.0309,  0.0485, -0.0046,  0.0476, -0.0493,  0.0408,  0.0449,\n",
      "        -0.0260, -0.0227, -0.0032,  0.0336,  0.0333,  0.0225, -0.0281,  0.0393,\n",
      "        -0.0427, -0.0169,  0.0288, -0.0306,  0.0319,  0.0086,  0.0105,  0.0499,\n",
      "         0.0344, -0.0444, -0.0407,  0.0363, -0.0008,  0.0243, -0.0239,  0.0485,\n",
      "        -0.0122,  0.0390,  0.0140, -0.0081, -0.0405,  0.0209,  0.0386, -0.0097,\n",
      "        -0.0239, -0.0485,  0.0305,  0.0072, -0.0133, -0.0476, -0.0037,  0.0131,\n",
      "         0.0022,  0.0045, -0.0179,  0.0268])\n"
     ]
    }
   ],
   "source": [
    "print(document_representations[293])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:28:19.719205Z",
     "iopub.status.busy": "2024-04-27T13:28:19.718799Z",
     "iopub.status.idle": "2024-04-27T13:36:49.277757Z",
     "shell.execute_reply": "2024-04-27T13:36:49.276796Z",
     "shell.execute_reply.started": "2024-04-27T13:28:19.719175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Iteration [100/20000], Training Loss: 0.6744\n",
      "Epoch [1/10], Iteration [200/20000], Training Loss: 0.6067\n",
      "Epoch [1/10], Iteration [300/20000], Training Loss: 0.5591\n",
      "Epoch [1/10], Iteration [400/20000], Training Loss: 0.5879\n",
      "Epoch [1/10], Iteration [500/20000], Training Loss: 0.5811\n",
      "Epoch [1/10], Iteration [600/20000], Training Loss: 0.6028\n",
      "Epoch [1/10], Iteration [700/20000], Training Loss: 0.5851\n",
      "Epoch [1/10], Iteration [800/20000], Training Loss: 0.5709\n",
      "Epoch [1/10], Iteration [900/20000], Training Loss: 0.4936\n",
      "Epoch [1/10], Iteration [1000/20000], Training Loss: 0.4986\n",
      "Epoch [1/10], Iteration [1100/20000], Training Loss: 0.4803\n",
      "Epoch [1/10], Iteration [1200/20000], Training Loss: 0.7243\n",
      "Epoch [1/10], Iteration [1300/20000], Training Loss: 0.5631\n",
      "Epoch [1/10], Iteration [1400/20000], Training Loss: 0.5096\n",
      "Epoch [1/10], Iteration [1500/20000], Training Loss: 0.5366\n",
      "Epoch [1/10], Iteration [1600/20000], Training Loss: 0.5468\n",
      "Epoch [1/10], Iteration [1700/20000], Training Loss: 0.5443\n",
      "Epoch [1/10], Iteration [1800/20000], Training Loss: 0.5561\n",
      "Epoch [1/10], Iteration [1900/20000], Training Loss: 0.5204\n",
      "Epoch [1/10], Iteration [2000/20000], Training Loss: 0.4905\n",
      "Epoch [1/10], Iteration [2100/20000], Training Loss: 0.6011\n",
      "Epoch [1/10], Iteration [2200/20000], Training Loss: 0.5665\n",
      "Epoch [1/10], Iteration [2300/20000], Training Loss: 0.6189\n",
      "Epoch [1/10], Iteration [2400/20000], Training Loss: 0.4985\n",
      "Epoch [1/10], Iteration [2500/20000], Training Loss: 0.5421\n",
      "Epoch [1/10], Iteration [2600/20000], Training Loss: 0.5250\n",
      "Epoch [1/10], Iteration [2700/20000], Training Loss: 0.4757\n",
      "Epoch [1/10], Iteration [2800/20000], Training Loss: 0.5109\n",
      "Epoch [1/10], Iteration [2900/20000], Training Loss: 0.3887\n",
      "Epoch [1/10], Iteration [3000/20000], Training Loss: 0.5127\n",
      "Epoch [1/10], Iteration [3100/20000], Training Loss: 0.5654\n",
      "Epoch [1/10], Iteration [3200/20000], Training Loss: 0.5311\n",
      "Epoch [1/10], Iteration [3300/20000], Training Loss: 0.4404\n",
      "Epoch [1/10], Iteration [3400/20000], Training Loss: 0.5678\n",
      "Epoch [1/10], Iteration [3500/20000], Training Loss: 0.5079\n",
      "Epoch [1/10], Iteration [3600/20000], Training Loss: 0.6006\n",
      "Epoch [1/10], Iteration [3700/20000], Training Loss: 0.5043\n",
      "Epoch [1/10], Iteration [3800/20000], Training Loss: 0.5020\n",
      "Epoch [1/10], Iteration [3900/20000], Training Loss: 0.5160\n",
      "Epoch [1/10], Iteration [4000/20000], Training Loss: 0.5199\n",
      "Epoch [1/10], Iteration [4100/20000], Training Loss: 0.6683\n",
      "Epoch [1/10], Iteration [4200/20000], Training Loss: 0.5570\n",
      "Epoch [1/10], Iteration [4300/20000], Training Loss: 0.4383\n",
      "Epoch [1/10], Iteration [4400/20000], Training Loss: 0.5075\n",
      "Epoch [1/10], Iteration [4500/20000], Training Loss: 0.6259\n",
      "Epoch [1/10], Iteration [4600/20000], Training Loss: 0.5301\n",
      "Epoch [1/10], Iteration [4700/20000], Training Loss: 0.5036\n",
      "Epoch [1/10], Iteration [4800/20000], Training Loss: 0.4780\n",
      "Epoch [1/10], Iteration [4900/20000], Training Loss: 0.4586\n",
      "Epoch [1/10], Iteration [5000/20000], Training Loss: 0.5427\n",
      "Epoch [1/10], Iteration [5100/20000], Training Loss: 0.5047\n",
      "Epoch [1/10], Iteration [5200/20000], Training Loss: 0.5522\n",
      "Epoch [1/10], Iteration [5300/20000], Training Loss: 0.5460\n",
      "Epoch [1/10], Iteration [5400/20000], Training Loss: 0.5984\n",
      "Epoch [1/10], Iteration [5500/20000], Training Loss: 0.4087\n",
      "Epoch [1/10], Iteration [5600/20000], Training Loss: 0.6689\n",
      "Epoch [1/10], Iteration [5700/20000], Training Loss: 0.5914\n",
      "Epoch [1/10], Iteration [5800/20000], Training Loss: 0.5478\n",
      "Epoch [1/10], Iteration [5900/20000], Training Loss: 0.4877\n",
      "Epoch [1/10], Iteration [6000/20000], Training Loss: 0.6035\n",
      "Epoch [1/10], Iteration [6100/20000], Training Loss: 0.5602\n",
      "Epoch [1/10], Iteration [6200/20000], Training Loss: 0.5075\n",
      "Epoch [1/10], Iteration [6300/20000], Training Loss: 0.4492\n",
      "Epoch [1/10], Iteration [6400/20000], Training Loss: 0.6137\n",
      "Epoch [1/10], Iteration [6500/20000], Training Loss: 0.6286\n",
      "Epoch [1/10], Iteration [6600/20000], Training Loss: 0.5077\n",
      "Epoch [1/10], Iteration [6700/20000], Training Loss: 0.4784\n",
      "Epoch [1/10], Iteration [6800/20000], Training Loss: 0.4524\n",
      "Epoch [1/10], Iteration [6900/20000], Training Loss: 0.5872\n",
      "Epoch [1/10], Iteration [7000/20000], Training Loss: 0.4349\n",
      "Epoch [1/10], Iteration [7100/20000], Training Loss: 0.5564\n",
      "Epoch [1/10], Iteration [7200/20000], Training Loss: 0.5914\n",
      "Epoch [1/10], Iteration [7300/20000], Training Loss: 0.5203\n",
      "Epoch [1/10], Iteration [7400/20000], Training Loss: 0.5164\n",
      "Epoch [1/10], Iteration [7500/20000], Training Loss: 0.4966\n",
      "Epoch [1/10], Iteration [7600/20000], Training Loss: 0.5225\n",
      "Epoch [1/10], Iteration [7700/20000], Training Loss: 0.4768\n",
      "Epoch [1/10], Iteration [7800/20000], Training Loss: 0.5906\n",
      "Epoch [1/10], Iteration [7900/20000], Training Loss: 0.5549\n",
      "Epoch [1/10], Iteration [8000/20000], Training Loss: 0.5003\n",
      "Epoch [1/10], Iteration [8100/20000], Training Loss: 0.6810\n",
      "Epoch [1/10], Iteration [8200/20000], Training Loss: 0.5412\n",
      "Epoch [1/10], Iteration [8300/20000], Training Loss: 0.5035\n",
      "Epoch [1/10], Iteration [8400/20000], Training Loss: 0.4585\n",
      "Epoch [1/10], Iteration [8500/20000], Training Loss: 0.3996\n",
      "Epoch [1/10], Iteration [8600/20000], Training Loss: 0.5309\n",
      "Epoch [1/10], Iteration [8700/20000], Training Loss: 0.5295\n",
      "Epoch [1/10], Iteration [8800/20000], Training Loss: 0.5556\n",
      "Epoch [1/10], Iteration [8900/20000], Training Loss: 0.4819\n",
      "Epoch [1/10], Iteration [9000/20000], Training Loss: 0.6110\n",
      "Epoch [1/10], Iteration [9100/20000], Training Loss: 0.5845\n",
      "Epoch [1/10], Iteration [9200/20000], Training Loss: 0.5032\n",
      "Epoch [1/10], Iteration [9300/20000], Training Loss: 0.5972\n",
      "Epoch [1/10], Iteration [9400/20000], Training Loss: 0.4630\n",
      "Epoch [1/10], Iteration [9500/20000], Training Loss: 0.4904\n",
      "Epoch [1/10], Iteration [9600/20000], Training Loss: 0.5567\n",
      "Epoch [1/10], Iteration [9700/20000], Training Loss: 0.5743\n",
      "Epoch [1/10], Iteration [9800/20000], Training Loss: 0.5720\n",
      "Epoch [1/10], Iteration [9900/20000], Training Loss: 0.4657\n",
      "Epoch [1/10], Iteration [10000/20000], Training Loss: 0.4830\n",
      "Epoch [1/10], Iteration [10100/20000], Training Loss: 0.5701\n",
      "Epoch [1/10], Iteration [10200/20000], Training Loss: 0.5417\n",
      "Epoch [1/10], Iteration [10300/20000], Training Loss: 0.6232\n",
      "Epoch [1/10], Iteration [10400/20000], Training Loss: 0.5708\n",
      "Epoch [1/10], Iteration [10500/20000], Training Loss: 0.5289\n",
      "Epoch [1/10], Iteration [10600/20000], Training Loss: 0.4809\n",
      "Epoch [1/10], Iteration [10700/20000], Training Loss: 0.5547\n",
      "Epoch [1/10], Iteration [10800/20000], Training Loss: 0.4595\n",
      "Epoch [1/10], Iteration [10900/20000], Training Loss: 0.5370\n",
      "Epoch [1/10], Iteration [11000/20000], Training Loss: 0.6630\n",
      "Epoch [1/10], Iteration [11100/20000], Training Loss: 0.5879\n",
      "Epoch [1/10], Iteration [11200/20000], Training Loss: 0.5648\n",
      "Epoch [1/10], Iteration [11300/20000], Training Loss: 0.5698\n",
      "Epoch [1/10], Iteration [11400/20000], Training Loss: 0.5057\n",
      "Epoch [1/10], Iteration [11500/20000], Training Loss: 0.5352\n",
      "Epoch [1/10], Iteration [11600/20000], Training Loss: 0.4150\n",
      "Epoch [1/10], Iteration [11700/20000], Training Loss: 0.5261\n",
      "Epoch [1/10], Iteration [11800/20000], Training Loss: 0.5605\n",
      "Epoch [1/10], Iteration [11900/20000], Training Loss: 0.4931\n",
      "Epoch [1/10], Iteration [12000/20000], Training Loss: 0.5341\n",
      "Epoch [1/10], Iteration [12100/20000], Training Loss: 0.4959\n",
      "Epoch [1/10], Iteration [12200/20000], Training Loss: 0.4681\n",
      "Epoch [1/10], Iteration [12300/20000], Training Loss: 0.5625\n",
      "Epoch [1/10], Iteration [12400/20000], Training Loss: 0.5028\n",
      "Epoch [1/10], Iteration [12500/20000], Training Loss: 0.4160\n",
      "Epoch [1/10], Iteration [12600/20000], Training Loss: 0.4486\n",
      "Epoch [1/10], Iteration [12700/20000], Training Loss: 0.4999\n",
      "Epoch [1/10], Iteration [12800/20000], Training Loss: 0.4724\n",
      "Epoch [1/10], Iteration [12900/20000], Training Loss: 0.5028\n",
      "Epoch [1/10], Iteration [13000/20000], Training Loss: 0.5633\n",
      "Epoch [1/10], Iteration [13100/20000], Training Loss: 0.4999\n",
      "Epoch [1/10], Iteration [13200/20000], Training Loss: 0.4900\n",
      "Epoch [1/10], Iteration [13300/20000], Training Loss: 0.5665\n",
      "Epoch [1/10], Iteration [13400/20000], Training Loss: 0.5754\n",
      "Epoch [1/10], Iteration [13500/20000], Training Loss: 0.6640\n",
      "Epoch [1/10], Iteration [13600/20000], Training Loss: 0.4781\n",
      "Epoch [1/10], Iteration [13700/20000], Training Loss: 0.5072\n",
      "Epoch [1/10], Iteration [13800/20000], Training Loss: 0.5758\n",
      "Epoch [1/10], Iteration [13900/20000], Training Loss: 0.5024\n",
      "Epoch [1/10], Iteration [14000/20000], Training Loss: 0.5168\n",
      "Epoch [1/10], Iteration [14100/20000], Training Loss: 0.4687\n",
      "Epoch [1/10], Iteration [14200/20000], Training Loss: 0.4838\n",
      "Epoch [1/10], Iteration [14300/20000], Training Loss: 0.4872\n",
      "Epoch [1/10], Iteration [14400/20000], Training Loss: 0.4726\n",
      "Epoch [1/10], Iteration [14500/20000], Training Loss: 0.4296\n",
      "Epoch [1/10], Iteration [14600/20000], Training Loss: 0.4596\n",
      "Epoch [1/10], Iteration [14700/20000], Training Loss: 0.4744\n",
      "Epoch [1/10], Iteration [14800/20000], Training Loss: 0.4928\n",
      "Epoch [1/10], Iteration [14900/20000], Training Loss: 0.3979\n",
      "Epoch [1/10], Iteration [15000/20000], Training Loss: 0.4627\n",
      "Epoch [1/10], Iteration [15100/20000], Training Loss: 0.4442\n",
      "Epoch [1/10], Iteration [15200/20000], Training Loss: 0.5054\n",
      "Epoch [1/10], Iteration [15300/20000], Training Loss: 0.6292\n",
      "Epoch [1/10], Iteration [15400/20000], Training Loss: 0.4669\n",
      "Epoch [1/10], Iteration [15500/20000], Training Loss: 0.5874\n",
      "Epoch [1/10], Iteration [15600/20000], Training Loss: 0.4910\n",
      "Epoch [1/10], Iteration [15700/20000], Training Loss: 0.5444\n",
      "Epoch [1/10], Iteration [15800/20000], Training Loss: 0.5830\n",
      "Epoch [1/10], Iteration [15900/20000], Training Loss: 0.5111\n",
      "Epoch [1/10], Iteration [16000/20000], Training Loss: 0.4534\n",
      "Epoch [1/10], Iteration [16100/20000], Training Loss: 0.5737\n",
      "Epoch [1/10], Iteration [16200/20000], Training Loss: 0.5337\n",
      "Epoch [1/10], Iteration [16300/20000], Training Loss: 0.5126\n",
      "Epoch [1/10], Iteration [16400/20000], Training Loss: 0.5644\n",
      "Epoch [1/10], Iteration [16500/20000], Training Loss: 0.5196\n",
      "Epoch [1/10], Iteration [16600/20000], Training Loss: 0.5119\n",
      "Epoch [1/10], Iteration [16700/20000], Training Loss: 0.4174\n",
      "Epoch [1/10], Iteration [16800/20000], Training Loss: 0.5903\n",
      "Epoch [1/10], Iteration [16900/20000], Training Loss: 0.5216\n",
      "Epoch [1/10], Iteration [17000/20000], Training Loss: 0.5234\n",
      "Epoch [1/10], Iteration [17100/20000], Training Loss: 0.4721\n",
      "Epoch [1/10], Iteration [17200/20000], Training Loss: 0.4505\n",
      "Epoch [1/10], Iteration [17300/20000], Training Loss: 0.4117\n",
      "Epoch [1/10], Iteration [17400/20000], Training Loss: 0.6201\n",
      "Epoch [1/10], Iteration [17500/20000], Training Loss: 0.5066\n",
      "Epoch [1/10], Iteration [17600/20000], Training Loss: 0.6564\n",
      "Epoch [1/10], Iteration [17700/20000], Training Loss: 0.4493\n",
      "Epoch [1/10], Iteration [17800/20000], Training Loss: 0.4982\n",
      "Epoch [1/10], Iteration [17900/20000], Training Loss: 0.5613\n",
      "Epoch [1/10], Iteration [18000/20000], Training Loss: 0.4932\n",
      "Epoch [1/10], Iteration [18100/20000], Training Loss: 0.5974\n",
      "Epoch [1/10], Iteration [18200/20000], Training Loss: 0.5267\n",
      "Epoch [1/10], Iteration [18300/20000], Training Loss: 0.5135\n",
      "Epoch [1/10], Iteration [18400/20000], Training Loss: 0.4632\n",
      "Epoch [1/10], Iteration [18500/20000], Training Loss: 0.5932\n",
      "Epoch [1/10], Iteration [18600/20000], Training Loss: 0.4127\n",
      "Epoch [1/10], Iteration [18700/20000], Training Loss: 0.4742\n",
      "Epoch [1/10], Iteration [18800/20000], Training Loss: 0.5149\n",
      "Epoch [1/10], Iteration [18900/20000], Training Loss: 0.5256\n",
      "Epoch [1/10], Iteration [19000/20000], Training Loss: 0.4866\n",
      "Epoch [1/10], Iteration [19100/20000], Training Loss: 0.4564\n",
      "Epoch [1/10], Iteration [19200/20000], Training Loss: 0.6191\n",
      "Epoch [1/10], Iteration [19300/20000], Training Loss: 0.4896\n",
      "Epoch [1/10], Iteration [19400/20000], Training Loss: 0.6008\n",
      "Epoch [1/10], Iteration [19500/20000], Training Loss: 0.4896\n",
      "Epoch [1/10], Iteration [19600/20000], Training Loss: 0.5526\n",
      "Epoch [1/10], Iteration [19700/20000], Training Loss: 0.5460\n",
      "Epoch [1/10], Iteration [19800/20000], Training Loss: 0.4947\n",
      "Epoch [1/10], Iteration [19900/20000], Training Loss: 0.4956\n",
      "Epoch [1/10], Iteration [20000/20000], Training Loss: 0.4344\n",
      "Epoch [1/10], Average Training Loss: 0.5305\n",
      "Epoch [2/10], Iteration [100/20000], Training Loss: 0.4715\n",
      "Epoch [2/10], Iteration [200/20000], Training Loss: 0.5472\n",
      "Epoch [2/10], Iteration [300/20000], Training Loss: 0.4455\n",
      "Epoch [2/10], Iteration [400/20000], Training Loss: 0.5857\n",
      "Epoch [2/10], Iteration [500/20000], Training Loss: 0.5897\n",
      "Epoch [2/10], Iteration [600/20000], Training Loss: 0.4758\n",
      "Epoch [2/10], Iteration [700/20000], Training Loss: 0.4992\n",
      "Epoch [2/10], Iteration [800/20000], Training Loss: 0.5259\n",
      "Epoch [2/10], Iteration [900/20000], Training Loss: 0.6164\n",
      "Epoch [2/10], Iteration [1000/20000], Training Loss: 0.5233\n",
      "Epoch [2/10], Iteration [1100/20000], Training Loss: 0.4636\n",
      "Epoch [2/10], Iteration [1200/20000], Training Loss: 0.5094\n",
      "Epoch [2/10], Iteration [1300/20000], Training Loss: 0.5249\n",
      "Epoch [2/10], Iteration [1400/20000], Training Loss: 0.5481\n",
      "Epoch [2/10], Iteration [1500/20000], Training Loss: 0.4702\n",
      "Epoch [2/10], Iteration [1600/20000], Training Loss: 0.5223\n",
      "Epoch [2/10], Iteration [1700/20000], Training Loss: 0.4858\n",
      "Epoch [2/10], Iteration [1800/20000], Training Loss: 0.5063\n",
      "Epoch [2/10], Iteration [1900/20000], Training Loss: 0.6345\n",
      "Epoch [2/10], Iteration [2000/20000], Training Loss: 0.5066\n",
      "Epoch [2/10], Iteration [2100/20000], Training Loss: 0.4933\n",
      "Epoch [2/10], Iteration [2200/20000], Training Loss: 0.4993\n",
      "Epoch [2/10], Iteration [2300/20000], Training Loss: 0.5469\n",
      "Epoch [2/10], Iteration [2400/20000], Training Loss: 0.5017\n",
      "Epoch [2/10], Iteration [2500/20000], Training Loss: 0.5591\n",
      "Epoch [2/10], Iteration [2600/20000], Training Loss: 0.5827\n",
      "Epoch [2/10], Iteration [2700/20000], Training Loss: 0.4455\n",
      "Epoch [2/10], Iteration [2800/20000], Training Loss: 0.4911\n",
      "Epoch [2/10], Iteration [2900/20000], Training Loss: 0.5764\n",
      "Epoch [2/10], Iteration [3000/20000], Training Loss: 0.6896\n",
      "Epoch [2/10], Iteration [3100/20000], Training Loss: 0.4774\n",
      "Epoch [2/10], Iteration [3200/20000], Training Loss: 0.5712\n",
      "Epoch [2/10], Iteration [3300/20000], Training Loss: 0.4672\n",
      "Epoch [2/10], Iteration [3400/20000], Training Loss: 0.4391\n",
      "Epoch [2/10], Iteration [3500/20000], Training Loss: 0.5376\n",
      "Epoch [2/10], Iteration [3600/20000], Training Loss: 0.4306\n",
      "Epoch [2/10], Iteration [3700/20000], Training Loss: 0.4418\n",
      "Epoch [2/10], Iteration [3800/20000], Training Loss: 0.4749\n",
      "Epoch [2/10], Iteration [3900/20000], Training Loss: 0.3947\n",
      "Epoch [2/10], Iteration [4000/20000], Training Loss: 0.6048\n",
      "Epoch [2/10], Iteration [4100/20000], Training Loss: 0.4761\n",
      "Epoch [2/10], Iteration [4200/20000], Training Loss: 0.5997\n",
      "Epoch [2/10], Iteration [4300/20000], Training Loss: 0.4217\n",
      "Epoch [2/10], Iteration [4400/20000], Training Loss: 0.5595\n",
      "Epoch [2/10], Iteration [4500/20000], Training Loss: 0.4284\n",
      "Epoch [2/10], Iteration [4600/20000], Training Loss: 0.5306\n",
      "Epoch [2/10], Iteration [4700/20000], Training Loss: 0.5040\n",
      "Epoch [2/10], Iteration [4800/20000], Training Loss: 0.4932\n",
      "Epoch [2/10], Iteration [4900/20000], Training Loss: 0.4858\n",
      "Epoch [2/10], Iteration [5000/20000], Training Loss: 0.6106\n",
      "Epoch [2/10], Iteration [5100/20000], Training Loss: 0.5242\n",
      "Epoch [2/10], Iteration [5200/20000], Training Loss: 0.5342\n",
      "Epoch [2/10], Iteration [5300/20000], Training Loss: 0.5441\n",
      "Epoch [2/10], Iteration [5400/20000], Training Loss: 0.4977\n",
      "Epoch [2/10], Iteration [5500/20000], Training Loss: 0.4861\n",
      "Epoch [2/10], Iteration [5600/20000], Training Loss: 0.5321\n",
      "Epoch [2/10], Iteration [5700/20000], Training Loss: 0.5241\n",
      "Epoch [2/10], Iteration [5800/20000], Training Loss: 0.5416\n",
      "Epoch [2/10], Iteration [5900/20000], Training Loss: 0.4158\n",
      "Epoch [2/10], Iteration [6000/20000], Training Loss: 0.5313\n",
      "Epoch [2/10], Iteration [6100/20000], Training Loss: 0.4737\n",
      "Epoch [2/10], Iteration [6200/20000], Training Loss: 0.4865\n",
      "Epoch [2/10], Iteration [6300/20000], Training Loss: 0.4434\n",
      "Epoch [2/10], Iteration [6400/20000], Training Loss: 0.4786\n",
      "Epoch [2/10], Iteration [6500/20000], Training Loss: 0.5384\n",
      "Epoch [2/10], Iteration [6600/20000], Training Loss: 0.5676\n",
      "Epoch [2/10], Iteration [6700/20000], Training Loss: 0.5930\n",
      "Epoch [2/10], Iteration [6800/20000], Training Loss: 0.5304\n",
      "Epoch [2/10], Iteration [6900/20000], Training Loss: 0.4294\n",
      "Epoch [2/10], Iteration [7000/20000], Training Loss: 0.4580\n",
      "Epoch [2/10], Iteration [7100/20000], Training Loss: 0.5483\n",
      "Epoch [2/10], Iteration [7200/20000], Training Loss: 0.5615\n",
      "Epoch [2/10], Iteration [7300/20000], Training Loss: 0.4546\n",
      "Epoch [2/10], Iteration [7400/20000], Training Loss: 0.5101\n",
      "Epoch [2/10], Iteration [7500/20000], Training Loss: 0.4197\n",
      "Epoch [2/10], Iteration [7600/20000], Training Loss: 0.5845\n",
      "Epoch [2/10], Iteration [7700/20000], Training Loss: 0.4588\n",
      "Epoch [2/10], Iteration [7800/20000], Training Loss: 0.4600\n",
      "Epoch [2/10], Iteration [7900/20000], Training Loss: 0.6576\n",
      "Epoch [2/10], Iteration [8000/20000], Training Loss: 0.4111\n",
      "Epoch [2/10], Iteration [8100/20000], Training Loss: 0.5490\n",
      "Epoch [2/10], Iteration [8200/20000], Training Loss: 0.5667\n",
      "Epoch [2/10], Iteration [8300/20000], Training Loss: 0.5286\n",
      "Epoch [2/10], Iteration [8400/20000], Training Loss: 0.4675\n",
      "Epoch [2/10], Iteration [8500/20000], Training Loss: 0.4999\n",
      "Epoch [2/10], Iteration [8600/20000], Training Loss: 0.5184\n",
      "Epoch [2/10], Iteration [8700/20000], Training Loss: 0.5196\n",
      "Epoch [2/10], Iteration [8800/20000], Training Loss: 0.4284\n",
      "Epoch [2/10], Iteration [8900/20000], Training Loss: 0.5864\n",
      "Epoch [2/10], Iteration [9000/20000], Training Loss: 0.6176\n",
      "Epoch [2/10], Iteration [9100/20000], Training Loss: 0.4047\n",
      "Epoch [2/10], Iteration [9200/20000], Training Loss: 0.5437\n",
      "Epoch [2/10], Iteration [9300/20000], Training Loss: 0.5143\n",
      "Epoch [2/10], Iteration [9400/20000], Training Loss: 0.5064\n",
      "Epoch [2/10], Iteration [9500/20000], Training Loss: 0.4965\n",
      "Epoch [2/10], Iteration [9600/20000], Training Loss: 0.5715\n",
      "Epoch [2/10], Iteration [9700/20000], Training Loss: 0.4871\n",
      "Epoch [2/10], Iteration [9800/20000], Training Loss: 0.4332\n",
      "Epoch [2/10], Iteration [9900/20000], Training Loss: 0.4916\n",
      "Epoch [2/10], Iteration [10000/20000], Training Loss: 0.4496\n",
      "Epoch [2/10], Iteration [10100/20000], Training Loss: 0.5534\n",
      "Epoch [2/10], Iteration [10200/20000], Training Loss: 0.5551\n",
      "Epoch [2/10], Iteration [10300/20000], Training Loss: 0.5406\n",
      "Epoch [2/10], Iteration [10400/20000], Training Loss: 0.5379\n",
      "Epoch [2/10], Iteration [10500/20000], Training Loss: 0.5509\n",
      "Epoch [2/10], Iteration [10600/20000], Training Loss: 0.4850\n",
      "Epoch [2/10], Iteration [10700/20000], Training Loss: 0.6050\n",
      "Epoch [2/10], Iteration [10800/20000], Training Loss: 0.5052\n",
      "Epoch [2/10], Iteration [10900/20000], Training Loss: 0.4644\n",
      "Epoch [2/10], Iteration [11000/20000], Training Loss: 0.5352\n",
      "Epoch [2/10], Iteration [11100/20000], Training Loss: 0.6266\n",
      "Epoch [2/10], Iteration [11200/20000], Training Loss: 0.5084\n",
      "Epoch [2/10], Iteration [11300/20000], Training Loss: 0.5157\n",
      "Epoch [2/10], Iteration [11400/20000], Training Loss: 0.4281\n",
      "Epoch [2/10], Iteration [11500/20000], Training Loss: 0.3757\n",
      "Epoch [2/10], Iteration [11600/20000], Training Loss: 0.4891\n",
      "Epoch [2/10], Iteration [11700/20000], Training Loss: 0.4489\n",
      "Epoch [2/10], Iteration [11800/20000], Training Loss: 0.4055\n",
      "Epoch [2/10], Iteration [11900/20000], Training Loss: 0.5181\n",
      "Epoch [2/10], Iteration [12000/20000], Training Loss: 0.4843\n",
      "Epoch [2/10], Iteration [12100/20000], Training Loss: 0.4327\n",
      "Epoch [2/10], Iteration [12200/20000], Training Loss: 0.4381\n",
      "Epoch [2/10], Iteration [12300/20000], Training Loss: 0.5710\n",
      "Epoch [2/10], Iteration [12400/20000], Training Loss: 0.4455\n",
      "Epoch [2/10], Iteration [12500/20000], Training Loss: 0.4834\n",
      "Epoch [2/10], Iteration [12600/20000], Training Loss: 0.5324\n",
      "Epoch [2/10], Iteration [12700/20000], Training Loss: 0.6226\n",
      "Epoch [2/10], Iteration [12800/20000], Training Loss: 0.5179\n",
      "Epoch [2/10], Iteration [12900/20000], Training Loss: 0.5858\n",
      "Epoch [2/10], Iteration [13000/20000], Training Loss: 0.4535\n",
      "Epoch [2/10], Iteration [13100/20000], Training Loss: 0.5034\n",
      "Epoch [2/10], Iteration [13200/20000], Training Loss: 0.5310\n",
      "Epoch [2/10], Iteration [13300/20000], Training Loss: 0.5610\n",
      "Epoch [2/10], Iteration [13400/20000], Training Loss: 0.4711\n",
      "Epoch [2/10], Iteration [13500/20000], Training Loss: 0.4314\n",
      "Epoch [2/10], Iteration [13600/20000], Training Loss: 0.4842\n",
      "Epoch [2/10], Iteration [13700/20000], Training Loss: 0.5858\n",
      "Epoch [2/10], Iteration [13800/20000], Training Loss: 0.5600\n",
      "Epoch [2/10], Iteration [13900/20000], Training Loss: 0.5400\n",
      "Epoch [2/10], Iteration [14000/20000], Training Loss: 0.5559\n",
      "Epoch [2/10], Iteration [14100/20000], Training Loss: 0.5639\n",
      "Epoch [2/10], Iteration [14200/20000], Training Loss: 0.4906\n",
      "Epoch [2/10], Iteration [14300/20000], Training Loss: 0.6562\n",
      "Epoch [2/10], Iteration [14400/20000], Training Loss: 0.5565\n",
      "Epoch [2/10], Iteration [14500/20000], Training Loss: 0.4922\n",
      "Epoch [2/10], Iteration [14600/20000], Training Loss: 0.4794\n",
      "Epoch [2/10], Iteration [14700/20000], Training Loss: 0.6746\n",
      "Epoch [2/10], Iteration [14800/20000], Training Loss: 0.6280\n",
      "Epoch [2/10], Iteration [14900/20000], Training Loss: 0.4932\n",
      "Epoch [2/10], Iteration [15000/20000], Training Loss: 0.4772\n",
      "Epoch [2/10], Iteration [15100/20000], Training Loss: 0.4328\n",
      "Epoch [2/10], Iteration [15200/20000], Training Loss: 0.5567\n",
      "Epoch [2/10], Iteration [15300/20000], Training Loss: 0.6076\n",
      "Epoch [2/10], Iteration [15400/20000], Training Loss: 0.5394\n",
      "Epoch [2/10], Iteration [15500/20000], Training Loss: 0.5242\n",
      "Epoch [2/10], Iteration [15600/20000], Training Loss: 0.5358\n",
      "Epoch [2/10], Iteration [15700/20000], Training Loss: 0.5271\n",
      "Epoch [2/10], Iteration [15800/20000], Training Loss: 0.4663\n",
      "Epoch [2/10], Iteration [15900/20000], Training Loss: 0.4625\n",
      "Epoch [2/10], Iteration [16000/20000], Training Loss: 0.5250\n",
      "Epoch [2/10], Iteration [16100/20000], Training Loss: 0.5495\n",
      "Epoch [2/10], Iteration [16200/20000], Training Loss: 0.5749\n",
      "Epoch [2/10], Iteration [16300/20000], Training Loss: 0.4774\n",
      "Epoch [2/10], Iteration [16400/20000], Training Loss: 0.5427\n",
      "Epoch [2/10], Iteration [16500/20000], Training Loss: 0.4826\n",
      "Epoch [2/10], Iteration [16600/20000], Training Loss: 0.5854\n",
      "Epoch [2/10], Iteration [16700/20000], Training Loss: 0.5143\n",
      "Epoch [2/10], Iteration [16800/20000], Training Loss: 0.5232\n",
      "Epoch [2/10], Iteration [16900/20000], Training Loss: 0.5527\n",
      "Epoch [2/10], Iteration [17000/20000], Training Loss: 0.5758\n",
      "Epoch [2/10], Iteration [17100/20000], Training Loss: 0.4369\n",
      "Epoch [2/10], Iteration [17200/20000], Training Loss: 0.5501\n",
      "Epoch [2/10], Iteration [17300/20000], Training Loss: 0.4255\n",
      "Epoch [2/10], Iteration [17400/20000], Training Loss: 0.4958\n",
      "Epoch [2/10], Iteration [17500/20000], Training Loss: 0.5244\n",
      "Epoch [2/10], Iteration [17600/20000], Training Loss: 0.4266\n",
      "Epoch [2/10], Iteration [17700/20000], Training Loss: 0.4979\n",
      "Epoch [2/10], Iteration [17800/20000], Training Loss: 0.5004\n",
      "Epoch [2/10], Iteration [17900/20000], Training Loss: 0.4562\n",
      "Epoch [2/10], Iteration [18000/20000], Training Loss: 0.5118\n",
      "Epoch [2/10], Iteration [18100/20000], Training Loss: 0.4450\n",
      "Epoch [2/10], Iteration [18200/20000], Training Loss: 0.3836\n",
      "Epoch [2/10], Iteration [18300/20000], Training Loss: 0.3985\n",
      "Epoch [2/10], Iteration [18400/20000], Training Loss: 0.5233\n",
      "Epoch [2/10], Iteration [18500/20000], Training Loss: 0.4543\n",
      "Epoch [2/10], Iteration [18600/20000], Training Loss: 0.5053\n",
      "Epoch [2/10], Iteration [18700/20000], Training Loss: 0.3762\n",
      "Epoch [2/10], Iteration [18800/20000], Training Loss: 0.4338\n",
      "Epoch [2/10], Iteration [18900/20000], Training Loss: 0.5207\n",
      "Epoch [2/10], Iteration [19000/20000], Training Loss: 0.4502\n",
      "Epoch [2/10], Iteration [19100/20000], Training Loss: 0.5552\n",
      "Epoch [2/10], Iteration [19200/20000], Training Loss: 0.5213\n",
      "Epoch [2/10], Iteration [19300/20000], Training Loss: 0.6492\n",
      "Epoch [2/10], Iteration [19400/20000], Training Loss: 0.4963\n",
      "Epoch [2/10], Iteration [19500/20000], Training Loss: 0.5741\n",
      "Epoch [2/10], Iteration [19600/20000], Training Loss: 0.4900\n",
      "Epoch [2/10], Iteration [19700/20000], Training Loss: 0.4240\n",
      "Epoch [2/10], Iteration [19800/20000], Training Loss: 0.4728\n",
      "Epoch [2/10], Iteration [19900/20000], Training Loss: 0.4736\n",
      "Epoch [2/10], Iteration [20000/20000], Training Loss: 0.4766\n",
      "Epoch [2/10], Average Training Loss: 0.5157\n",
      "Epoch [3/10], Iteration [100/20000], Training Loss: 0.4224\n",
      "Epoch [3/10], Iteration [200/20000], Training Loss: 0.5357\n",
      "Epoch [3/10], Iteration [300/20000], Training Loss: 0.5070\n",
      "Epoch [3/10], Iteration [400/20000], Training Loss: 0.5938\n",
      "Epoch [3/10], Iteration [500/20000], Training Loss: 0.5371\n",
      "Epoch [3/10], Iteration [600/20000], Training Loss: 0.4921\n",
      "Epoch [3/10], Iteration [700/20000], Training Loss: 0.5907\n",
      "Epoch [3/10], Iteration [800/20000], Training Loss: 0.4261\n",
      "Epoch [3/10], Iteration [900/20000], Training Loss: 0.4045\n",
      "Epoch [3/10], Iteration [1000/20000], Training Loss: 0.5061\n",
      "Epoch [3/10], Iteration [1100/20000], Training Loss: 0.4906\n",
      "Epoch [3/10], Iteration [1200/20000], Training Loss: 0.5013\n",
      "Epoch [3/10], Iteration [1300/20000], Training Loss: 0.5402\n",
      "Epoch [3/10], Iteration [1400/20000], Training Loss: 0.4801\n",
      "Epoch [3/10], Iteration [1500/20000], Training Loss: 0.4083\n",
      "Epoch [3/10], Iteration [1600/20000], Training Loss: 0.4046\n",
      "Epoch [3/10], Iteration [1700/20000], Training Loss: 0.5677\n",
      "Epoch [3/10], Iteration [1800/20000], Training Loss: 0.5120\n",
      "Epoch [3/10], Iteration [1900/20000], Training Loss: 0.5511\n",
      "Epoch [3/10], Iteration [2000/20000], Training Loss: 0.4705\n",
      "Epoch [3/10], Iteration [2100/20000], Training Loss: 0.5239\n",
      "Epoch [3/10], Iteration [2200/20000], Training Loss: 0.7498\n",
      "Epoch [3/10], Iteration [2300/20000], Training Loss: 0.4970\n",
      "Epoch [3/10], Iteration [2400/20000], Training Loss: 0.4889\n",
      "Epoch [3/10], Iteration [2500/20000], Training Loss: 0.4883\n",
      "Epoch [3/10], Iteration [2600/20000], Training Loss: 0.4671\n",
      "Epoch [3/10], Iteration [2700/20000], Training Loss: 0.5406\n",
      "Epoch [3/10], Iteration [2800/20000], Training Loss: 0.4222\n",
      "Epoch [3/10], Iteration [2900/20000], Training Loss: 0.5617\n",
      "Epoch [3/10], Iteration [3000/20000], Training Loss: 0.4701\n",
      "Epoch [3/10], Iteration [3100/20000], Training Loss: 0.5576\n",
      "Epoch [3/10], Iteration [3200/20000], Training Loss: 0.4297\n",
      "Epoch [3/10], Iteration [3300/20000], Training Loss: 0.5715\n",
      "Epoch [3/10], Iteration [3400/20000], Training Loss: 0.5466\n",
      "Epoch [3/10], Iteration [3500/20000], Training Loss: 0.5929\n",
      "Epoch [3/10], Iteration [3600/20000], Training Loss: 0.5359\n",
      "Epoch [3/10], Iteration [3700/20000], Training Loss: 0.4232\n",
      "Epoch [3/10], Iteration [3800/20000], Training Loss: 0.5597\n",
      "Epoch [3/10], Iteration [3900/20000], Training Loss: 0.4579\n",
      "Epoch [3/10], Iteration [4000/20000], Training Loss: 0.4526\n",
      "Epoch [3/10], Iteration [4100/20000], Training Loss: 0.4713\n",
      "Epoch [3/10], Iteration [4200/20000], Training Loss: 0.5542\n",
      "Epoch [3/10], Iteration [4300/20000], Training Loss: 0.4965\n",
      "Epoch [3/10], Iteration [4400/20000], Training Loss: 0.4722\n",
      "Epoch [3/10], Iteration [4500/20000], Training Loss: 0.4863\n",
      "Epoch [3/10], Iteration [4600/20000], Training Loss: 0.5701\n",
      "Epoch [3/10], Iteration [4700/20000], Training Loss: 0.5193\n",
      "Epoch [3/10], Iteration [4800/20000], Training Loss: 0.5713\n",
      "Epoch [3/10], Iteration [4900/20000], Training Loss: 0.4813\n",
      "Epoch [3/10], Iteration [5000/20000], Training Loss: 0.5097\n",
      "Epoch [3/10], Iteration [5100/20000], Training Loss: 0.5229\n",
      "Epoch [3/10], Iteration [5200/20000], Training Loss: 0.5041\n",
      "Epoch [3/10], Iteration [5300/20000], Training Loss: 0.5070\n",
      "Epoch [3/10], Iteration [5400/20000], Training Loss: 0.5900\n",
      "Epoch [3/10], Iteration [5500/20000], Training Loss: 0.4839\n",
      "Epoch [3/10], Iteration [5600/20000], Training Loss: 0.5632\n",
      "Epoch [3/10], Iteration [5700/20000], Training Loss: 0.4878\n",
      "Epoch [3/10], Iteration [5800/20000], Training Loss: 0.4787\n",
      "Epoch [3/10], Iteration [5900/20000], Training Loss: 0.5039\n",
      "Epoch [3/10], Iteration [6000/20000], Training Loss: 0.4681\n",
      "Epoch [3/10], Iteration [6100/20000], Training Loss: 0.4442\n",
      "Epoch [3/10], Iteration [6200/20000], Training Loss: 0.4369\n",
      "Epoch [3/10], Iteration [6300/20000], Training Loss: 0.5284\n",
      "Epoch [3/10], Iteration [6400/20000], Training Loss: 0.3974\n",
      "Epoch [3/10], Iteration [6500/20000], Training Loss: 0.3925\n",
      "Epoch [3/10], Iteration [6600/20000], Training Loss: 0.5078\n",
      "Epoch [3/10], Iteration [6700/20000], Training Loss: 0.4795\n",
      "Epoch [3/10], Iteration [6800/20000], Training Loss: 0.5837\n",
      "Epoch [3/10], Iteration [6900/20000], Training Loss: 0.5627\n",
      "Epoch [3/10], Iteration [7000/20000], Training Loss: 0.4850\n",
      "Epoch [3/10], Iteration [7100/20000], Training Loss: 0.5018\n",
      "Epoch [3/10], Iteration [7200/20000], Training Loss: 0.5097\n",
      "Epoch [3/10], Iteration [7300/20000], Training Loss: 0.5123\n",
      "Epoch [3/10], Iteration [7400/20000], Training Loss: 0.3809\n",
      "Epoch [3/10], Iteration [7500/20000], Training Loss: 0.5768\n",
      "Epoch [3/10], Iteration [7600/20000], Training Loss: 0.5768\n",
      "Epoch [3/10], Iteration [7700/20000], Training Loss: 0.5830\n",
      "Epoch [3/10], Iteration [7800/20000], Training Loss: 0.4497\n",
      "Epoch [3/10], Iteration [7900/20000], Training Loss: 0.5795\n",
      "Epoch [3/10], Iteration [8000/20000], Training Loss: 0.4958\n",
      "Epoch [3/10], Iteration [8100/20000], Training Loss: 0.4924\n",
      "Epoch [3/10], Iteration [8200/20000], Training Loss: 0.3993\n",
      "Epoch [3/10], Iteration [8300/20000], Training Loss: 0.4459\n",
      "Epoch [3/10], Iteration [8400/20000], Training Loss: 0.5068\n",
      "Epoch [3/10], Iteration [8500/20000], Training Loss: 0.4882\n",
      "Epoch [3/10], Iteration [8600/20000], Training Loss: 0.4266\n",
      "Epoch [3/10], Iteration [8700/20000], Training Loss: 0.5103\n",
      "Epoch [3/10], Iteration [8800/20000], Training Loss: 0.5488\n",
      "Epoch [3/10], Iteration [8900/20000], Training Loss: 0.5088\n",
      "Epoch [3/10], Iteration [9000/20000], Training Loss: 0.5715\n",
      "Epoch [3/10], Iteration [9100/20000], Training Loss: 0.5404\n",
      "Epoch [3/10], Iteration [9200/20000], Training Loss: 0.6109\n",
      "Epoch [3/10], Iteration [9300/20000], Training Loss: 0.4343\n",
      "Epoch [3/10], Iteration [9400/20000], Training Loss: 0.5146\n",
      "Epoch [3/10], Iteration [9500/20000], Training Loss: 0.5659\n",
      "Epoch [3/10], Iteration [9600/20000], Training Loss: 0.5235\n",
      "Epoch [3/10], Iteration [9700/20000], Training Loss: 0.4890\n",
      "Epoch [3/10], Iteration [9800/20000], Training Loss: 0.4516\n",
      "Epoch [3/10], Iteration [9900/20000], Training Loss: 0.5835\n",
      "Epoch [3/10], Iteration [10000/20000], Training Loss: 0.5684\n",
      "Epoch [3/10], Iteration [10100/20000], Training Loss: 0.5631\n",
      "Epoch [3/10], Iteration [10200/20000], Training Loss: 0.5655\n",
      "Epoch [3/10], Iteration [10300/20000], Training Loss: 0.4710\n",
      "Epoch [3/10], Iteration [10400/20000], Training Loss: 0.5670\n",
      "Epoch [3/10], Iteration [10500/20000], Training Loss: 0.5913\n",
      "Epoch [3/10], Iteration [10600/20000], Training Loss: 0.4998\n",
      "Epoch [3/10], Iteration [10700/20000], Training Loss: 0.4948\n",
      "Epoch [3/10], Iteration [10800/20000], Training Loss: 0.4973\n",
      "Epoch [3/10], Iteration [10900/20000], Training Loss: 0.5102\n",
      "Epoch [3/10], Iteration [11000/20000], Training Loss: 0.5023\n",
      "Epoch [3/10], Iteration [11100/20000], Training Loss: 0.4909\n",
      "Epoch [3/10], Iteration [11200/20000], Training Loss: 0.5583\n",
      "Epoch [3/10], Iteration [11300/20000], Training Loss: 0.6326\n",
      "Epoch [3/10], Iteration [11400/20000], Training Loss: 0.5847\n",
      "Epoch [3/10], Iteration [11500/20000], Training Loss: 0.4939\n",
      "Epoch [3/10], Iteration [11600/20000], Training Loss: 0.5937\n",
      "Epoch [3/10], Iteration [11700/20000], Training Loss: 0.4983\n",
      "Epoch [3/10], Iteration [11800/20000], Training Loss: 0.5390\n",
      "Epoch [3/10], Iteration [11900/20000], Training Loss: 0.5676\n",
      "Epoch [3/10], Iteration [12000/20000], Training Loss: 0.4960\n",
      "Epoch [3/10], Iteration [12100/20000], Training Loss: 0.5361\n",
      "Epoch [3/10], Iteration [12200/20000], Training Loss: 0.5490\n",
      "Epoch [3/10], Iteration [12300/20000], Training Loss: 0.4434\n",
      "Epoch [3/10], Iteration [12400/20000], Training Loss: 0.5510\n",
      "Epoch [3/10], Iteration [12500/20000], Training Loss: 0.5090\n",
      "Epoch [3/10], Iteration [12600/20000], Training Loss: 0.5626\n",
      "Epoch [3/10], Iteration [12700/20000], Training Loss: 0.4966\n",
      "Epoch [3/10], Iteration [12800/20000], Training Loss: 0.5414\n",
      "Epoch [3/10], Iteration [12900/20000], Training Loss: 0.6009\n",
      "Epoch [3/10], Iteration [13000/20000], Training Loss: 0.4748\n",
      "Epoch [3/10], Iteration [13100/20000], Training Loss: 0.4133\n",
      "Epoch [3/10], Iteration [13200/20000], Training Loss: 0.4836\n",
      "Epoch [3/10], Iteration [13300/20000], Training Loss: 0.5300\n",
      "Epoch [3/10], Iteration [13400/20000], Training Loss: 0.5358\n",
      "Epoch [3/10], Iteration [13500/20000], Training Loss: 0.5263\n",
      "Epoch [3/10], Iteration [13600/20000], Training Loss: 0.4458\n",
      "Epoch [3/10], Iteration [13700/20000], Training Loss: 0.5278\n",
      "Epoch [3/10], Iteration [13800/20000], Training Loss: 0.4619\n",
      "Epoch [3/10], Iteration [13900/20000], Training Loss: 0.4599\n",
      "Epoch [3/10], Iteration [14000/20000], Training Loss: 0.5458\n",
      "Epoch [3/10], Iteration [14100/20000], Training Loss: 0.4895\n",
      "Epoch [3/10], Iteration [14200/20000], Training Loss: 0.5904\n",
      "Epoch [3/10], Iteration [14300/20000], Training Loss: 0.5098\n",
      "Epoch [3/10], Iteration [14400/20000], Training Loss: 0.4908\n",
      "Epoch [3/10], Iteration [14500/20000], Training Loss: 0.5665\n",
      "Epoch [3/10], Iteration [14600/20000], Training Loss: 0.4733\n",
      "Epoch [3/10], Iteration [14700/20000], Training Loss: 0.4933\n",
      "Epoch [3/10], Iteration [14800/20000], Training Loss: 0.6269\n",
      "Epoch [3/10], Iteration [14900/20000], Training Loss: 0.4934\n",
      "Epoch [3/10], Iteration [15000/20000], Training Loss: 0.5550\n",
      "Epoch [3/10], Iteration [15100/20000], Training Loss: 0.6136\n",
      "Epoch [3/10], Iteration [15200/20000], Training Loss: 0.4785\n",
      "Epoch [3/10], Iteration [15300/20000], Training Loss: 0.4396\n",
      "Epoch [3/10], Iteration [15400/20000], Training Loss: 0.5671\n",
      "Epoch [3/10], Iteration [15500/20000], Training Loss: 0.4504\n",
      "Epoch [3/10], Iteration [15600/20000], Training Loss: 0.4890\n",
      "Epoch [3/10], Iteration [15700/20000], Training Loss: 0.5074\n",
      "Epoch [3/10], Iteration [15800/20000], Training Loss: 0.4534\n",
      "Epoch [3/10], Iteration [15900/20000], Training Loss: 0.4803\n",
      "Epoch [3/10], Iteration [16000/20000], Training Loss: 0.5360\n",
      "Epoch [3/10], Iteration [16100/20000], Training Loss: 0.5482\n",
      "Epoch [3/10], Iteration [16200/20000], Training Loss: 0.5662\n",
      "Epoch [3/10], Iteration [16300/20000], Training Loss: 0.5102\n",
      "Epoch [3/10], Iteration [16400/20000], Training Loss: 0.5582\n",
      "Epoch [3/10], Iteration [16500/20000], Training Loss: 0.5487\n",
      "Epoch [3/10], Iteration [16600/20000], Training Loss: 0.6242\n",
      "Epoch [3/10], Iteration [16700/20000], Training Loss: 0.4484\n",
      "Epoch [3/10], Iteration [16800/20000], Training Loss: 0.4473\n",
      "Epoch [3/10], Iteration [16900/20000], Training Loss: 0.4947\n",
      "Epoch [3/10], Iteration [17000/20000], Training Loss: 0.4568\n",
      "Epoch [3/10], Iteration [17100/20000], Training Loss: 0.5815\n",
      "Epoch [3/10], Iteration [17200/20000], Training Loss: 0.5678\n",
      "Epoch [3/10], Iteration [17300/20000], Training Loss: 0.4978\n",
      "Epoch [3/10], Iteration [17400/20000], Training Loss: 0.4416\n",
      "Epoch [3/10], Iteration [17500/20000], Training Loss: 0.5534\n",
      "Epoch [3/10], Iteration [17600/20000], Training Loss: 0.4599\n",
      "Epoch [3/10], Iteration [17700/20000], Training Loss: 0.4891\n",
      "Epoch [3/10], Iteration [17800/20000], Training Loss: 0.5581\n",
      "Epoch [3/10], Iteration [17900/20000], Training Loss: 0.6145\n",
      "Epoch [3/10], Iteration [18000/20000], Training Loss: 0.5334\n",
      "Epoch [3/10], Iteration [18100/20000], Training Loss: 0.5515\n",
      "Epoch [3/10], Iteration [18200/20000], Training Loss: 0.4684\n",
      "Epoch [3/10], Iteration [18300/20000], Training Loss: 0.5601\n",
      "Epoch [3/10], Iteration [18400/20000], Training Loss: 0.5411\n",
      "Epoch [3/10], Iteration [18500/20000], Training Loss: 0.5589\n",
      "Epoch [3/10], Iteration [18600/20000], Training Loss: 0.4768\n",
      "Epoch [3/10], Iteration [18700/20000], Training Loss: 0.5405\n",
      "Epoch [3/10], Iteration [18800/20000], Training Loss: 0.5370\n",
      "Epoch [3/10], Iteration [18900/20000], Training Loss: 0.5079\n",
      "Epoch [3/10], Iteration [19000/20000], Training Loss: 0.5872\n",
      "Epoch [3/10], Iteration [19100/20000], Training Loss: 0.4909\n",
      "Epoch [3/10], Iteration [19200/20000], Training Loss: 0.5824\n",
      "Epoch [3/10], Iteration [19300/20000], Training Loss: 0.4859\n",
      "Epoch [3/10], Iteration [19400/20000], Training Loss: 0.4475\n",
      "Epoch [3/10], Iteration [19500/20000], Training Loss: 0.5404\n",
      "Epoch [3/10], Iteration [19600/20000], Training Loss: 0.5689\n",
      "Epoch [3/10], Iteration [19700/20000], Training Loss: 0.6532\n",
      "Epoch [3/10], Iteration [19800/20000], Training Loss: 0.5701\n",
      "Epoch [3/10], Iteration [19900/20000], Training Loss: 0.5737\n",
      "Epoch [3/10], Iteration [20000/20000], Training Loss: 0.4651\n",
      "Epoch [3/10], Average Training Loss: 0.5129\n",
      "Epoch [4/10], Iteration [100/20000], Training Loss: 0.5150\n",
      "Epoch [4/10], Iteration [200/20000], Training Loss: 0.4566\n",
      "Epoch [4/10], Iteration [300/20000], Training Loss: 0.5238\n",
      "Epoch [4/10], Iteration [400/20000], Training Loss: 0.5225\n",
      "Epoch [4/10], Iteration [500/20000], Training Loss: 0.3462\n",
      "Epoch [4/10], Iteration [600/20000], Training Loss: 0.5201\n",
      "Epoch [4/10], Iteration [700/20000], Training Loss: 0.4822\n",
      "Epoch [4/10], Iteration [800/20000], Training Loss: 0.4504\n",
      "Epoch [4/10], Iteration [900/20000], Training Loss: 0.4590\n",
      "Epoch [4/10], Iteration [1000/20000], Training Loss: 0.4699\n",
      "Epoch [4/10], Iteration [1100/20000], Training Loss: 0.5277\n",
      "Epoch [4/10], Iteration [1200/20000], Training Loss: 0.4540\n",
      "Epoch [4/10], Iteration [1300/20000], Training Loss: 0.5679\n",
      "Epoch [4/10], Iteration [1400/20000], Training Loss: 0.4418\n",
      "Epoch [4/10], Iteration [1500/20000], Training Loss: 0.6205\n",
      "Epoch [4/10], Iteration [1600/20000], Training Loss: 0.4579\n",
      "Epoch [4/10], Iteration [1700/20000], Training Loss: 0.4712\n",
      "Epoch [4/10], Iteration [1800/20000], Training Loss: 0.4012\n",
      "Epoch [4/10], Iteration [1900/20000], Training Loss: 0.5325\n",
      "Epoch [4/10], Iteration [2000/20000], Training Loss: 0.5911\n",
      "Epoch [4/10], Iteration [2100/20000], Training Loss: 0.4946\n",
      "Epoch [4/10], Iteration [2200/20000], Training Loss: 0.5135\n",
      "Epoch [4/10], Iteration [2300/20000], Training Loss: 0.5273\n",
      "Epoch [4/10], Iteration [2400/20000], Training Loss: 0.4341\n",
      "Epoch [4/10], Iteration [2500/20000], Training Loss: 0.5359\n",
      "Epoch [4/10], Iteration [2600/20000], Training Loss: 0.5116\n",
      "Epoch [4/10], Iteration [2700/20000], Training Loss: 0.5121\n",
      "Epoch [4/10], Iteration [2800/20000], Training Loss: 0.5381\n",
      "Epoch [4/10], Iteration [2900/20000], Training Loss: 0.4446\n",
      "Epoch [4/10], Iteration [3000/20000], Training Loss: 0.5809\n",
      "Epoch [4/10], Iteration [3100/20000], Training Loss: 0.6278\n",
      "Epoch [4/10], Iteration [3200/20000], Training Loss: 0.3665\n",
      "Epoch [4/10], Iteration [3300/20000], Training Loss: 0.4859\n",
      "Epoch [4/10], Iteration [3400/20000], Training Loss: 0.4362\n",
      "Epoch [4/10], Iteration [3500/20000], Training Loss: 0.5570\n",
      "Epoch [4/10], Iteration [3600/20000], Training Loss: 0.4454\n",
      "Epoch [4/10], Iteration [3700/20000], Training Loss: 0.4855\n",
      "Epoch [4/10], Iteration [3800/20000], Training Loss: 0.5258\n",
      "Epoch [4/10], Iteration [3900/20000], Training Loss: 0.4552\n",
      "Epoch [4/10], Iteration [4000/20000], Training Loss: 0.5242\n",
      "Epoch [4/10], Iteration [4100/20000], Training Loss: 0.3884\n",
      "Epoch [4/10], Iteration [4200/20000], Training Loss: 0.5943\n",
      "Epoch [4/10], Iteration [4300/20000], Training Loss: 0.4802\n",
      "Epoch [4/10], Iteration [4400/20000], Training Loss: 0.4666\n",
      "Epoch [4/10], Iteration [4500/20000], Training Loss: 0.4505\n",
      "Epoch [4/10], Iteration [4600/20000], Training Loss: 0.4974\n",
      "Epoch [4/10], Iteration [4700/20000], Training Loss: 0.5015\n",
      "Epoch [4/10], Iteration [4800/20000], Training Loss: 0.4970\n",
      "Epoch [4/10], Iteration [4900/20000], Training Loss: 0.6184\n",
      "Epoch [4/10], Iteration [5000/20000], Training Loss: 0.4893\n",
      "Epoch [4/10], Iteration [5100/20000], Training Loss: 0.5958\n",
      "Epoch [4/10], Iteration [5200/20000], Training Loss: 0.4935\n",
      "Epoch [4/10], Iteration [5300/20000], Training Loss: 0.4317\n",
      "Epoch [4/10], Iteration [5400/20000], Training Loss: 0.5679\n",
      "Epoch [4/10], Iteration [5500/20000], Training Loss: 0.5552\n",
      "Epoch [4/10], Iteration [5600/20000], Training Loss: 0.5069\n",
      "Epoch [4/10], Iteration [5700/20000], Training Loss: 0.5614\n",
      "Epoch [4/10], Iteration [5800/20000], Training Loss: 0.6102\n",
      "Epoch [4/10], Iteration [5900/20000], Training Loss: 0.5468\n",
      "Epoch [4/10], Iteration [6000/20000], Training Loss: 0.4223\n",
      "Epoch [4/10], Iteration [6100/20000], Training Loss: 0.5661\n",
      "Epoch [4/10], Iteration [6200/20000], Training Loss: 0.5715\n",
      "Epoch [4/10], Iteration [6300/20000], Training Loss: 0.5221\n",
      "Epoch [4/10], Iteration [6400/20000], Training Loss: 0.6102\n",
      "Epoch [4/10], Iteration [6500/20000], Training Loss: 0.5108\n",
      "Epoch [4/10], Iteration [6600/20000], Training Loss: 0.4961\n",
      "Epoch [4/10], Iteration [6700/20000], Training Loss: 0.5009\n",
      "Epoch [4/10], Iteration [6800/20000], Training Loss: 0.4470\n",
      "Epoch [4/10], Iteration [6900/20000], Training Loss: 0.4671\n",
      "Epoch [4/10], Iteration [7000/20000], Training Loss: 0.5425\n",
      "Epoch [4/10], Iteration [7100/20000], Training Loss: 0.5174\n",
      "Epoch [4/10], Iteration [7200/20000], Training Loss: 0.4638\n",
      "Epoch [4/10], Iteration [7300/20000], Training Loss: 0.5225\n",
      "Epoch [4/10], Iteration [7400/20000], Training Loss: 0.5104\n",
      "Epoch [4/10], Iteration [7500/20000], Training Loss: 0.5854\n",
      "Epoch [4/10], Iteration [7600/20000], Training Loss: 0.3990\n",
      "Epoch [4/10], Iteration [7700/20000], Training Loss: 0.4502\n",
      "Epoch [4/10], Iteration [7800/20000], Training Loss: 0.5123\n",
      "Epoch [4/10], Iteration [7900/20000], Training Loss: 0.5920\n",
      "Epoch [4/10], Iteration [8000/20000], Training Loss: 0.5036\n",
      "Epoch [4/10], Iteration [8100/20000], Training Loss: 0.4774\n",
      "Epoch [4/10], Iteration [8200/20000], Training Loss: 0.4249\n",
      "Epoch [4/10], Iteration [8300/20000], Training Loss: 0.5961\n",
      "Epoch [4/10], Iteration [8400/20000], Training Loss: 0.5797\n",
      "Epoch [4/10], Iteration [8500/20000], Training Loss: 0.4449\n",
      "Epoch [4/10], Iteration [8600/20000], Training Loss: 0.4960\n",
      "Epoch [4/10], Iteration [8700/20000], Training Loss: 0.4169\n",
      "Epoch [4/10], Iteration [8800/20000], Training Loss: 0.5306\n",
      "Epoch [4/10], Iteration [8900/20000], Training Loss: 0.4402\n",
      "Epoch [4/10], Iteration [9000/20000], Training Loss: 0.5978\n",
      "Epoch [4/10], Iteration [9100/20000], Training Loss: 0.4465\n",
      "Epoch [4/10], Iteration [9200/20000], Training Loss: 0.4989\n",
      "Epoch [4/10], Iteration [9300/20000], Training Loss: 0.4294\n",
      "Epoch [4/10], Iteration [9400/20000], Training Loss: 0.4879\n",
      "Epoch [4/10], Iteration [9500/20000], Training Loss: 0.5764\n",
      "Epoch [4/10], Iteration [9600/20000], Training Loss: 0.4958\n",
      "Epoch [4/10], Iteration [9700/20000], Training Loss: 0.4720\n",
      "Epoch [4/10], Iteration [9800/20000], Training Loss: 0.5099\n",
      "Epoch [4/10], Iteration [9900/20000], Training Loss: 0.6191\n",
      "Epoch [4/10], Iteration [10000/20000], Training Loss: 0.3809\n",
      "Epoch [4/10], Iteration [10100/20000], Training Loss: 0.5713\n",
      "Epoch [4/10], Iteration [10200/20000], Training Loss: 0.5940\n",
      "Epoch [4/10], Iteration [10300/20000], Training Loss: 0.5241\n",
      "Epoch [4/10], Iteration [10400/20000], Training Loss: 0.5332\n",
      "Epoch [4/10], Iteration [10500/20000], Training Loss: 0.6043\n",
      "Epoch [4/10], Iteration [10600/20000], Training Loss: 0.4549\n",
      "Epoch [4/10], Iteration [10700/20000], Training Loss: 0.4754\n",
      "Epoch [4/10], Iteration [10800/20000], Training Loss: 0.5412\n",
      "Epoch [4/10], Iteration [10900/20000], Training Loss: 0.4312\n",
      "Epoch [4/10], Iteration [11000/20000], Training Loss: 0.4753\n",
      "Epoch [4/10], Iteration [11100/20000], Training Loss: 0.3980\n",
      "Epoch [4/10], Iteration [11200/20000], Training Loss: 0.4938\n",
      "Epoch [4/10], Iteration [11300/20000], Training Loss: 0.5080\n",
      "Epoch [4/10], Iteration [11400/20000], Training Loss: 0.5508\n",
      "Epoch [4/10], Iteration [11500/20000], Training Loss: 0.5212\n",
      "Epoch [4/10], Iteration [11600/20000], Training Loss: 0.5018\n",
      "Epoch [4/10], Iteration [11700/20000], Training Loss: 0.5568\n",
      "Epoch [4/10], Iteration [11800/20000], Training Loss: 0.4551\n",
      "Epoch [4/10], Iteration [11900/20000], Training Loss: 0.5039\n",
      "Epoch [4/10], Iteration [12000/20000], Training Loss: 0.5308\n",
      "Epoch [4/10], Iteration [12100/20000], Training Loss: 0.4312\n",
      "Epoch [4/10], Iteration [12200/20000], Training Loss: 0.4304\n",
      "Epoch [4/10], Iteration [12300/20000], Training Loss: 0.6068\n",
      "Epoch [4/10], Iteration [12400/20000], Training Loss: 0.4956\n",
      "Epoch [4/10], Iteration [12500/20000], Training Loss: 0.5902\n",
      "Epoch [4/10], Iteration [12600/20000], Training Loss: 0.5175\n",
      "Epoch [4/10], Iteration [12700/20000], Training Loss: 0.4692\n",
      "Epoch [4/10], Iteration [12800/20000], Training Loss: 0.4849\n",
      "Epoch [4/10], Iteration [12900/20000], Training Loss: 0.4664\n",
      "Epoch [4/10], Iteration [13000/20000], Training Loss: 0.5868\n",
      "Epoch [4/10], Iteration [13100/20000], Training Loss: 0.4812\n",
      "Epoch [4/10], Iteration [13200/20000], Training Loss: 0.5652\n",
      "Epoch [4/10], Iteration [13300/20000], Training Loss: 0.4894\n",
      "Epoch [4/10], Iteration [13400/20000], Training Loss: 0.4664\n",
      "Epoch [4/10], Iteration [13500/20000], Training Loss: 0.5338\n",
      "Epoch [4/10], Iteration [13600/20000], Training Loss: 0.4882\n",
      "Epoch [4/10], Iteration [13700/20000], Training Loss: 0.5873\n",
      "Epoch [4/10], Iteration [13800/20000], Training Loss: 0.5136\n",
      "Epoch [4/10], Iteration [13900/20000], Training Loss: 0.5754\n",
      "Epoch [4/10], Iteration [14000/20000], Training Loss: 0.6354\n",
      "Epoch [4/10], Iteration [14100/20000], Training Loss: 0.5639\n",
      "Epoch [4/10], Iteration [14200/20000], Training Loss: 0.4903\n",
      "Epoch [4/10], Iteration [14300/20000], Training Loss: 0.4752\n",
      "Epoch [4/10], Iteration [14400/20000], Training Loss: 0.5348\n",
      "Epoch [4/10], Iteration [14500/20000], Training Loss: 0.5015\n",
      "Epoch [4/10], Iteration [14600/20000], Training Loss: 0.4139\n",
      "Epoch [4/10], Iteration [14700/20000], Training Loss: 0.4556\n",
      "Epoch [4/10], Iteration [14800/20000], Training Loss: 0.5253\n",
      "Epoch [4/10], Iteration [14900/20000], Training Loss: 0.4559\n",
      "Epoch [4/10], Iteration [15000/20000], Training Loss: 0.4551\n",
      "Epoch [4/10], Iteration [15100/20000], Training Loss: 0.5483\n",
      "Epoch [4/10], Iteration [15200/20000], Training Loss: 0.5547\n",
      "Epoch [4/10], Iteration [15300/20000], Training Loss: 0.5931\n",
      "Epoch [4/10], Iteration [15400/20000], Training Loss: 0.4756\n",
      "Epoch [4/10], Iteration [15500/20000], Training Loss: 0.5043\n",
      "Epoch [4/10], Iteration [15600/20000], Training Loss: 0.4545\n",
      "Epoch [4/10], Iteration [15700/20000], Training Loss: 0.4305\n",
      "Epoch [4/10], Iteration [15800/20000], Training Loss: 0.5018\n",
      "Epoch [4/10], Iteration [15900/20000], Training Loss: 0.5916\n",
      "Epoch [4/10], Iteration [16000/20000], Training Loss: 0.4792\n",
      "Epoch [4/10], Iteration [16100/20000], Training Loss: 0.5835\n",
      "Epoch [4/10], Iteration [16200/20000], Training Loss: 0.5375\n",
      "Epoch [4/10], Iteration [16300/20000], Training Loss: 0.4689\n",
      "Epoch [4/10], Iteration [16400/20000], Training Loss: 0.4580\n",
      "Epoch [4/10], Iteration [16500/20000], Training Loss: 0.4815\n",
      "Epoch [4/10], Iteration [16600/20000], Training Loss: 0.6649\n",
      "Epoch [4/10], Iteration [16700/20000], Training Loss: 0.4800\n",
      "Epoch [4/10], Iteration [16800/20000], Training Loss: 0.6989\n",
      "Epoch [4/10], Iteration [16900/20000], Training Loss: 0.4942\n",
      "Epoch [4/10], Iteration [17000/20000], Training Loss: 0.5432\n",
      "Epoch [4/10], Iteration [17100/20000], Training Loss: 0.4626\n",
      "Epoch [4/10], Iteration [17200/20000], Training Loss: 0.5888\n",
      "Epoch [4/10], Iteration [17300/20000], Training Loss: 0.4885\n",
      "Epoch [4/10], Iteration [17400/20000], Training Loss: 0.4626\n",
      "Epoch [4/10], Iteration [17500/20000], Training Loss: 0.4113\n",
      "Epoch [4/10], Iteration [17600/20000], Training Loss: 0.5053\n",
      "Epoch [4/10], Iteration [17700/20000], Training Loss: 0.4966\n",
      "Epoch [4/10], Iteration [17800/20000], Training Loss: 0.4796\n",
      "Epoch [4/10], Iteration [17900/20000], Training Loss: 0.5526\n",
      "Epoch [4/10], Iteration [18000/20000], Training Loss: 0.5334\n",
      "Epoch [4/10], Iteration [18100/20000], Training Loss: 0.5505\n",
      "Epoch [4/10], Iteration [18200/20000], Training Loss: 0.5813\n",
      "Epoch [4/10], Iteration [18300/20000], Training Loss: 0.4260\n",
      "Epoch [4/10], Iteration [18400/20000], Training Loss: 0.5361\n",
      "Epoch [4/10], Iteration [18500/20000], Training Loss: 0.4715\n",
      "Epoch [4/10], Iteration [18600/20000], Training Loss: 0.5273\n",
      "Epoch [4/10], Iteration [18700/20000], Training Loss: 0.6266\n",
      "Epoch [4/10], Iteration [18800/20000], Training Loss: 0.4841\n",
      "Epoch [4/10], Iteration [18900/20000], Training Loss: 0.5667\n",
      "Epoch [4/10], Iteration [19000/20000], Training Loss: 0.5736\n",
      "Epoch [4/10], Iteration [19100/20000], Training Loss: 0.5658\n",
      "Epoch [4/10], Iteration [19200/20000], Training Loss: 0.4881\n",
      "Epoch [4/10], Iteration [19300/20000], Training Loss: 0.5297\n",
      "Epoch [4/10], Iteration [19400/20000], Training Loss: 0.4812\n",
      "Epoch [4/10], Iteration [19500/20000], Training Loss: 0.4591\n",
      "Epoch [4/10], Iteration [19600/20000], Training Loss: 0.5791\n",
      "Epoch [4/10], Iteration [19700/20000], Training Loss: 0.6140\n",
      "Epoch [4/10], Iteration [19800/20000], Training Loss: 0.4732\n",
      "Epoch [4/10], Iteration [19900/20000], Training Loss: 0.5020\n",
      "Epoch [4/10], Iteration [20000/20000], Training Loss: 0.4269\n",
      "Epoch [4/10], Average Training Loss: 0.5115\n",
      "Epoch [5/10], Iteration [100/20000], Training Loss: 0.5691\n",
      "Epoch [5/10], Iteration [200/20000], Training Loss: 0.4419\n",
      "Epoch [5/10], Iteration [300/20000], Training Loss: 0.6012\n",
      "Epoch [5/10], Iteration [400/20000], Training Loss: 0.5395\n",
      "Epoch [5/10], Iteration [500/20000], Training Loss: 0.4906\n",
      "Epoch [5/10], Iteration [600/20000], Training Loss: 0.4379\n",
      "Epoch [5/10], Iteration [700/20000], Training Loss: 0.6225\n",
      "Epoch [5/10], Iteration [800/20000], Training Loss: 0.5926\n",
      "Epoch [5/10], Iteration [900/20000], Training Loss: 0.4650\n",
      "Epoch [5/10], Iteration [1000/20000], Training Loss: 0.5461\n",
      "Epoch [5/10], Iteration [1100/20000], Training Loss: 0.5226\n",
      "Epoch [5/10], Iteration [1200/20000], Training Loss: 0.5880\n",
      "Epoch [5/10], Iteration [1300/20000], Training Loss: 0.4801\n",
      "Epoch [5/10], Iteration [1400/20000], Training Loss: 0.4145\n",
      "Epoch [5/10], Iteration [1500/20000], Training Loss: 0.4860\n",
      "Epoch [5/10], Iteration [1600/20000], Training Loss: 0.5620\n",
      "Epoch [5/10], Iteration [1700/20000], Training Loss: 0.4506\n",
      "Epoch [5/10], Iteration [1800/20000], Training Loss: 0.4375\n",
      "Epoch [5/10], Iteration [1900/20000], Training Loss: 0.4854\n",
      "Epoch [5/10], Iteration [2000/20000], Training Loss: 0.4576\n",
      "Epoch [5/10], Iteration [2100/20000], Training Loss: 0.4912\n",
      "Epoch [5/10], Iteration [2200/20000], Training Loss: 0.5557\n",
      "Epoch [5/10], Iteration [2300/20000], Training Loss: 0.6225\n",
      "Epoch [5/10], Iteration [2400/20000], Training Loss: 0.5043\n",
      "Epoch [5/10], Iteration [2500/20000], Training Loss: 0.5104\n",
      "Epoch [5/10], Iteration [2600/20000], Training Loss: 0.4881\n",
      "Epoch [5/10], Iteration [2700/20000], Training Loss: 0.5693\n",
      "Epoch [5/10], Iteration [2800/20000], Training Loss: 0.5112\n",
      "Epoch [5/10], Iteration [2900/20000], Training Loss: 0.4005\n",
      "Epoch [5/10], Iteration [3000/20000], Training Loss: 0.4854\n",
      "Epoch [5/10], Iteration [3100/20000], Training Loss: 0.6444\n",
      "Epoch [5/10], Iteration [3200/20000], Training Loss: 0.4513\n",
      "Epoch [5/10], Iteration [3300/20000], Training Loss: 0.4679\n",
      "Epoch [5/10], Iteration [3400/20000], Training Loss: 0.4562\n",
      "Epoch [5/10], Iteration [3500/20000], Training Loss: 0.5616\n",
      "Epoch [5/10], Iteration [3600/20000], Training Loss: 0.4817\n",
      "Epoch [5/10], Iteration [3700/20000], Training Loss: 0.5611\n",
      "Epoch [5/10], Iteration [3800/20000], Training Loss: 0.5171\n",
      "Epoch [5/10], Iteration [3900/20000], Training Loss: 0.5603\n",
      "Epoch [5/10], Iteration [4000/20000], Training Loss: 0.5432\n",
      "Epoch [5/10], Iteration [4100/20000], Training Loss: 0.5135\n",
      "Epoch [5/10], Iteration [4200/20000], Training Loss: 0.5035\n",
      "Epoch [5/10], Iteration [4300/20000], Training Loss: 0.4777\n",
      "Epoch [5/10], Iteration [4400/20000], Training Loss: 0.5435\n",
      "Epoch [5/10], Iteration [4500/20000], Training Loss: 0.5275\n",
      "Epoch [5/10], Iteration [4600/20000], Training Loss: 0.3876\n",
      "Epoch [5/10], Iteration [4700/20000], Training Loss: 0.5323\n",
      "Epoch [5/10], Iteration [4800/20000], Training Loss: 0.5169\n",
      "Epoch [5/10], Iteration [4900/20000], Training Loss: 0.5009\n",
      "Epoch [5/10], Iteration [5000/20000], Training Loss: 0.4931\n",
      "Epoch [5/10], Iteration [5100/20000], Training Loss: 0.4924\n",
      "Epoch [5/10], Iteration [5200/20000], Training Loss: 0.4805\n",
      "Epoch [5/10], Iteration [5300/20000], Training Loss: 0.4544\n",
      "Epoch [5/10], Iteration [5400/20000], Training Loss: 0.4498\n",
      "Epoch [5/10], Iteration [5500/20000], Training Loss: 0.5017\n",
      "Epoch [5/10], Iteration [5600/20000], Training Loss: 0.4954\n",
      "Epoch [5/10], Iteration [5700/20000], Training Loss: 0.5727\n",
      "Epoch [5/10], Iteration [5800/20000], Training Loss: 0.4396\n",
      "Epoch [5/10], Iteration [5900/20000], Training Loss: 0.4067\n",
      "Epoch [5/10], Iteration [6000/20000], Training Loss: 0.4945\n",
      "Epoch [5/10], Iteration [6100/20000], Training Loss: 0.4392\n",
      "Epoch [5/10], Iteration [6200/20000], Training Loss: 0.4966\n",
      "Epoch [5/10], Iteration [6300/20000], Training Loss: 0.5274\n",
      "Epoch [5/10], Iteration [6400/20000], Training Loss: 0.5739\n",
      "Epoch [5/10], Iteration [6500/20000], Training Loss: 0.4520\n",
      "Epoch [5/10], Iteration [6600/20000], Training Loss: 0.6873\n",
      "Epoch [5/10], Iteration [6700/20000], Training Loss: 0.5722\n",
      "Epoch [5/10], Iteration [6800/20000], Training Loss: 0.4182\n",
      "Epoch [5/10], Iteration [6900/20000], Training Loss: 0.5252\n",
      "Epoch [5/10], Iteration [7000/20000], Training Loss: 0.4489\n",
      "Epoch [5/10], Iteration [7100/20000], Training Loss: 0.4323\n",
      "Epoch [5/10], Iteration [7200/20000], Training Loss: 0.5772\n",
      "Epoch [5/10], Iteration [7300/20000], Training Loss: 0.5198\n",
      "Epoch [5/10], Iteration [7400/20000], Training Loss: 0.5268\n",
      "Epoch [5/10], Iteration [7500/20000], Training Loss: 0.3890\n",
      "Epoch [5/10], Iteration [7600/20000], Training Loss: 0.5026\n",
      "Epoch [5/10], Iteration [7700/20000], Training Loss: 0.5702\n",
      "Epoch [5/10], Iteration [7800/20000], Training Loss: 0.5185\n",
      "Epoch [5/10], Iteration [7900/20000], Training Loss: 0.4476\n",
      "Epoch [5/10], Iteration [8000/20000], Training Loss: 0.4420\n",
      "Epoch [5/10], Iteration [8100/20000], Training Loss: 0.4928\n",
      "Epoch [5/10], Iteration [8200/20000], Training Loss: 0.5266\n",
      "Epoch [5/10], Iteration [8300/20000], Training Loss: 0.5022\n",
      "Epoch [5/10], Iteration [8400/20000], Training Loss: 0.4958\n",
      "Epoch [5/10], Iteration [8500/20000], Training Loss: 0.6113\n",
      "Epoch [5/10], Iteration [8600/20000], Training Loss: 0.5529\n",
      "Epoch [5/10], Iteration [8700/20000], Training Loss: 0.5352\n",
      "Epoch [5/10], Iteration [8800/20000], Training Loss: 0.4299\n",
      "Epoch [5/10], Iteration [8900/20000], Training Loss: 0.4598\n",
      "Epoch [5/10], Iteration [9000/20000], Training Loss: 0.5416\n",
      "Epoch [5/10], Iteration [9100/20000], Training Loss: 0.4417\n",
      "Epoch [5/10], Iteration [9200/20000], Training Loss: 0.4731\n",
      "Epoch [5/10], Iteration [9300/20000], Training Loss: 0.4889\n",
      "Epoch [5/10], Iteration [9400/20000], Training Loss: 0.4468\n",
      "Epoch [5/10], Iteration [9500/20000], Training Loss: 0.3247\n",
      "Epoch [5/10], Iteration [9600/20000], Training Loss: 0.3926\n",
      "Epoch [5/10], Iteration [9700/20000], Training Loss: 0.4970\n",
      "Epoch [5/10], Iteration [9800/20000], Training Loss: 0.4971\n",
      "Epoch [5/10], Iteration [9900/20000], Training Loss: 0.4525\n",
      "Epoch [5/10], Iteration [10000/20000], Training Loss: 0.4731\n",
      "Epoch [5/10], Iteration [10100/20000], Training Loss: 0.5756\n",
      "Epoch [5/10], Iteration [10200/20000], Training Loss: 0.4874\n",
      "Epoch [5/10], Iteration [10300/20000], Training Loss: 0.4010\n",
      "Epoch [5/10], Iteration [10400/20000], Training Loss: 0.4302\n",
      "Epoch [5/10], Iteration [10500/20000], Training Loss: 0.5108\n",
      "Epoch [5/10], Iteration [10600/20000], Training Loss: 0.5072\n",
      "Epoch [5/10], Iteration [10700/20000], Training Loss: 0.5495\n",
      "Epoch [5/10], Iteration [10800/20000], Training Loss: 0.4350\n",
      "Epoch [5/10], Iteration [10900/20000], Training Loss: 0.5211\n",
      "Epoch [5/10], Iteration [11000/20000], Training Loss: 0.5692\n",
      "Epoch [5/10], Iteration [11100/20000], Training Loss: 0.5440\n",
      "Epoch [5/10], Iteration [11200/20000], Training Loss: 0.4043\n",
      "Epoch [5/10], Iteration [11300/20000], Training Loss: 0.3718\n",
      "Epoch [5/10], Iteration [11400/20000], Training Loss: 0.5647\n",
      "Epoch [5/10], Iteration [11500/20000], Training Loss: 0.4631\n",
      "Epoch [5/10], Iteration [11600/20000], Training Loss: 0.5320\n",
      "Epoch [5/10], Iteration [11700/20000], Training Loss: 0.5484\n",
      "Epoch [5/10], Iteration [11800/20000], Training Loss: 0.5346\n",
      "Epoch [5/10], Iteration [11900/20000], Training Loss: 0.5044\n",
      "Epoch [5/10], Iteration [12000/20000], Training Loss: 0.4261\n",
      "Epoch [5/10], Iteration [12100/20000], Training Loss: 0.5334\n",
      "Epoch [5/10], Iteration [12200/20000], Training Loss: 0.4961\n",
      "Epoch [5/10], Iteration [12300/20000], Training Loss: 0.6115\n",
      "Epoch [5/10], Iteration [12400/20000], Training Loss: 0.4990\n",
      "Epoch [5/10], Iteration [12500/20000], Training Loss: 0.5818\n",
      "Epoch [5/10], Iteration [12600/20000], Training Loss: 0.3698\n",
      "Epoch [5/10], Iteration [12700/20000], Training Loss: 0.5871\n",
      "Epoch [5/10], Iteration [12800/20000], Training Loss: 0.6131\n",
      "Epoch [5/10], Iteration [12900/20000], Training Loss: 0.6890\n",
      "Epoch [5/10], Iteration [13000/20000], Training Loss: 0.6302\n",
      "Epoch [5/10], Iteration [13100/20000], Training Loss: 0.4298\n",
      "Epoch [5/10], Iteration [13200/20000], Training Loss: 0.6053\n",
      "Epoch [5/10], Iteration [13300/20000], Training Loss: 0.5911\n",
      "Epoch [5/10], Iteration [13400/20000], Training Loss: 0.5482\n",
      "Epoch [5/10], Iteration [13500/20000], Training Loss: 0.5687\n",
      "Epoch [5/10], Iteration [13600/20000], Training Loss: 0.6619\n",
      "Epoch [5/10], Iteration [13700/20000], Training Loss: 0.5183\n",
      "Epoch [5/10], Iteration [13800/20000], Training Loss: 0.5353\n",
      "Epoch [5/10], Iteration [13900/20000], Training Loss: 0.5574\n",
      "Epoch [5/10], Iteration [14000/20000], Training Loss: 0.5170\n",
      "Epoch [5/10], Iteration [14100/20000], Training Loss: 0.5321\n",
      "Epoch [5/10], Iteration [14200/20000], Training Loss: 0.5234\n",
      "Epoch [5/10], Iteration [14300/20000], Training Loss: 0.4947\n",
      "Epoch [5/10], Iteration [14400/20000], Training Loss: 0.4763\n",
      "Epoch [5/10], Iteration [14500/20000], Training Loss: 0.4112\n",
      "Epoch [5/10], Iteration [14600/20000], Training Loss: 0.4795\n",
      "Epoch [5/10], Iteration [14700/20000], Training Loss: 0.5019\n",
      "Epoch [5/10], Iteration [14800/20000], Training Loss: 0.5179\n",
      "Epoch [5/10], Iteration [14900/20000], Training Loss: 0.4133\n",
      "Epoch [5/10], Iteration [15000/20000], Training Loss: 0.3573\n",
      "Epoch [5/10], Iteration [15100/20000], Training Loss: 0.5599\n",
      "Epoch [5/10], Iteration [15200/20000], Training Loss: 0.5593\n",
      "Epoch [5/10], Iteration [15300/20000], Training Loss: 0.4750\n",
      "Epoch [5/10], Iteration [15400/20000], Training Loss: 0.4743\n",
      "Epoch [5/10], Iteration [15500/20000], Training Loss: 0.4775\n",
      "Epoch [5/10], Iteration [15600/20000], Training Loss: 0.5591\n",
      "Epoch [5/10], Iteration [15700/20000], Training Loss: 0.5451\n",
      "Epoch [5/10], Iteration [15800/20000], Training Loss: 0.4632\n",
      "Epoch [5/10], Iteration [15900/20000], Training Loss: 0.4230\n",
      "Epoch [5/10], Iteration [16000/20000], Training Loss: 0.5327\n",
      "Epoch [5/10], Iteration [16100/20000], Training Loss: 0.5053\n",
      "Epoch [5/10], Iteration [16200/20000], Training Loss: 0.5256\n",
      "Epoch [5/10], Iteration [16300/20000], Training Loss: 0.4851\n",
      "Epoch [5/10], Iteration [16400/20000], Training Loss: 0.4452\n",
      "Epoch [5/10], Iteration [16500/20000], Training Loss: 0.4039\n",
      "Epoch [5/10], Iteration [16600/20000], Training Loss: 0.4652\n",
      "Epoch [5/10], Iteration [16700/20000], Training Loss: 0.4713\n",
      "Epoch [5/10], Iteration [16800/20000], Training Loss: 0.5301\n",
      "Epoch [5/10], Iteration [16900/20000], Training Loss: 0.3855\n",
      "Epoch [5/10], Iteration [17000/20000], Training Loss: 0.5454\n",
      "Epoch [5/10], Iteration [17100/20000], Training Loss: 0.4613\n",
      "Epoch [5/10], Iteration [17200/20000], Training Loss: 0.4994\n",
      "Epoch [5/10], Iteration [17300/20000], Training Loss: 0.4788\n",
      "Epoch [5/10], Iteration [17400/20000], Training Loss: 0.4262\n",
      "Epoch [5/10], Iteration [17500/20000], Training Loss: 0.4521\n",
      "Epoch [5/10], Iteration [17600/20000], Training Loss: 0.4524\n",
      "Epoch [5/10], Iteration [17700/20000], Training Loss: 0.4175\n",
      "Epoch [5/10], Iteration [17800/20000], Training Loss: 0.5234\n",
      "Epoch [5/10], Iteration [17900/20000], Training Loss: 0.5268\n",
      "Epoch [5/10], Iteration [18000/20000], Training Loss: 0.4788\n",
      "Epoch [5/10], Iteration [18100/20000], Training Loss: 0.4398\n",
      "Epoch [5/10], Iteration [18200/20000], Training Loss: 0.4009\n",
      "Epoch [5/10], Iteration [18300/20000], Training Loss: 0.3998\n",
      "Epoch [5/10], Iteration [18400/20000], Training Loss: 0.4652\n",
      "Epoch [5/10], Iteration [18500/20000], Training Loss: 0.5855\n",
      "Epoch [5/10], Iteration [18600/20000], Training Loss: 0.4638\n",
      "Epoch [5/10], Iteration [18700/20000], Training Loss: 0.4648\n",
      "Epoch [5/10], Iteration [18800/20000], Training Loss: 0.4496\n",
      "Epoch [5/10], Iteration [18900/20000], Training Loss: 0.4151\n",
      "Epoch [5/10], Iteration [19000/20000], Training Loss: 0.4629\n",
      "Epoch [5/10], Iteration [19100/20000], Training Loss: 0.5476\n",
      "Epoch [5/10], Iteration [19200/20000], Training Loss: 0.5835\n",
      "Epoch [5/10], Iteration [19300/20000], Training Loss: 0.4299\n",
      "Epoch [5/10], Iteration [19400/20000], Training Loss: 0.5418\n",
      "Epoch [5/10], Iteration [19500/20000], Training Loss: 0.5508\n",
      "Epoch [5/10], Iteration [19600/20000], Training Loss: 0.4994\n",
      "Epoch [5/10], Iteration [19700/20000], Training Loss: 0.5716\n",
      "Epoch [5/10], Iteration [19800/20000], Training Loss: 0.5851\n",
      "Epoch [5/10], Iteration [19900/20000], Training Loss: 0.5740\n",
      "Epoch [5/10], Iteration [20000/20000], Training Loss: 0.4349\n",
      "Epoch [5/10], Average Training Loss: 0.5102\n",
      "Epoch [6/10], Iteration [100/20000], Training Loss: 0.5357\n",
      "Epoch [6/10], Iteration [200/20000], Training Loss: 0.5403\n",
      "Epoch [6/10], Iteration [300/20000], Training Loss: 0.4746\n",
      "Epoch [6/10], Iteration [400/20000], Training Loss: 0.5359\n",
      "Epoch [6/10], Iteration [500/20000], Training Loss: 0.6144\n",
      "Epoch [6/10], Iteration [600/20000], Training Loss: 0.4876\n",
      "Epoch [6/10], Iteration [700/20000], Training Loss: 0.4579\n",
      "Epoch [6/10], Iteration [800/20000], Training Loss: 0.5966\n",
      "Epoch [6/10], Iteration [900/20000], Training Loss: 0.4916\n",
      "Epoch [6/10], Iteration [1000/20000], Training Loss: 0.4461\n",
      "Epoch [6/10], Iteration [1100/20000], Training Loss: 0.3838\n",
      "Epoch [6/10], Iteration [1200/20000], Training Loss: 0.5141\n",
      "Epoch [6/10], Iteration [1300/20000], Training Loss: 0.5686\n",
      "Epoch [6/10], Iteration [1400/20000], Training Loss: 0.4938\n",
      "Epoch [6/10], Iteration [1500/20000], Training Loss: 0.4854\n",
      "Epoch [6/10], Iteration [1600/20000], Training Loss: 0.5130\n",
      "Epoch [6/10], Iteration [1700/20000], Training Loss: 0.5569\n",
      "Epoch [6/10], Iteration [1800/20000], Training Loss: 0.6136\n",
      "Epoch [6/10], Iteration [1900/20000], Training Loss: 0.6581\n",
      "Epoch [6/10], Iteration [2000/20000], Training Loss: 0.4358\n",
      "Epoch [6/10], Iteration [2100/20000], Training Loss: 0.4162\n",
      "Epoch [6/10], Iteration [2200/20000], Training Loss: 0.4866\n",
      "Epoch [6/10], Iteration [2300/20000], Training Loss: 0.4348\n",
      "Epoch [6/10], Iteration [2400/20000], Training Loss: 0.5788\n",
      "Epoch [6/10], Iteration [2500/20000], Training Loss: 0.4278\n",
      "Epoch [6/10], Iteration [2600/20000], Training Loss: 0.5618\n",
      "Epoch [6/10], Iteration [2700/20000], Training Loss: 0.3985\n",
      "Epoch [6/10], Iteration [2800/20000], Training Loss: 0.4776\n",
      "Epoch [6/10], Iteration [2900/20000], Training Loss: 0.4538\n",
      "Epoch [6/10], Iteration [3000/20000], Training Loss: 0.5038\n",
      "Epoch [6/10], Iteration [3100/20000], Training Loss: 0.5183\n",
      "Epoch [6/10], Iteration [3200/20000], Training Loss: 0.4668\n",
      "Epoch [6/10], Iteration [3300/20000], Training Loss: 0.4933\n",
      "Epoch [6/10], Iteration [3400/20000], Training Loss: 0.5362\n",
      "Epoch [6/10], Iteration [3500/20000], Training Loss: 0.4719\n",
      "Epoch [6/10], Iteration [3600/20000], Training Loss: 0.5195\n",
      "Epoch [6/10], Iteration [3700/20000], Training Loss: 0.5510\n",
      "Epoch [6/10], Iteration [3800/20000], Training Loss: 0.5211\n",
      "Epoch [6/10], Iteration [3900/20000], Training Loss: 0.5121\n",
      "Epoch [6/10], Iteration [4000/20000], Training Loss: 0.5072\n",
      "Epoch [6/10], Iteration [4100/20000], Training Loss: 0.5136\n",
      "Epoch [6/10], Iteration [4200/20000], Training Loss: 0.4851\n",
      "Epoch [6/10], Iteration [4300/20000], Training Loss: 0.4404\n",
      "Epoch [6/10], Iteration [4400/20000], Training Loss: 0.5008\n",
      "Epoch [6/10], Iteration [4500/20000], Training Loss: 0.3920\n",
      "Epoch [6/10], Iteration [4600/20000], Training Loss: 0.4936\n",
      "Epoch [6/10], Iteration [4700/20000], Training Loss: 0.5786\n",
      "Epoch [6/10], Iteration [4800/20000], Training Loss: 0.6533\n",
      "Epoch [6/10], Iteration [4900/20000], Training Loss: 0.4778\n",
      "Epoch [6/10], Iteration [5000/20000], Training Loss: 0.5200\n",
      "Epoch [6/10], Iteration [5100/20000], Training Loss: 0.5078\n",
      "Epoch [6/10], Iteration [5200/20000], Training Loss: 0.4156\n",
      "Epoch [6/10], Iteration [5300/20000], Training Loss: 0.5150\n",
      "Epoch [6/10], Iteration [5400/20000], Training Loss: 0.4503\n",
      "Epoch [6/10], Iteration [5500/20000], Training Loss: 0.5530\n",
      "Epoch [6/10], Iteration [5600/20000], Training Loss: 0.3736\n",
      "Epoch [6/10], Iteration [5700/20000], Training Loss: 0.5830\n",
      "Epoch [6/10], Iteration [5800/20000], Training Loss: 0.4461\n",
      "Epoch [6/10], Iteration [5900/20000], Training Loss: 0.4363\n",
      "Epoch [6/10], Iteration [6000/20000], Training Loss: 0.4616\n",
      "Epoch [6/10], Iteration [6100/20000], Training Loss: 0.5228\n",
      "Epoch [6/10], Iteration [6200/20000], Training Loss: 0.5277\n",
      "Epoch [6/10], Iteration [6300/20000], Training Loss: 0.5473\n",
      "Epoch [6/10], Iteration [6400/20000], Training Loss: 0.5069\n",
      "Epoch [6/10], Iteration [6500/20000], Training Loss: 0.4115\n",
      "Epoch [6/10], Iteration [6600/20000], Training Loss: 0.5129\n",
      "Epoch [6/10], Iteration [6700/20000], Training Loss: 0.4781\n",
      "Epoch [6/10], Iteration [6800/20000], Training Loss: 0.4485\n",
      "Epoch [6/10], Iteration [6900/20000], Training Loss: 0.5117\n",
      "Epoch [6/10], Iteration [7000/20000], Training Loss: 0.4621\n",
      "Epoch [6/10], Iteration [7100/20000], Training Loss: 0.4810\n",
      "Epoch [6/10], Iteration [7200/20000], Training Loss: 0.4627\n",
      "Epoch [6/10], Iteration [7300/20000], Training Loss: 0.5332\n",
      "Epoch [6/10], Iteration [7400/20000], Training Loss: 0.5625\n",
      "Epoch [6/10], Iteration [7500/20000], Training Loss: 0.4027\n",
      "Epoch [6/10], Iteration [7600/20000], Training Loss: 0.6042\n",
      "Epoch [6/10], Iteration [7700/20000], Training Loss: 0.5024\n",
      "Epoch [6/10], Iteration [7800/20000], Training Loss: 0.5656\n",
      "Epoch [6/10], Iteration [7900/20000], Training Loss: 0.5322\n",
      "Epoch [6/10], Iteration [8000/20000], Training Loss: 0.4056\n",
      "Epoch [6/10], Iteration [8100/20000], Training Loss: 0.4885\n",
      "Epoch [6/10], Iteration [8200/20000], Training Loss: 0.4383\n",
      "Epoch [6/10], Iteration [8300/20000], Training Loss: 0.5682\n",
      "Epoch [6/10], Iteration [8400/20000], Training Loss: 0.3972\n",
      "Epoch [6/10], Iteration [8500/20000], Training Loss: 0.5823\n",
      "Epoch [6/10], Iteration [8600/20000], Training Loss: 0.5595\n",
      "Epoch [6/10], Iteration [8700/20000], Training Loss: 0.5564\n",
      "Epoch [6/10], Iteration [8800/20000], Training Loss: 0.5799\n",
      "Epoch [6/10], Iteration [8900/20000], Training Loss: 0.4526\n",
      "Epoch [6/10], Iteration [9000/20000], Training Loss: 0.5856\n",
      "Epoch [6/10], Iteration [9100/20000], Training Loss: 0.5605\n",
      "Epoch [6/10], Iteration [9200/20000], Training Loss: 0.4018\n",
      "Epoch [6/10], Iteration [9300/20000], Training Loss: 0.4500\n",
      "Epoch [6/10], Iteration [9400/20000], Training Loss: 0.5614\n",
      "Epoch [6/10], Iteration [9500/20000], Training Loss: 0.4863\n",
      "Epoch [6/10], Iteration [9600/20000], Training Loss: 0.4362\n",
      "Epoch [6/10], Iteration [9700/20000], Training Loss: 0.3945\n",
      "Epoch [6/10], Iteration [9800/20000], Training Loss: 0.5116\n",
      "Epoch [6/10], Iteration [9900/20000], Training Loss: 0.4379\n",
      "Epoch [6/10], Iteration [10000/20000], Training Loss: 0.4941\n",
      "Epoch [6/10], Iteration [10100/20000], Training Loss: 0.5447\n",
      "Epoch [6/10], Iteration [10200/20000], Training Loss: 0.5219\n",
      "Epoch [6/10], Iteration [10300/20000], Training Loss: 0.6748\n",
      "Epoch [6/10], Iteration [10400/20000], Training Loss: 0.5402\n",
      "Epoch [6/10], Iteration [10500/20000], Training Loss: 0.5719\n",
      "Epoch [6/10], Iteration [10600/20000], Training Loss: 0.5157\n",
      "Epoch [6/10], Iteration [10700/20000], Training Loss: 0.5034\n",
      "Epoch [6/10], Iteration [10800/20000], Training Loss: 0.5254\n",
      "Epoch [6/10], Iteration [10900/20000], Training Loss: 0.4769\n",
      "Epoch [6/10], Iteration [11000/20000], Training Loss: 0.4251\n",
      "Epoch [6/10], Iteration [11100/20000], Training Loss: 0.4683\n",
      "Epoch [6/10], Iteration [11200/20000], Training Loss: 0.6782\n",
      "Epoch [6/10], Iteration [11300/20000], Training Loss: 0.4673\n",
      "Epoch [6/10], Iteration [11400/20000], Training Loss: 0.5258\n",
      "Epoch [6/10], Iteration [11500/20000], Training Loss: 0.4232\n",
      "Epoch [6/10], Iteration [11600/20000], Training Loss: 0.4983\n",
      "Epoch [6/10], Iteration [11700/20000], Training Loss: 0.5670\n",
      "Epoch [6/10], Iteration [11800/20000], Training Loss: 0.3757\n",
      "Epoch [6/10], Iteration [11900/20000], Training Loss: 0.5590\n",
      "Epoch [6/10], Iteration [12000/20000], Training Loss: 0.5661\n",
      "Epoch [6/10], Iteration [12100/20000], Training Loss: 0.4336\n",
      "Epoch [6/10], Iteration [12200/20000], Training Loss: 0.4650\n",
      "Epoch [6/10], Iteration [12300/20000], Training Loss: 0.5683\n",
      "Epoch [6/10], Iteration [12400/20000], Training Loss: 0.5599\n",
      "Epoch [6/10], Iteration [12500/20000], Training Loss: 0.5345\n",
      "Epoch [6/10], Iteration [12600/20000], Training Loss: 0.4995\n",
      "Epoch [6/10], Iteration [12700/20000], Training Loss: 0.6113\n",
      "Epoch [6/10], Iteration [12800/20000], Training Loss: 0.4984\n",
      "Epoch [6/10], Iteration [12900/20000], Training Loss: 0.4879\n",
      "Epoch [6/10], Iteration [13000/20000], Training Loss: 0.8641\n",
      "Epoch [6/10], Iteration [13100/20000], Training Loss: 0.5233\n",
      "Epoch [6/10], Iteration [13200/20000], Training Loss: 0.5916\n",
      "Epoch [6/10], Iteration [13300/20000], Training Loss: 0.5493\n",
      "Epoch [6/10], Iteration [13400/20000], Training Loss: 0.5033\n",
      "Epoch [6/10], Iteration [13500/20000], Training Loss: 0.5450\n",
      "Epoch [6/10], Iteration [13600/20000], Training Loss: 0.4545\n",
      "Epoch [6/10], Iteration [13700/20000], Training Loss: 0.4965\n",
      "Epoch [6/10], Iteration [13800/20000], Training Loss: 0.4971\n",
      "Epoch [6/10], Iteration [13900/20000], Training Loss: 0.4795\n",
      "Epoch [6/10], Iteration [14000/20000], Training Loss: 0.5705\n",
      "Epoch [6/10], Iteration [14100/20000], Training Loss: 0.4707\n",
      "Epoch [6/10], Iteration [14200/20000], Training Loss: 0.5668\n",
      "Epoch [6/10], Iteration [14300/20000], Training Loss: 0.4793\n",
      "Epoch [6/10], Iteration [14400/20000], Training Loss: 0.5250\n",
      "Epoch [6/10], Iteration [14500/20000], Training Loss: 0.7508\n",
      "Epoch [6/10], Iteration [14600/20000], Training Loss: 0.4375\n",
      "Epoch [6/10], Iteration [14700/20000], Training Loss: 0.6209\n",
      "Epoch [6/10], Iteration [14800/20000], Training Loss: 0.4879\n",
      "Epoch [6/10], Iteration [14900/20000], Training Loss: 0.5495\n",
      "Epoch [6/10], Iteration [15000/20000], Training Loss: 0.5962\n",
      "Epoch [6/10], Iteration [15100/20000], Training Loss: 0.5564\n",
      "Epoch [6/10], Iteration [15200/20000], Training Loss: 0.5359\n",
      "Epoch [6/10], Iteration [15300/20000], Training Loss: 0.5331\n",
      "Epoch [6/10], Iteration [15400/20000], Training Loss: 0.5406\n",
      "Epoch [6/10], Iteration [15500/20000], Training Loss: 0.5864\n",
      "Epoch [6/10], Iteration [15600/20000], Training Loss: 0.4991\n",
      "Epoch [6/10], Iteration [15700/20000], Training Loss: 0.5303\n",
      "Epoch [6/10], Iteration [15800/20000], Training Loss: 0.4786\n",
      "Epoch [6/10], Iteration [15900/20000], Training Loss: 0.5972\n",
      "Epoch [6/10], Iteration [16000/20000], Training Loss: 0.5627\n",
      "Epoch [6/10], Iteration [16100/20000], Training Loss: 0.5303\n",
      "Epoch [6/10], Iteration [16200/20000], Training Loss: 0.5450\n",
      "Epoch [6/10], Iteration [16300/20000], Training Loss: 0.6065\n",
      "Epoch [6/10], Iteration [16400/20000], Training Loss: 0.5210\n",
      "Epoch [6/10], Iteration [16500/20000], Training Loss: 0.5557\n",
      "Epoch [6/10], Iteration [16600/20000], Training Loss: 0.4487\n",
      "Epoch [6/10], Iteration [16700/20000], Training Loss: 0.4916\n",
      "Epoch [6/10], Iteration [16800/20000], Training Loss: 0.5700\n",
      "Epoch [6/10], Iteration [16900/20000], Training Loss: 0.5570\n",
      "Epoch [6/10], Iteration [17000/20000], Training Loss: 0.6019\n",
      "Epoch [6/10], Iteration [17100/20000], Training Loss: 0.5335\n",
      "Epoch [6/10], Iteration [17200/20000], Training Loss: 0.4376\n",
      "Epoch [6/10], Iteration [17300/20000], Training Loss: 0.5812\n",
      "Epoch [6/10], Iteration [17400/20000], Training Loss: 0.4831\n",
      "Epoch [6/10], Iteration [17500/20000], Training Loss: 0.4169\n",
      "Epoch [6/10], Iteration [17600/20000], Training Loss: 0.4630\n",
      "Epoch [6/10], Iteration [17700/20000], Training Loss: 0.5742\n",
      "Epoch [6/10], Iteration [17800/20000], Training Loss: 0.4365\n",
      "Epoch [6/10], Iteration [17900/20000], Training Loss: 0.5531\n",
      "Epoch [6/10], Iteration [18000/20000], Training Loss: 0.5557\n",
      "Epoch [6/10], Iteration [18100/20000], Training Loss: 0.5083\n",
      "Epoch [6/10], Iteration [18200/20000], Training Loss: 0.5254\n",
      "Epoch [6/10], Iteration [18300/20000], Training Loss: 0.5309\n",
      "Epoch [6/10], Iteration [18400/20000], Training Loss: 0.5856\n",
      "Epoch [6/10], Iteration [18500/20000], Training Loss: 0.4580\n",
      "Epoch [6/10], Iteration [18600/20000], Training Loss: 0.4937\n",
      "Epoch [6/10], Iteration [18700/20000], Training Loss: 0.4739\n",
      "Epoch [6/10], Iteration [18800/20000], Training Loss: 0.4721\n",
      "Epoch [6/10], Iteration [18900/20000], Training Loss: 0.4605\n",
      "Epoch [6/10], Iteration [19000/20000], Training Loss: 0.5460\n",
      "Epoch [6/10], Iteration [19100/20000], Training Loss: 0.4901\n",
      "Epoch [6/10], Iteration [19200/20000], Training Loss: 0.5059\n",
      "Epoch [6/10], Iteration [19300/20000], Training Loss: 0.6399\n",
      "Epoch [6/10], Iteration [19400/20000], Training Loss: 0.4830\n",
      "Epoch [6/10], Iteration [19500/20000], Training Loss: 0.5297\n",
      "Epoch [6/10], Iteration [19600/20000], Training Loss: 0.5082\n",
      "Epoch [6/10], Iteration [19700/20000], Training Loss: 0.5185\n",
      "Epoch [6/10], Iteration [19800/20000], Training Loss: 0.4787\n",
      "Epoch [6/10], Iteration [19900/20000], Training Loss: 0.4372\n",
      "Epoch [6/10], Iteration [20000/20000], Training Loss: 0.5606\n",
      "Epoch [6/10], Average Training Loss: 0.5099\n",
      "Epoch [7/10], Iteration [100/20000], Training Loss: 0.4496\n",
      "Epoch [7/10], Iteration [200/20000], Training Loss: 0.4951\n",
      "Epoch [7/10], Iteration [300/20000], Training Loss: 0.5454\n",
      "Epoch [7/10], Iteration [400/20000], Training Loss: 0.5260\n",
      "Epoch [7/10], Iteration [500/20000], Training Loss: 0.5268\n",
      "Epoch [7/10], Iteration [600/20000], Training Loss: 0.5064\n",
      "Epoch [7/10], Iteration [700/20000], Training Loss: 0.5112\n",
      "Epoch [7/10], Iteration [800/20000], Training Loss: 0.4746\n",
      "Epoch [7/10], Iteration [900/20000], Training Loss: 0.3355\n",
      "Epoch [7/10], Iteration [1000/20000], Training Loss: 0.4278\n",
      "Epoch [7/10], Iteration [1100/20000], Training Loss: 0.5127\n",
      "Epoch [7/10], Iteration [1200/20000], Training Loss: 0.5416\n",
      "Epoch [7/10], Iteration [1300/20000], Training Loss: 0.5479\n",
      "Epoch [7/10], Iteration [1400/20000], Training Loss: 0.4373\n",
      "Epoch [7/10], Iteration [1500/20000], Training Loss: 0.4691\n",
      "Epoch [7/10], Iteration [1600/20000], Training Loss: 0.5495\n",
      "Epoch [7/10], Iteration [1700/20000], Training Loss: 0.4759\n",
      "Epoch [7/10], Iteration [1800/20000], Training Loss: 0.4956\n",
      "Epoch [7/10], Iteration [1900/20000], Training Loss: 0.5876\n",
      "Epoch [7/10], Iteration [2000/20000], Training Loss: 0.4764\n",
      "Epoch [7/10], Iteration [2100/20000], Training Loss: 0.4414\n",
      "Epoch [7/10], Iteration [2200/20000], Training Loss: 0.5418\n",
      "Epoch [7/10], Iteration [2300/20000], Training Loss: 0.4374\n",
      "Epoch [7/10], Iteration [2400/20000], Training Loss: 0.4409\n",
      "Epoch [7/10], Iteration [2500/20000], Training Loss: 0.4774\n",
      "Epoch [7/10], Iteration [2600/20000], Training Loss: 0.5878\n",
      "Epoch [7/10], Iteration [2700/20000], Training Loss: 0.5457\n",
      "Epoch [7/10], Iteration [2800/20000], Training Loss: 0.4542\n",
      "Epoch [7/10], Iteration [2900/20000], Training Loss: 0.5234\n",
      "Epoch [7/10], Iteration [3000/20000], Training Loss: 0.5926\n",
      "Epoch [7/10], Iteration [3100/20000], Training Loss: 0.4515\n",
      "Epoch [7/10], Iteration [3200/20000], Training Loss: 0.5032\n",
      "Epoch [7/10], Iteration [3300/20000], Training Loss: 0.5025\n",
      "Epoch [7/10], Iteration [3400/20000], Training Loss: 0.5200\n",
      "Epoch [7/10], Iteration [3500/20000], Training Loss: 0.5555\n",
      "Epoch [7/10], Iteration [3600/20000], Training Loss: 0.4962\n",
      "Epoch [7/10], Iteration [3700/20000], Training Loss: 0.5193\n",
      "Epoch [7/10], Iteration [3800/20000], Training Loss: 0.4570\n",
      "Epoch [7/10], Iteration [3900/20000], Training Loss: 0.4348\n",
      "Epoch [7/10], Iteration [4000/20000], Training Loss: 0.5650\n",
      "Epoch [7/10], Iteration [4100/20000], Training Loss: 0.6364\n",
      "Epoch [7/10], Iteration [4200/20000], Training Loss: 0.5187\n",
      "Epoch [7/10], Iteration [4300/20000], Training Loss: 0.4911\n",
      "Epoch [7/10], Iteration [4400/20000], Training Loss: 0.5000\n",
      "Epoch [7/10], Iteration [4500/20000], Training Loss: 0.4404\n",
      "Epoch [7/10], Iteration [4600/20000], Training Loss: 0.4646\n",
      "Epoch [7/10], Iteration [4700/20000], Training Loss: 0.3872\n",
      "Epoch [7/10], Iteration [4800/20000], Training Loss: 0.5250\n",
      "Epoch [7/10], Iteration [4900/20000], Training Loss: 0.4489\n",
      "Epoch [7/10], Iteration [5000/20000], Training Loss: 0.5165\n",
      "Epoch [7/10], Iteration [5100/20000], Training Loss: 0.4134\n",
      "Epoch [7/10], Iteration [5200/20000], Training Loss: 0.5091\n",
      "Epoch [7/10], Iteration [5300/20000], Training Loss: 0.5810\n",
      "Epoch [7/10], Iteration [5400/20000], Training Loss: 0.4670\n",
      "Epoch [7/10], Iteration [5500/20000], Training Loss: 0.4399\n",
      "Epoch [7/10], Iteration [5600/20000], Training Loss: 0.4560\n",
      "Epoch [7/10], Iteration [5700/20000], Training Loss: 0.5463\n",
      "Epoch [7/10], Iteration [5800/20000], Training Loss: 0.5253\n",
      "Epoch [7/10], Iteration [5900/20000], Training Loss: 0.5940\n",
      "Epoch [7/10], Iteration [6000/20000], Training Loss: 0.4426\n",
      "Epoch [7/10], Iteration [6100/20000], Training Loss: 0.5686\n",
      "Epoch [7/10], Iteration [6200/20000], Training Loss: 0.4849\n",
      "Epoch [7/10], Iteration [6300/20000], Training Loss: 0.4321\n",
      "Epoch [7/10], Iteration [6400/20000], Training Loss: 0.5471\n",
      "Epoch [7/10], Iteration [6500/20000], Training Loss: 0.4788\n",
      "Epoch [7/10], Iteration [6600/20000], Training Loss: 0.5951\n",
      "Epoch [7/10], Iteration [6700/20000], Training Loss: 0.4534\n",
      "Epoch [7/10], Iteration [6800/20000], Training Loss: 0.4252\n",
      "Epoch [7/10], Iteration [6900/20000], Training Loss: 0.5299\n",
      "Epoch [7/10], Iteration [7000/20000], Training Loss: 0.5940\n",
      "Epoch [7/10], Iteration [7100/20000], Training Loss: 0.5105\n",
      "Epoch [7/10], Iteration [7200/20000], Training Loss: 0.4865\n",
      "Epoch [7/10], Iteration [7300/20000], Training Loss: 0.4607\n",
      "Epoch [7/10], Iteration [7400/20000], Training Loss: 0.5633\n",
      "Epoch [7/10], Iteration [7500/20000], Training Loss: 0.4619\n",
      "Epoch [7/10], Iteration [7600/20000], Training Loss: 0.4671\n",
      "Epoch [7/10], Iteration [7700/20000], Training Loss: 0.4666\n",
      "Epoch [7/10], Iteration [7800/20000], Training Loss: 0.5401\n",
      "Epoch [7/10], Iteration [7900/20000], Training Loss: 0.4940\n",
      "Epoch [7/10], Iteration [8000/20000], Training Loss: 0.4849\n",
      "Epoch [7/10], Iteration [8100/20000], Training Loss: 0.4653\n",
      "Epoch [7/10], Iteration [8200/20000], Training Loss: 0.4528\n",
      "Epoch [7/10], Iteration [8300/20000], Training Loss: 0.4203\n",
      "Epoch [7/10], Iteration [8400/20000], Training Loss: 0.5659\n",
      "Epoch [7/10], Iteration [8500/20000], Training Loss: 0.5403\n",
      "Epoch [7/10], Iteration [8600/20000], Training Loss: 0.4640\n",
      "Epoch [7/10], Iteration [8700/20000], Training Loss: 0.4870\n",
      "Epoch [7/10], Iteration [8800/20000], Training Loss: 0.5908\n",
      "Epoch [7/10], Iteration [8900/20000], Training Loss: 0.4267\n",
      "Epoch [7/10], Iteration [9000/20000], Training Loss: 0.5187\n",
      "Epoch [7/10], Iteration [9100/20000], Training Loss: 0.5839\n",
      "Epoch [7/10], Iteration [9200/20000], Training Loss: 0.4370\n",
      "Epoch [7/10], Iteration [9300/20000], Training Loss: 0.5572\n",
      "Epoch [7/10], Iteration [9400/20000], Training Loss: 0.5242\n",
      "Epoch [7/10], Iteration [9500/20000], Training Loss: 0.6002\n",
      "Epoch [7/10], Iteration [9600/20000], Training Loss: 0.4759\n",
      "Epoch [7/10], Iteration [9700/20000], Training Loss: 0.5610\n",
      "Epoch [7/10], Iteration [9800/20000], Training Loss: 0.5336\n",
      "Epoch [7/10], Iteration [9900/20000], Training Loss: 0.5162\n",
      "Epoch [7/10], Iteration [10000/20000], Training Loss: 0.5347\n",
      "Epoch [7/10], Iteration [10100/20000], Training Loss: 0.5741\n",
      "Epoch [7/10], Iteration [10200/20000], Training Loss: 0.5393\n",
      "Epoch [7/10], Iteration [10300/20000], Training Loss: 0.4172\n",
      "Epoch [7/10], Iteration [10400/20000], Training Loss: 0.5151\n",
      "Epoch [7/10], Iteration [10500/20000], Training Loss: 0.5566\n",
      "Epoch [7/10], Iteration [10600/20000], Training Loss: 0.5750\n",
      "Epoch [7/10], Iteration [10700/20000], Training Loss: 0.4098\n",
      "Epoch [7/10], Iteration [10800/20000], Training Loss: 0.5744\n",
      "Epoch [7/10], Iteration [10900/20000], Training Loss: 0.5516\n",
      "Epoch [7/10], Iteration [11000/20000], Training Loss: 0.5466\n",
      "Epoch [7/10], Iteration [11100/20000], Training Loss: 0.5560\n",
      "Epoch [7/10], Iteration [11200/20000], Training Loss: 0.5765\n",
      "Epoch [7/10], Iteration [11300/20000], Training Loss: 0.5277\n",
      "Epoch [7/10], Iteration [11400/20000], Training Loss: 0.4121\n",
      "Epoch [7/10], Iteration [11500/20000], Training Loss: 0.4013\n",
      "Epoch [7/10], Iteration [11600/20000], Training Loss: 0.4794\n",
      "Epoch [7/10], Iteration [11700/20000], Training Loss: 0.4726\n",
      "Epoch [7/10], Iteration [11800/20000], Training Loss: 0.6030\n",
      "Epoch [7/10], Iteration [11900/20000], Training Loss: 0.5331\n",
      "Epoch [7/10], Iteration [12000/20000], Training Loss: 0.5934\n",
      "Epoch [7/10], Iteration [12100/20000], Training Loss: 0.4551\n",
      "Epoch [7/10], Iteration [12200/20000], Training Loss: 0.5827\n",
      "Epoch [7/10], Iteration [12300/20000], Training Loss: 0.4366\n",
      "Epoch [7/10], Iteration [12400/20000], Training Loss: 0.5408\n",
      "Epoch [7/10], Iteration [12500/20000], Training Loss: 0.6013\n",
      "Epoch [7/10], Iteration [12600/20000], Training Loss: 0.5245\n",
      "Epoch [7/10], Iteration [12700/20000], Training Loss: 0.4886\n",
      "Epoch [7/10], Iteration [12800/20000], Training Loss: 0.5570\n",
      "Epoch [7/10], Iteration [12900/20000], Training Loss: 0.4734\n",
      "Epoch [7/10], Iteration [13000/20000], Training Loss: 0.5415\n",
      "Epoch [7/10], Iteration [13100/20000], Training Loss: 0.5193\n",
      "Epoch [7/10], Iteration [13200/20000], Training Loss: 0.5183\n",
      "Epoch [7/10], Iteration [13300/20000], Training Loss: 0.5131\n",
      "Epoch [7/10], Iteration [13400/20000], Training Loss: 0.4320\n",
      "Epoch [7/10], Iteration [13500/20000], Training Loss: 0.5653\n",
      "Epoch [7/10], Iteration [13600/20000], Training Loss: 0.5726\n",
      "Epoch [7/10], Iteration [13700/20000], Training Loss: 0.5455\n",
      "Epoch [7/10], Iteration [13800/20000], Training Loss: 0.4926\n",
      "Epoch [7/10], Iteration [13900/20000], Training Loss: 0.5203\n",
      "Epoch [7/10], Iteration [14000/20000], Training Loss: 0.5965\n",
      "Epoch [7/10], Iteration [14100/20000], Training Loss: 0.5669\n",
      "Epoch [7/10], Iteration [14200/20000], Training Loss: 0.6304\n",
      "Epoch [7/10], Iteration [14300/20000], Training Loss: 0.5194\n",
      "Epoch [7/10], Iteration [14400/20000], Training Loss: 0.5607\n",
      "Epoch [7/10], Iteration [14500/20000], Training Loss: 0.4918\n",
      "Epoch [7/10], Iteration [14600/20000], Training Loss: 0.4632\n",
      "Epoch [7/10], Iteration [14700/20000], Training Loss: 0.4164\n",
      "Epoch [7/10], Iteration [14800/20000], Training Loss: 0.4835\n",
      "Epoch [7/10], Iteration [14900/20000], Training Loss: 0.5003\n",
      "Epoch [7/10], Iteration [15000/20000], Training Loss: 0.3862\n",
      "Epoch [7/10], Iteration [15100/20000], Training Loss: 0.5014\n",
      "Epoch [7/10], Iteration [15200/20000], Training Loss: 0.5626\n",
      "Epoch [7/10], Iteration [15300/20000], Training Loss: 0.6456\n",
      "Epoch [7/10], Iteration [15400/20000], Training Loss: 0.5411\n",
      "Epoch [7/10], Iteration [15500/20000], Training Loss: 0.5330\n",
      "Epoch [7/10], Iteration [15600/20000], Training Loss: 0.4740\n",
      "Epoch [7/10], Iteration [15700/20000], Training Loss: 0.4163\n",
      "Epoch [7/10], Iteration [15800/20000], Training Loss: 0.4757\n",
      "Epoch [7/10], Iteration [15900/20000], Training Loss: 0.6014\n",
      "Epoch [7/10], Iteration [16000/20000], Training Loss: 0.5222\n",
      "Epoch [7/10], Iteration [16100/20000], Training Loss: 0.4624\n",
      "Epoch [7/10], Iteration [16200/20000], Training Loss: 0.5271\n",
      "Epoch [7/10], Iteration [16300/20000], Training Loss: 0.4962\n",
      "Epoch [7/10], Iteration [16400/20000], Training Loss: 0.3749\n",
      "Epoch [7/10], Iteration [16500/20000], Training Loss: 0.5389\n",
      "Epoch [7/10], Iteration [16600/20000], Training Loss: 0.5290\n",
      "Epoch [7/10], Iteration [16700/20000], Training Loss: 0.5538\n",
      "Epoch [7/10], Iteration [16800/20000], Training Loss: 0.4697\n",
      "Epoch [7/10], Iteration [16900/20000], Training Loss: 0.4467\n",
      "Epoch [7/10], Iteration [17000/20000], Training Loss: 0.4711\n",
      "Epoch [7/10], Iteration [17100/20000], Training Loss: 0.6732\n",
      "Epoch [7/10], Iteration [17200/20000], Training Loss: 0.5246\n",
      "Epoch [7/10], Iteration [17300/20000], Training Loss: 0.4558\n",
      "Epoch [7/10], Iteration [17400/20000], Training Loss: 0.5223\n",
      "Epoch [7/10], Iteration [17500/20000], Training Loss: 0.4897\n",
      "Epoch [7/10], Iteration [17600/20000], Training Loss: 0.5684\n",
      "Epoch [7/10], Iteration [17700/20000], Training Loss: 0.4663\n",
      "Epoch [7/10], Iteration [17800/20000], Training Loss: 0.4845\n",
      "Epoch [7/10], Iteration [17900/20000], Training Loss: 0.4597\n",
      "Epoch [7/10], Iteration [18000/20000], Training Loss: 0.5557\n",
      "Epoch [7/10], Iteration [18100/20000], Training Loss: 0.5656\n",
      "Epoch [7/10], Iteration [18200/20000], Training Loss: 0.5183\n",
      "Epoch [7/10], Iteration [18300/20000], Training Loss: 0.5742\n",
      "Epoch [7/10], Iteration [18400/20000], Training Loss: 0.4900\n",
      "Epoch [7/10], Iteration [18500/20000], Training Loss: 0.6015\n",
      "Epoch [7/10], Iteration [18600/20000], Training Loss: 0.5012\n",
      "Epoch [7/10], Iteration [18700/20000], Training Loss: 0.5551\n",
      "Epoch [7/10], Iteration [18800/20000], Training Loss: 0.5305\n",
      "Epoch [7/10], Iteration [18900/20000], Training Loss: 0.4295\n",
      "Epoch [7/10], Iteration [19000/20000], Training Loss: 0.4422\n",
      "Epoch [7/10], Iteration [19100/20000], Training Loss: 0.5152\n",
      "Epoch [7/10], Iteration [19200/20000], Training Loss: 0.5092\n",
      "Epoch [7/10], Iteration [19300/20000], Training Loss: 0.4078\n",
      "Epoch [7/10], Iteration [19400/20000], Training Loss: 0.6015\n",
      "Epoch [7/10], Iteration [19500/20000], Training Loss: 0.5042\n",
      "Epoch [7/10], Iteration [19600/20000], Training Loss: 0.6110\n",
      "Epoch [7/10], Iteration [19700/20000], Training Loss: 0.4186\n",
      "Epoch [7/10], Iteration [19800/20000], Training Loss: 0.4959\n",
      "Epoch [7/10], Iteration [19900/20000], Training Loss: 0.5812\n",
      "Epoch [7/10], Iteration [20000/20000], Training Loss: 0.4813\n",
      "Epoch [7/10], Average Training Loss: 0.5098\n",
      "Epoch [8/10], Iteration [100/20000], Training Loss: 0.4873\n",
      "Epoch [8/10], Iteration [200/20000], Training Loss: 0.5534\n",
      "Epoch [8/10], Iteration [300/20000], Training Loss: 0.4997\n",
      "Epoch [8/10], Iteration [400/20000], Training Loss: 0.4568\n",
      "Epoch [8/10], Iteration [500/20000], Training Loss: 0.4236\n",
      "Epoch [8/10], Iteration [600/20000], Training Loss: 0.4668\n",
      "Epoch [8/10], Iteration [700/20000], Training Loss: 0.5622\n",
      "Epoch [8/10], Iteration [800/20000], Training Loss: 0.6319\n",
      "Epoch [8/10], Iteration [900/20000], Training Loss: 0.4661\n",
      "Epoch [8/10], Iteration [1000/20000], Training Loss: 0.4557\n",
      "Epoch [8/10], Iteration [1100/20000], Training Loss: 0.5423\n",
      "Epoch [8/10], Iteration [1200/20000], Training Loss: 0.5791\n",
      "Epoch [8/10], Iteration [1300/20000], Training Loss: 0.5702\n",
      "Epoch [8/10], Iteration [1400/20000], Training Loss: 0.7094\n",
      "Epoch [8/10], Iteration [1500/20000], Training Loss: 0.4378\n",
      "Epoch [8/10], Iteration [1600/20000], Training Loss: 0.6488\n",
      "Epoch [8/10], Iteration [1700/20000], Training Loss: 0.5579\n",
      "Epoch [8/10], Iteration [1800/20000], Training Loss: 0.4918\n",
      "Epoch [8/10], Iteration [1900/20000], Training Loss: 0.4650\n",
      "Epoch [8/10], Iteration [2000/20000], Training Loss: 0.4795\n",
      "Epoch [8/10], Iteration [2100/20000], Training Loss: 0.5064\n",
      "Epoch [8/10], Iteration [2200/20000], Training Loss: 0.6312\n",
      "Epoch [8/10], Iteration [2300/20000], Training Loss: 0.4287\n",
      "Epoch [8/10], Iteration [2400/20000], Training Loss: 0.6685\n",
      "Epoch [8/10], Iteration [2500/20000], Training Loss: 0.4293\n",
      "Epoch [8/10], Iteration [2600/20000], Training Loss: 0.5463\n",
      "Epoch [8/10], Iteration [2700/20000], Training Loss: 0.5772\n",
      "Epoch [8/10], Iteration [2800/20000], Training Loss: 0.4357\n",
      "Epoch [8/10], Iteration [2900/20000], Training Loss: 0.5506\n",
      "Epoch [8/10], Iteration [3000/20000], Training Loss: 0.5614\n",
      "Epoch [8/10], Iteration [3100/20000], Training Loss: 0.4789\n",
      "Epoch [8/10], Iteration [3200/20000], Training Loss: 0.5442\n",
      "Epoch [8/10], Iteration [3300/20000], Training Loss: 0.5374\n",
      "Epoch [8/10], Iteration [3400/20000], Training Loss: 0.4576\n",
      "Epoch [8/10], Iteration [3500/20000], Training Loss: 0.3852\n",
      "Epoch [8/10], Iteration [3600/20000], Training Loss: 0.4519\n",
      "Epoch [8/10], Iteration [3700/20000], Training Loss: 0.4881\n",
      "Epoch [8/10], Iteration [3800/20000], Training Loss: 0.4775\n",
      "Epoch [8/10], Iteration [3900/20000], Training Loss: 0.3561\n",
      "Epoch [8/10], Iteration [4000/20000], Training Loss: 0.6053\n",
      "Epoch [8/10], Iteration [4100/20000], Training Loss: 0.4208\n",
      "Epoch [8/10], Iteration [4200/20000], Training Loss: 0.6364\n",
      "Epoch [8/10], Iteration [4300/20000], Training Loss: 0.4573\n",
      "Epoch [8/10], Iteration [4400/20000], Training Loss: 0.4525\n",
      "Epoch [8/10], Iteration [4500/20000], Training Loss: 0.5076\n",
      "Epoch [8/10], Iteration [4600/20000], Training Loss: 0.3961\n",
      "Epoch [8/10], Iteration [4700/20000], Training Loss: 0.4762\n",
      "Epoch [8/10], Iteration [4800/20000], Training Loss: 0.4685\n",
      "Epoch [8/10], Iteration [4900/20000], Training Loss: 0.4947\n",
      "Epoch [8/10], Iteration [5000/20000], Training Loss: 0.5156\n",
      "Epoch [8/10], Iteration [5100/20000], Training Loss: 0.5146\n",
      "Epoch [8/10], Iteration [5200/20000], Training Loss: 0.3484\n",
      "Epoch [8/10], Iteration [5300/20000], Training Loss: 0.4809\n",
      "Epoch [8/10], Iteration [5400/20000], Training Loss: 0.4807\n",
      "Epoch [8/10], Iteration [5500/20000], Training Loss: 0.4705\n",
      "Epoch [8/10], Iteration [5600/20000], Training Loss: 0.5980\n",
      "Epoch [8/10], Iteration [5700/20000], Training Loss: 0.5431\n",
      "Epoch [8/10], Iteration [5800/20000], Training Loss: 0.6001\n",
      "Epoch [8/10], Iteration [5900/20000], Training Loss: 0.4416\n",
      "Epoch [8/10], Iteration [6000/20000], Training Loss: 0.5158\n",
      "Epoch [8/10], Iteration [6100/20000], Training Loss: 0.5145\n",
      "Epoch [8/10], Iteration [6200/20000], Training Loss: 0.5016\n",
      "Epoch [8/10], Iteration [6300/20000], Training Loss: 0.4533\n",
      "Epoch [8/10], Iteration [6400/20000], Training Loss: 0.5673\n",
      "Epoch [8/10], Iteration [6500/20000], Training Loss: 0.5342\n",
      "Epoch [8/10], Iteration [6600/20000], Training Loss: 0.5464\n",
      "Epoch [8/10], Iteration [6700/20000], Training Loss: 0.4717\n",
      "Epoch [8/10], Iteration [6800/20000], Training Loss: 0.4607\n",
      "Epoch [8/10], Iteration [6900/20000], Training Loss: 0.5647\n",
      "Epoch [8/10], Iteration [7000/20000], Training Loss: 0.5948\n",
      "Epoch [8/10], Iteration [7100/20000], Training Loss: 0.4377\n",
      "Epoch [8/10], Iteration [7200/20000], Training Loss: 0.5826\n",
      "Epoch [8/10], Iteration [7300/20000], Training Loss: 0.4721\n",
      "Epoch [8/10], Iteration [7400/20000], Training Loss: 0.4542\n",
      "Epoch [8/10], Iteration [7500/20000], Training Loss: 0.5787\n",
      "Epoch [8/10], Iteration [7600/20000], Training Loss: 0.5273\n",
      "Epoch [8/10], Iteration [7700/20000], Training Loss: 0.4995\n",
      "Epoch [8/10], Iteration [7800/20000], Training Loss: 0.5287\n",
      "Epoch [8/10], Iteration [7900/20000], Training Loss: 0.5374\n",
      "Epoch [8/10], Iteration [8000/20000], Training Loss: 0.4432\n",
      "Epoch [8/10], Iteration [8100/20000], Training Loss: 0.4974\n",
      "Epoch [8/10], Iteration [8200/20000], Training Loss: 0.5131\n",
      "Epoch [8/10], Iteration [8300/20000], Training Loss: 0.4765\n",
      "Epoch [8/10], Iteration [8400/20000], Training Loss: 0.5612\n",
      "Epoch [8/10], Iteration [8500/20000], Training Loss: 0.4739\n",
      "Epoch [8/10], Iteration [8600/20000], Training Loss: 0.5636\n",
      "Epoch [8/10], Iteration [8700/20000], Training Loss: 0.5072\n",
      "Epoch [8/10], Iteration [8800/20000], Training Loss: 0.6654\n",
      "Epoch [8/10], Iteration [8900/20000], Training Loss: 0.3703\n",
      "Epoch [8/10], Iteration [9000/20000], Training Loss: 0.4511\n",
      "Epoch [8/10], Iteration [9100/20000], Training Loss: 0.4653\n",
      "Epoch [8/10], Iteration [9200/20000], Training Loss: 0.4038\n",
      "Epoch [8/10], Iteration [9300/20000], Training Loss: 0.5753\n",
      "Epoch [8/10], Iteration [9400/20000], Training Loss: 0.5459\n",
      "Epoch [8/10], Iteration [9500/20000], Training Loss: 0.4444\n",
      "Epoch [8/10], Iteration [9600/20000], Training Loss: 0.4570\n",
      "Epoch [8/10], Iteration [9700/20000], Training Loss: 0.5523\n",
      "Epoch [8/10], Iteration [9800/20000], Training Loss: 0.6416\n",
      "Epoch [8/10], Iteration [9900/20000], Training Loss: 0.5340\n",
      "Epoch [8/10], Iteration [10000/20000], Training Loss: 0.5723\n",
      "Epoch [8/10], Iteration [10100/20000], Training Loss: 0.4578\n",
      "Epoch [8/10], Iteration [10200/20000], Training Loss: 0.5203\n",
      "Epoch [8/10], Iteration [10300/20000], Training Loss: 0.5326\n",
      "Epoch [8/10], Iteration [10400/20000], Training Loss: 0.5095\n",
      "Epoch [8/10], Iteration [10500/20000], Training Loss: 0.5392\n",
      "Epoch [8/10], Iteration [10600/20000], Training Loss: 0.6613\n",
      "Epoch [8/10], Iteration [10700/20000], Training Loss: 0.5551\n",
      "Epoch [8/10], Iteration [10800/20000], Training Loss: 0.4574\n",
      "Epoch [8/10], Iteration [10900/20000], Training Loss: 0.5049\n",
      "Epoch [8/10], Iteration [11000/20000], Training Loss: 0.3933\n",
      "Epoch [8/10], Iteration [11100/20000], Training Loss: 0.5054\n",
      "Epoch [8/10], Iteration [11200/20000], Training Loss: 0.4532\n",
      "Epoch [8/10], Iteration [11300/20000], Training Loss: 0.5467\n",
      "Epoch [8/10], Iteration [11400/20000], Training Loss: 0.5062\n",
      "Epoch [8/10], Iteration [11500/20000], Training Loss: 0.6077\n",
      "Epoch [8/10], Iteration [11600/20000], Training Loss: 0.5998\n",
      "Epoch [8/10], Iteration [11700/20000], Training Loss: 0.4976\n",
      "Epoch [8/10], Iteration [11800/20000], Training Loss: 0.4531\n",
      "Epoch [8/10], Iteration [11900/20000], Training Loss: 0.4699\n",
      "Epoch [8/10], Iteration [12000/20000], Training Loss: 0.5287\n",
      "Epoch [8/10], Iteration [12100/20000], Training Loss: 0.5409\n",
      "Epoch [8/10], Iteration [12200/20000], Training Loss: 0.5629\n",
      "Epoch [8/10], Iteration [12300/20000], Training Loss: 0.5349\n",
      "Epoch [8/10], Iteration [12400/20000], Training Loss: 0.5541\n",
      "Epoch [8/10], Iteration [12500/20000], Training Loss: 0.5100\n",
      "Epoch [8/10], Iteration [12600/20000], Training Loss: 0.5467\n",
      "Epoch [8/10], Iteration [12700/20000], Training Loss: 0.5287\n",
      "Epoch [8/10], Iteration [12800/20000], Training Loss: 0.5027\n",
      "Epoch [8/10], Iteration [12900/20000], Training Loss: 0.5262\n",
      "Epoch [8/10], Iteration [13000/20000], Training Loss: 0.5507\n",
      "Epoch [8/10], Iteration [13100/20000], Training Loss: 0.6041\n",
      "Epoch [8/10], Iteration [13200/20000], Training Loss: 0.5392\n",
      "Epoch [8/10], Iteration [13300/20000], Training Loss: 0.5605\n",
      "Epoch [8/10], Iteration [13400/20000], Training Loss: 0.4898\n",
      "Epoch [8/10], Iteration [13500/20000], Training Loss: 0.5578\n",
      "Epoch [8/10], Iteration [13600/20000], Training Loss: 0.5552\n",
      "Epoch [8/10], Iteration [13700/20000], Training Loss: 0.4402\n",
      "Epoch [8/10], Iteration [13800/20000], Training Loss: 0.5106\n",
      "Epoch [8/10], Iteration [13900/20000], Training Loss: 0.4824\n",
      "Epoch [8/10], Iteration [14000/20000], Training Loss: 0.4793\n",
      "Epoch [8/10], Iteration [14100/20000], Training Loss: 0.4493\n",
      "Epoch [8/10], Iteration [14200/20000], Training Loss: 0.4911\n",
      "Epoch [8/10], Iteration [14300/20000], Training Loss: 0.4552\n",
      "Epoch [8/10], Iteration [14400/20000], Training Loss: 0.5666\n",
      "Epoch [8/10], Iteration [14500/20000], Training Loss: 0.5689\n",
      "Epoch [8/10], Iteration [14600/20000], Training Loss: 0.5155\n",
      "Epoch [8/10], Iteration [14700/20000], Training Loss: 0.4303\n",
      "Epoch [8/10], Iteration [14800/20000], Training Loss: 0.5160\n",
      "Epoch [8/10], Iteration [14900/20000], Training Loss: 0.5581\n",
      "Epoch [8/10], Iteration [15000/20000], Training Loss: 0.4467\n",
      "Epoch [8/10], Iteration [15100/20000], Training Loss: 0.6068\n",
      "Epoch [8/10], Iteration [15200/20000], Training Loss: 0.5038\n",
      "Epoch [8/10], Iteration [15300/20000], Training Loss: 0.6338\n",
      "Epoch [8/10], Iteration [15400/20000], Training Loss: 0.5472\n",
      "Epoch [8/10], Iteration [15500/20000], Training Loss: 0.5405\n",
      "Epoch [8/10], Iteration [15600/20000], Training Loss: 0.5349\n",
      "Epoch [8/10], Iteration [15700/20000], Training Loss: 0.5533\n",
      "Epoch [8/10], Iteration [15800/20000], Training Loss: 0.5705\n",
      "Epoch [8/10], Iteration [15900/20000], Training Loss: 0.5742\n",
      "Epoch [8/10], Iteration [16000/20000], Training Loss: 0.4713\n",
      "Epoch [8/10], Iteration [16100/20000], Training Loss: 0.5196\n",
      "Epoch [8/10], Iteration [16200/20000], Training Loss: 0.4894\n",
      "Epoch [8/10], Iteration [16300/20000], Training Loss: 0.4830\n",
      "Epoch [8/10], Iteration [16400/20000], Training Loss: 0.4938\n",
      "Epoch [8/10], Iteration [16500/20000], Training Loss: 0.3728\n",
      "Epoch [8/10], Iteration [16600/20000], Training Loss: 0.5295\n",
      "Epoch [8/10], Iteration [16700/20000], Training Loss: 0.5933\n",
      "Epoch [8/10], Iteration [16800/20000], Training Loss: 0.4465\n",
      "Epoch [8/10], Iteration [16900/20000], Training Loss: 0.4823\n",
      "Epoch [8/10], Iteration [17000/20000], Training Loss: 0.4921\n",
      "Epoch [8/10], Iteration [17100/20000], Training Loss: 0.4143\n",
      "Epoch [8/10], Iteration [17200/20000], Training Loss: 0.6252\n",
      "Epoch [8/10], Iteration [17300/20000], Training Loss: 0.4275\n",
      "Epoch [8/10], Iteration [17400/20000], Training Loss: 0.4602\n",
      "Epoch [8/10], Iteration [17500/20000], Training Loss: 0.4913\n",
      "Epoch [8/10], Iteration [17600/20000], Training Loss: 0.4822\n",
      "Epoch [8/10], Iteration [17700/20000], Training Loss: 0.5354\n",
      "Epoch [8/10], Iteration [17800/20000], Training Loss: 0.6024\n",
      "Epoch [8/10], Iteration [17900/20000], Training Loss: 0.5388\n",
      "Epoch [8/10], Iteration [18000/20000], Training Loss: 0.5923\n",
      "Epoch [8/10], Iteration [18100/20000], Training Loss: 0.5553\n",
      "Epoch [8/10], Iteration [18200/20000], Training Loss: 0.5146\n",
      "Epoch [8/10], Iteration [18300/20000], Training Loss: 0.4564\n",
      "Epoch [8/10], Iteration [18400/20000], Training Loss: 0.5198\n",
      "Epoch [8/10], Iteration [18500/20000], Training Loss: 0.4564\n",
      "Epoch [8/10], Iteration [18600/20000], Training Loss: 0.5081\n",
      "Epoch [8/10], Iteration [18700/20000], Training Loss: 0.5601\n",
      "Epoch [8/10], Iteration [18800/20000], Training Loss: 0.5182\n",
      "Epoch [8/10], Iteration [18900/20000], Training Loss: 0.4687\n",
      "Epoch [8/10], Iteration [19000/20000], Training Loss: 0.5031\n",
      "Epoch [8/10], Iteration [19100/20000], Training Loss: 0.3941\n",
      "Epoch [8/10], Iteration [19200/20000], Training Loss: 0.4726\n",
      "Epoch [8/10], Iteration [19300/20000], Training Loss: 0.4867\n",
      "Epoch [8/10], Iteration [19400/20000], Training Loss: 0.5274\n",
      "Epoch [8/10], Iteration [19500/20000], Training Loss: 0.5191\n",
      "Epoch [8/10], Iteration [19600/20000], Training Loss: 0.4494\n",
      "Epoch [8/10], Iteration [19700/20000], Training Loss: 0.6027\n",
      "Epoch [8/10], Iteration [19800/20000], Training Loss: 0.5140\n",
      "Epoch [8/10], Iteration [19900/20000], Training Loss: 0.4951\n",
      "Epoch [8/10], Iteration [20000/20000], Training Loss: 0.6100\n",
      "Epoch [8/10], Average Training Loss: 0.5090\n",
      "Epoch [9/10], Iteration [100/20000], Training Loss: 0.5263\n",
      "Epoch [9/10], Iteration [200/20000], Training Loss: 0.4331\n",
      "Epoch [9/10], Iteration [300/20000], Training Loss: 0.5093\n",
      "Epoch [9/10], Iteration [400/20000], Training Loss: 0.6114\n",
      "Epoch [9/10], Iteration [500/20000], Training Loss: 0.4446\n",
      "Epoch [9/10], Iteration [600/20000], Training Loss: 0.5402\n",
      "Epoch [9/10], Iteration [700/20000], Training Loss: 0.6421\n",
      "Epoch [9/10], Iteration [800/20000], Training Loss: 0.5176\n",
      "Epoch [9/10], Iteration [900/20000], Training Loss: 0.4813\n",
      "Epoch [9/10], Iteration [1000/20000], Training Loss: 0.4965\n",
      "Epoch [9/10], Iteration [1100/20000], Training Loss: 0.5850\n",
      "Epoch [9/10], Iteration [1200/20000], Training Loss: 0.5556\n",
      "Epoch [9/10], Iteration [1300/20000], Training Loss: 0.5059\n",
      "Epoch [9/10], Iteration [1400/20000], Training Loss: 0.4082\n",
      "Epoch [9/10], Iteration [1500/20000], Training Loss: 0.5617\n",
      "Epoch [9/10], Iteration [1600/20000], Training Loss: 0.5176\n",
      "Epoch [9/10], Iteration [1700/20000], Training Loss: 0.7481\n",
      "Epoch [9/10], Iteration [1800/20000], Training Loss: 0.4795\n",
      "Epoch [9/10], Iteration [1900/20000], Training Loss: 0.4853\n",
      "Epoch [9/10], Iteration [2000/20000], Training Loss: 0.6001\n",
      "Epoch [9/10], Iteration [2100/20000], Training Loss: 0.3891\n",
      "Epoch [9/10], Iteration [2200/20000], Training Loss: 0.4533\n",
      "Epoch [9/10], Iteration [2300/20000], Training Loss: 0.4576\n",
      "Epoch [9/10], Iteration [2400/20000], Training Loss: 0.5243\n",
      "Epoch [9/10], Iteration [2500/20000], Training Loss: 0.4031\n",
      "Epoch [9/10], Iteration [2600/20000], Training Loss: 0.5330\n",
      "Epoch [9/10], Iteration [2700/20000], Training Loss: 0.5123\n",
      "Epoch [9/10], Iteration [2800/20000], Training Loss: 0.4904\n",
      "Epoch [9/10], Iteration [2900/20000], Training Loss: 0.4617\n",
      "Epoch [9/10], Iteration [3000/20000], Training Loss: 0.5563\n",
      "Epoch [9/10], Iteration [3100/20000], Training Loss: 0.4715\n",
      "Epoch [9/10], Iteration [3200/20000], Training Loss: 0.5289\n",
      "Epoch [9/10], Iteration [3300/20000], Training Loss: 0.5333\n",
      "Epoch [9/10], Iteration [3400/20000], Training Loss: 0.5275\n",
      "Epoch [9/10], Iteration [3500/20000], Training Loss: 0.5127\n",
      "Epoch [9/10], Iteration [3600/20000], Training Loss: 0.4658\n",
      "Epoch [9/10], Iteration [3700/20000], Training Loss: 0.5431\n",
      "Epoch [9/10], Iteration [3800/20000], Training Loss: 0.4970\n",
      "Epoch [9/10], Iteration [3900/20000], Training Loss: 0.4471\n",
      "Epoch [9/10], Iteration [4000/20000], Training Loss: 0.4919\n",
      "Epoch [9/10], Iteration [4100/20000], Training Loss: 0.5672\n",
      "Epoch [9/10], Iteration [4200/20000], Training Loss: 0.5028\n",
      "Epoch [9/10], Iteration [4300/20000], Training Loss: 0.5078\n",
      "Epoch [9/10], Iteration [4400/20000], Training Loss: 0.5430\n",
      "Epoch [9/10], Iteration [4500/20000], Training Loss: 0.4707\n",
      "Epoch [9/10], Iteration [4600/20000], Training Loss: 0.4571\n",
      "Epoch [9/10], Iteration [4700/20000], Training Loss: 0.5006\n",
      "Epoch [9/10], Iteration [4800/20000], Training Loss: 0.5302\n",
      "Epoch [9/10], Iteration [4900/20000], Training Loss: 0.5429\n",
      "Epoch [9/10], Iteration [5000/20000], Training Loss: 0.4536\n",
      "Epoch [9/10], Iteration [5100/20000], Training Loss: 0.4515\n",
      "Epoch [9/10], Iteration [5200/20000], Training Loss: 0.4802\n",
      "Epoch [9/10], Iteration [5300/20000], Training Loss: 0.4931\n",
      "Epoch [9/10], Iteration [5400/20000], Training Loss: 0.4205\n",
      "Epoch [9/10], Iteration [5500/20000], Training Loss: 0.5780\n",
      "Epoch [9/10], Iteration [5600/20000], Training Loss: 0.4937\n",
      "Epoch [9/10], Iteration [5700/20000], Training Loss: 0.5519\n",
      "Epoch [9/10], Iteration [5800/20000], Training Loss: 0.5248\n",
      "Epoch [9/10], Iteration [5900/20000], Training Loss: 0.4936\n",
      "Epoch [9/10], Iteration [6000/20000], Training Loss: 0.6042\n",
      "Epoch [9/10], Iteration [6100/20000], Training Loss: 0.4504\n",
      "Epoch [9/10], Iteration [6200/20000], Training Loss: 0.5212\n",
      "Epoch [9/10], Iteration [6300/20000], Training Loss: 0.4985\n",
      "Epoch [9/10], Iteration [6400/20000], Training Loss: 0.5380\n",
      "Epoch [9/10], Iteration [6500/20000], Training Loss: 0.5123\n",
      "Epoch [9/10], Iteration [6600/20000], Training Loss: 0.4327\n",
      "Epoch [9/10], Iteration [6700/20000], Training Loss: 0.4758\n",
      "Epoch [9/10], Iteration [6800/20000], Training Loss: 0.5384\n",
      "Epoch [9/10], Iteration [6900/20000], Training Loss: 0.5261\n",
      "Epoch [9/10], Iteration [7000/20000], Training Loss: 0.5057\n",
      "Epoch [9/10], Iteration [7100/20000], Training Loss: 0.5256\n",
      "Epoch [9/10], Iteration [7200/20000], Training Loss: 0.4617\n",
      "Epoch [9/10], Iteration [7300/20000], Training Loss: 0.5109\n",
      "Epoch [9/10], Iteration [7400/20000], Training Loss: 0.5206\n",
      "Epoch [9/10], Iteration [7500/20000], Training Loss: 0.4546\n",
      "Epoch [9/10], Iteration [7600/20000], Training Loss: 0.5500\n",
      "Epoch [9/10], Iteration [7700/20000], Training Loss: 0.5374\n",
      "Epoch [9/10], Iteration [7800/20000], Training Loss: 0.6430\n",
      "Epoch [9/10], Iteration [7900/20000], Training Loss: 0.4894\n",
      "Epoch [9/10], Iteration [8000/20000], Training Loss: 0.5417\n",
      "Epoch [9/10], Iteration [8100/20000], Training Loss: 0.6323\n",
      "Epoch [9/10], Iteration [8200/20000], Training Loss: 0.5019\n",
      "Epoch [9/10], Iteration [8300/20000], Training Loss: 0.4349\n",
      "Epoch [9/10], Iteration [8400/20000], Training Loss: 0.4969\n",
      "Epoch [9/10], Iteration [8500/20000], Training Loss: 0.4591\n",
      "Epoch [9/10], Iteration [8600/20000], Training Loss: 0.5836\n",
      "Epoch [9/10], Iteration [8700/20000], Training Loss: 0.5592\n",
      "Epoch [9/10], Iteration [8800/20000], Training Loss: 0.5265\n",
      "Epoch [9/10], Iteration [8900/20000], Training Loss: 0.5888\n",
      "Epoch [9/10], Iteration [9000/20000], Training Loss: 0.4707\n",
      "Epoch [9/10], Iteration [9100/20000], Training Loss: 0.4716\n",
      "Epoch [9/10], Iteration [9200/20000], Training Loss: 0.5466\n",
      "Epoch [9/10], Iteration [9300/20000], Training Loss: 0.6131\n",
      "Epoch [9/10], Iteration [9400/20000], Training Loss: 0.5764\n",
      "Epoch [9/10], Iteration [9500/20000], Training Loss: 0.4968\n",
      "Epoch [9/10], Iteration [9600/20000], Training Loss: 0.4970\n",
      "Epoch [9/10], Iteration [9700/20000], Training Loss: 0.4789\n",
      "Epoch [9/10], Iteration [9800/20000], Training Loss: 0.4553\n",
      "Epoch [9/10], Iteration [9900/20000], Training Loss: 0.5076\n",
      "Epoch [9/10], Iteration [10000/20000], Training Loss: 0.5909\n",
      "Epoch [9/10], Iteration [10100/20000], Training Loss: 0.5390\n",
      "Epoch [9/10], Iteration [10200/20000], Training Loss: 0.5869\n",
      "Epoch [9/10], Iteration [10300/20000], Training Loss: 0.5868\n",
      "Epoch [9/10], Iteration [10400/20000], Training Loss: 0.5295\n",
      "Epoch [9/10], Iteration [10500/20000], Training Loss: 0.5115\n",
      "Epoch [9/10], Iteration [10600/20000], Training Loss: 0.6012\n",
      "Epoch [9/10], Iteration [10700/20000], Training Loss: 0.4117\n",
      "Epoch [9/10], Iteration [10800/20000], Training Loss: 0.5417\n",
      "Epoch [9/10], Iteration [10900/20000], Training Loss: 0.6283\n",
      "Epoch [9/10], Iteration [11000/20000], Training Loss: 0.5371\n",
      "Epoch [9/10], Iteration [11100/20000], Training Loss: 0.5047\n",
      "Epoch [9/10], Iteration [11200/20000], Training Loss: 0.4348\n",
      "Epoch [9/10], Iteration [11300/20000], Training Loss: 0.5009\n",
      "Epoch [9/10], Iteration [11400/20000], Training Loss: 0.5377\n",
      "Epoch [9/10], Iteration [11500/20000], Training Loss: 0.3714\n",
      "Epoch [9/10], Iteration [11600/20000], Training Loss: 0.4865\n",
      "Epoch [9/10], Iteration [11700/20000], Training Loss: 0.6359\n",
      "Epoch [9/10], Iteration [11800/20000], Training Loss: 0.6156\n",
      "Epoch [9/10], Iteration [11900/20000], Training Loss: 0.4557\n",
      "Epoch [9/10], Iteration [12000/20000], Training Loss: 0.5157\n",
      "Epoch [9/10], Iteration [12100/20000], Training Loss: 0.4681\n",
      "Epoch [9/10], Iteration [12200/20000], Training Loss: 0.5294\n",
      "Epoch [9/10], Iteration [12300/20000], Training Loss: 0.5677\n",
      "Epoch [9/10], Iteration [12400/20000], Training Loss: 0.4517\n",
      "Epoch [9/10], Iteration [12500/20000], Training Loss: 0.5163\n",
      "Epoch [9/10], Iteration [12600/20000], Training Loss: 0.5081\n",
      "Epoch [9/10], Iteration [12700/20000], Training Loss: 0.5609\n",
      "Epoch [9/10], Iteration [12800/20000], Training Loss: 0.5204\n",
      "Epoch [9/10], Iteration [12900/20000], Training Loss: 0.5779\n",
      "Epoch [9/10], Iteration [13000/20000], Training Loss: 0.7092\n",
      "Epoch [9/10], Iteration [13100/20000], Training Loss: 0.4846\n",
      "Epoch [9/10], Iteration [13200/20000], Training Loss: 0.4566\n",
      "Epoch [9/10], Iteration [13300/20000], Training Loss: 0.4257\n",
      "Epoch [9/10], Iteration [13400/20000], Training Loss: 0.4865\n",
      "Epoch [9/10], Iteration [13500/20000], Training Loss: 0.4986\n",
      "Epoch [9/10], Iteration [13600/20000], Training Loss: 0.4450\n",
      "Epoch [9/10], Iteration [13800/20000], Training Loss: 0.4493\n",
      "Epoch [9/10], Iteration [13900/20000], Training Loss: 0.6361\n",
      "Epoch [9/10], Iteration [14000/20000], Training Loss: 0.3879\n",
      "Epoch [9/10], Iteration [14100/20000], Training Loss: 0.4571\n",
      "Epoch [9/10], Iteration [14200/20000], Training Loss: 0.6617\n",
      "Epoch [9/10], Iteration [14300/20000], Training Loss: 0.3995\n",
      "Epoch [9/10], Iteration [14400/20000], Training Loss: 0.6414\n",
      "Epoch [9/10], Iteration [14500/20000], Training Loss: 0.5809\n",
      "Epoch [9/10], Iteration [14600/20000], Training Loss: 0.4952\n",
      "Epoch [9/10], Iteration [14700/20000], Training Loss: 0.4575\n",
      "Epoch [9/10], Iteration [14800/20000], Training Loss: 0.5665\n",
      "Epoch [9/10], Iteration [14900/20000], Training Loss: 0.5557\n",
      "Epoch [9/10], Iteration [15000/20000], Training Loss: 0.5368\n",
      "Epoch [9/10], Iteration [15100/20000], Training Loss: 0.5551\n",
      "Epoch [9/10], Iteration [15200/20000], Training Loss: 0.5594\n",
      "Epoch [9/10], Iteration [15300/20000], Training Loss: 0.5385\n",
      "Epoch [9/10], Iteration [15400/20000], Training Loss: 0.5596\n",
      "Epoch [9/10], Iteration [15500/20000], Training Loss: 0.5046\n",
      "Epoch [9/10], Iteration [15600/20000], Training Loss: 0.4369\n",
      "Epoch [9/10], Iteration [15700/20000], Training Loss: 0.5936\n",
      "Epoch [9/10], Iteration [15800/20000], Training Loss: 0.4106\n",
      "Epoch [9/10], Iteration [15900/20000], Training Loss: 0.5380\n",
      "Epoch [9/10], Iteration [16000/20000], Training Loss: 0.4949\n",
      "Epoch [9/10], Iteration [16100/20000], Training Loss: 0.4529\n",
      "Epoch [9/10], Iteration [16200/20000], Training Loss: 0.5116\n",
      "Epoch [9/10], Iteration [16300/20000], Training Loss: 0.5624\n",
      "Epoch [9/10], Iteration [16400/20000], Training Loss: 0.5958\n",
      "Epoch [9/10], Iteration [16500/20000], Training Loss: 0.4914\n",
      "Epoch [9/10], Iteration [16600/20000], Training Loss: 0.4783\n",
      "Epoch [9/10], Iteration [16700/20000], Training Loss: 0.5585\n",
      "Epoch [9/10], Iteration [16800/20000], Training Loss: 0.5565\n",
      "Epoch [9/10], Iteration [16900/20000], Training Loss: 0.5482\n",
      "Epoch [9/10], Iteration [17000/20000], Training Loss: 0.5566\n",
      "Epoch [9/10], Iteration [17100/20000], Training Loss: 0.5791\n",
      "Epoch [9/10], Iteration [17200/20000], Training Loss: 0.5148\n",
      "Epoch [9/10], Iteration [17300/20000], Training Loss: 0.4339\n",
      "Epoch [9/10], Iteration [17400/20000], Training Loss: 0.5032\n",
      "Epoch [9/10], Iteration [17500/20000], Training Loss: 0.4840\n",
      "Epoch [9/10], Iteration [17600/20000], Training Loss: 0.5078\n",
      "Epoch [9/10], Iteration [17700/20000], Training Loss: 0.5700\n",
      "Epoch [9/10], Iteration [17800/20000], Training Loss: 0.4731\n",
      "Epoch [9/10], Iteration [17900/20000], Training Loss: 0.4719\n",
      "Epoch [9/10], Iteration [18000/20000], Training Loss: 0.5761\n",
      "Epoch [9/10], Iteration [18100/20000], Training Loss: 0.4800\n",
      "Epoch [9/10], Iteration [18200/20000], Training Loss: 0.4869\n",
      "Epoch [9/10], Iteration [18300/20000], Training Loss: 0.5160\n",
      "Epoch [9/10], Iteration [18400/20000], Training Loss: 0.4961\n",
      "Epoch [9/10], Iteration [18500/20000], Training Loss: 0.4593\n",
      "Epoch [9/10], Iteration [18600/20000], Training Loss: 0.4302\n",
      "Epoch [9/10], Iteration [18700/20000], Training Loss: 0.5070\n",
      "Epoch [9/10], Iteration [18800/20000], Training Loss: 0.4690\n",
      "Epoch [9/10], Iteration [18900/20000], Training Loss: 0.4776\n",
      "Epoch [9/10], Iteration [19000/20000], Training Loss: 0.4524\n",
      "Epoch [9/10], Iteration [19100/20000], Training Loss: 0.4843\n",
      "Epoch [9/10], Iteration [19200/20000], Training Loss: 0.5789\n",
      "Epoch [9/10], Iteration [19300/20000], Training Loss: 0.5726\n",
      "Epoch [9/10], Iteration [19400/20000], Training Loss: 0.4255\n",
      "Epoch [9/10], Iteration [19500/20000], Training Loss: 0.4755\n",
      "Epoch [9/10], Iteration [19600/20000], Training Loss: 0.6063\n",
      "Epoch [9/10], Iteration [19700/20000], Training Loss: 0.4678\n",
      "Epoch [9/10], Iteration [19800/20000], Training Loss: 0.5162\n",
      "Epoch [9/10], Iteration [19900/20000], Training Loss: 0.5194\n",
      "Epoch [9/10], Iteration [20000/20000], Training Loss: 0.5793\n",
      "Epoch [9/10], Average Training Loss: 0.5089\n",
      "Epoch [10/10], Iteration [100/20000], Training Loss: 0.6498\n",
      "Epoch [10/10], Iteration [200/20000], Training Loss: 0.4851\n",
      "Epoch [10/10], Iteration [300/20000], Training Loss: 0.4556\n",
      "Epoch [10/10], Iteration [400/20000], Training Loss: 0.4552\n",
      "Epoch [10/10], Iteration [500/20000], Training Loss: 0.4478\n",
      "Epoch [10/10], Iteration [600/20000], Training Loss: 0.5711\n",
      "Epoch [10/10], Iteration [700/20000], Training Loss: 0.5251\n",
      "Epoch [10/10], Iteration [800/20000], Training Loss: 0.4941\n",
      "Epoch [10/10], Iteration [900/20000], Training Loss: 0.4070\n",
      "Epoch [10/10], Iteration [1000/20000], Training Loss: 0.4722\n",
      "Epoch [10/10], Iteration [1100/20000], Training Loss: 0.4846\n",
      "Epoch [10/10], Iteration [1200/20000], Training Loss: 0.4845\n",
      "Epoch [10/10], Iteration [1300/20000], Training Loss: 0.4885\n",
      "Epoch [10/10], Iteration [1400/20000], Training Loss: 0.6757\n",
      "Epoch [10/10], Iteration [1500/20000], Training Loss: 0.5097\n",
      "Epoch [10/10], Iteration [1600/20000], Training Loss: 0.4469\n",
      "Epoch [10/10], Iteration [1700/20000], Training Loss: 0.5526\n",
      "Epoch [10/10], Iteration [1800/20000], Training Loss: 0.5953\n",
      "Epoch [10/10], Iteration [1900/20000], Training Loss: 0.4071\n",
      "Epoch [10/10], Iteration [2000/20000], Training Loss: 0.4112\n",
      "Epoch [10/10], Iteration [2100/20000], Training Loss: 0.5093\n",
      "Epoch [10/10], Iteration [2200/20000], Training Loss: 0.4463\n",
      "Epoch [10/10], Iteration [2300/20000], Training Loss: 0.5364\n",
      "Epoch [10/10], Iteration [2400/20000], Training Loss: 0.4605\n",
      "Epoch [10/10], Iteration [2500/20000], Training Loss: 0.5998\n",
      "Epoch [10/10], Iteration [2600/20000], Training Loss: 0.6101\n",
      "Epoch [10/10], Iteration [2700/20000], Training Loss: 0.5378\n",
      "Epoch [10/10], Iteration [2800/20000], Training Loss: 0.4974\n",
      "Epoch [10/10], Iteration [2900/20000], Training Loss: 0.6466\n",
      "Epoch [10/10], Iteration [3000/20000], Training Loss: 0.5067\n",
      "Epoch [10/10], Iteration [3100/20000], Training Loss: 0.5662\n",
      "Epoch [10/10], Iteration [3200/20000], Training Loss: 0.5099\n",
      "Epoch [10/10], Iteration [3300/20000], Training Loss: 0.5611\n",
      "Epoch [10/10], Iteration [3400/20000], Training Loss: 0.4105\n",
      "Epoch [10/10], Iteration [3500/20000], Training Loss: 0.4855\n",
      "Epoch [10/10], Iteration [3600/20000], Training Loss: 0.4806\n",
      "Epoch [10/10], Iteration [3700/20000], Training Loss: 0.4197\n",
      "Epoch [10/10], Iteration [3800/20000], Training Loss: 0.5056\n",
      "Epoch [10/10], Iteration [3900/20000], Training Loss: 0.4644\n",
      "Epoch [10/10], Iteration [4000/20000], Training Loss: 0.4709\n",
      "Epoch [10/10], Iteration [4100/20000], Training Loss: 0.5228\n",
      "Epoch [10/10], Iteration [4200/20000], Training Loss: 0.4884\n",
      "Epoch [10/10], Iteration [4300/20000], Training Loss: 0.4480\n",
      "Epoch [10/10], Iteration [4400/20000], Training Loss: 0.6414\n",
      "Epoch [10/10], Iteration [4500/20000], Training Loss: 0.5503\n",
      "Epoch [10/10], Iteration [4600/20000], Training Loss: 0.3804\n",
      "Epoch [10/10], Iteration [4700/20000], Training Loss: 0.4409\n",
      "Epoch [10/10], Iteration [4800/20000], Training Loss: 0.4375\n",
      "Epoch [10/10], Iteration [4900/20000], Training Loss: 0.4808\n",
      "Epoch [10/10], Iteration [5000/20000], Training Loss: 0.4262\n",
      "Epoch [10/10], Iteration [5100/20000], Training Loss: 0.4752\n",
      "Epoch [10/10], Iteration [5200/20000], Training Loss: 0.5269\n",
      "Epoch [10/10], Iteration [5300/20000], Training Loss: 0.4719\n",
      "Epoch [10/10], Iteration [5400/20000], Training Loss: 0.5127\n",
      "Epoch [10/10], Iteration [5500/20000], Training Loss: 0.5362\n",
      "Epoch [10/10], Iteration [5600/20000], Training Loss: 0.6073\n",
      "Epoch [10/10], Iteration [5700/20000], Training Loss: 0.5167\n",
      "Epoch [10/10], Iteration [5800/20000], Training Loss: 0.4545\n",
      "Epoch [10/10], Iteration [5900/20000], Training Loss: 0.4341\n",
      "Epoch [10/10], Iteration [6000/20000], Training Loss: 0.4825\n",
      "Epoch [10/10], Iteration [6100/20000], Training Loss: 0.4681\n",
      "Epoch [10/10], Iteration [6200/20000], Training Loss: 0.5101\n",
      "Epoch [10/10], Iteration [6300/20000], Training Loss: 0.5969\n",
      "Epoch [10/10], Iteration [6400/20000], Training Loss: 0.5149\n",
      "Epoch [10/10], Iteration [6500/20000], Training Loss: 0.6392\n",
      "Epoch [10/10], Iteration [6600/20000], Training Loss: 0.5156\n",
      "Epoch [10/10], Iteration [6700/20000], Training Loss: 0.6118\n",
      "Epoch [10/10], Iteration [6800/20000], Training Loss: 0.5209\n",
      "Epoch [10/10], Iteration [6900/20000], Training Loss: 0.6761\n",
      "Epoch [10/10], Iteration [7000/20000], Training Loss: 0.4848\n",
      "Epoch [10/10], Iteration [7100/20000], Training Loss: 0.5154\n",
      "Epoch [10/10], Iteration [7200/20000], Training Loss: 0.4849\n",
      "Epoch [10/10], Iteration [7300/20000], Training Loss: 0.4387\n",
      "Epoch [10/10], Iteration [7400/20000], Training Loss: 0.5523\n",
      "Epoch [10/10], Iteration [7500/20000], Training Loss: 0.5053\n",
      "Epoch [10/10], Iteration [7600/20000], Training Loss: 0.5625\n",
      "Epoch [10/10], Iteration [7700/20000], Training Loss: 0.4397\n",
      "Epoch [10/10], Iteration [7800/20000], Training Loss: 0.5266\n",
      "Epoch [10/10], Iteration [7900/20000], Training Loss: 0.5003\n",
      "Epoch [10/10], Iteration [8000/20000], Training Loss: 0.5721\n",
      "Epoch [10/10], Iteration [8100/20000], Training Loss: 0.5760\n",
      "Epoch [10/10], Iteration [8200/20000], Training Loss: 0.3516\n",
      "Epoch [10/10], Iteration [8300/20000], Training Loss: 0.4968\n",
      "Epoch [10/10], Iteration [8400/20000], Training Loss: 0.6278\n",
      "Epoch [10/10], Iteration [8500/20000], Training Loss: 0.3216\n",
      "Epoch [10/10], Iteration [8600/20000], Training Loss: 0.6604\n",
      "Epoch [10/10], Iteration [8700/20000], Training Loss: 0.3591\n",
      "Epoch [10/10], Iteration [8800/20000], Training Loss: 0.5339\n",
      "Epoch [10/10], Iteration [8900/20000], Training Loss: 0.5050\n",
      "Epoch [10/10], Iteration [9000/20000], Training Loss: 0.4985\n",
      "Epoch [10/10], Iteration [9100/20000], Training Loss: 0.4617\n",
      "Epoch [10/10], Iteration [9200/20000], Training Loss: 0.5014\n",
      "Epoch [10/10], Iteration [9300/20000], Training Loss: 0.5417\n",
      "Epoch [10/10], Iteration [9400/20000], Training Loss: 0.5210\n",
      "Epoch [10/10], Iteration [9500/20000], Training Loss: 0.4444\n",
      "Epoch [10/10], Iteration [9600/20000], Training Loss: 0.5253\n",
      "Epoch [10/10], Iteration [9700/20000], Training Loss: 0.4808\n",
      "Epoch [10/10], Iteration [9800/20000], Training Loss: 0.3935\n",
      "Epoch [10/10], Iteration [9900/20000], Training Loss: 0.4315\n",
      "Epoch [10/10], Iteration [10000/20000], Training Loss: 0.4960\n",
      "Epoch [10/10], Iteration [10100/20000], Training Loss: 0.4233\n",
      "Epoch [10/10], Iteration [10200/20000], Training Loss: 0.5618\n",
      "Epoch [10/10], Iteration [10300/20000], Training Loss: 0.4732\n",
      "Epoch [10/10], Iteration [10400/20000], Training Loss: 0.5534\n",
      "Epoch [10/10], Iteration [10500/20000], Training Loss: 0.5951\n",
      "Epoch [10/10], Iteration [10600/20000], Training Loss: 0.5026\n",
      "Epoch [10/10], Iteration [10700/20000], Training Loss: 0.4705\n",
      "Epoch [10/10], Iteration [10800/20000], Training Loss: 0.4053\n",
      "Epoch [10/10], Iteration [10900/20000], Training Loss: 0.5024\n",
      "Epoch [10/10], Iteration [11000/20000], Training Loss: 0.5271\n",
      "Epoch [10/10], Iteration [11100/20000], Training Loss: 0.5049\n",
      "Epoch [10/10], Iteration [11200/20000], Training Loss: 0.6183\n",
      "Epoch [10/10], Iteration [11300/20000], Training Loss: 0.4237\n",
      "Epoch [10/10], Iteration [11400/20000], Training Loss: 0.4912\n",
      "Epoch [10/10], Iteration [11500/20000], Training Loss: 0.4917\n",
      "Epoch [10/10], Iteration [11600/20000], Training Loss: 0.4853\n",
      "Epoch [10/10], Iteration [11700/20000], Training Loss: 0.5724\n",
      "Epoch [10/10], Iteration [11800/20000], Training Loss: 0.4870\n",
      "Epoch [10/10], Iteration [11900/20000], Training Loss: 0.6160\n",
      "Epoch [10/10], Iteration [12000/20000], Training Loss: 0.5799\n",
      "Epoch [10/10], Iteration [12100/20000], Training Loss: 0.5698\n",
      "Epoch [10/10], Iteration [12200/20000], Training Loss: 0.4892\n",
      "Epoch [10/10], Iteration [12300/20000], Training Loss: 0.6217\n",
      "Epoch [10/10], Iteration [12400/20000], Training Loss: 0.4894\n",
      "Epoch [10/10], Iteration [12500/20000], Training Loss: 0.5539\n",
      "Epoch [10/10], Iteration [12600/20000], Training Loss: 0.5132\n",
      "Epoch [10/10], Iteration [12700/20000], Training Loss: 0.5188\n",
      "Epoch [10/10], Iteration [12800/20000], Training Loss: 0.5619\n",
      "Epoch [10/10], Iteration [12900/20000], Training Loss: 0.6707\n",
      "Epoch [10/10], Iteration [13000/20000], Training Loss: 0.4767\n",
      "Epoch [10/10], Iteration [13100/20000], Training Loss: 0.6892\n",
      "Epoch [10/10], Iteration [13200/20000], Training Loss: 0.5720\n",
      "Epoch [10/10], Iteration [13300/20000], Training Loss: 0.5304\n",
      "Epoch [10/10], Iteration [13400/20000], Training Loss: 0.5380\n",
      "Epoch [10/10], Iteration [13500/20000], Training Loss: 0.5243\n",
      "Epoch [10/10], Iteration [13600/20000], Training Loss: 0.4363\n",
      "Epoch [10/10], Iteration [13700/20000], Training Loss: 0.5803\n",
      "Epoch [10/10], Iteration [13800/20000], Training Loss: 0.5240\n",
      "Epoch [10/10], Iteration [13900/20000], Training Loss: 0.4916\n",
      "Epoch [10/10], Iteration [14000/20000], Training Loss: 0.4866\n",
      "Epoch [10/10], Iteration [14100/20000], Training Loss: 0.4281\n",
      "Epoch [10/10], Iteration [14200/20000], Training Loss: 0.6001\n",
      "Epoch [10/10], Iteration [14300/20000], Training Loss: 0.5255\n",
      "Epoch [10/10], Iteration [14400/20000], Training Loss: 0.4334\n",
      "Epoch [10/10], Iteration [14500/20000], Training Loss: 0.5422\n",
      "Epoch [10/10], Iteration [14600/20000], Training Loss: 0.5495\n",
      "Epoch [10/10], Iteration [14700/20000], Training Loss: 0.5319\n",
      "Epoch [10/10], Iteration [14800/20000], Training Loss: 0.5651\n",
      "Epoch [10/10], Iteration [14900/20000], Training Loss: 0.5808\n",
      "Epoch [10/10], Iteration [15000/20000], Training Loss: 0.5356\n",
      "Epoch [10/10], Iteration [15100/20000], Training Loss: 0.5403\n",
      "Epoch [10/10], Iteration [15200/20000], Training Loss: 0.5534\n",
      "Epoch [10/10], Iteration [15300/20000], Training Loss: 0.4337\n",
      "Epoch [10/10], Iteration [15400/20000], Training Loss: 0.5162\n",
      "Epoch [10/10], Iteration [15500/20000], Training Loss: 0.5627\n",
      "Epoch [10/10], Iteration [15600/20000], Training Loss: 0.3898\n",
      "Epoch [10/10], Iteration [15700/20000], Training Loss: 0.4887\n",
      "Epoch [10/10], Iteration [15800/20000], Training Loss: 0.6676\n",
      "Epoch [10/10], Iteration [15900/20000], Training Loss: 0.4463\n",
      "Epoch [10/10], Iteration [16000/20000], Training Loss: 0.6297\n",
      "Epoch [10/10], Iteration [16100/20000], Training Loss: 0.5295\n",
      "Epoch [10/10], Iteration [16200/20000], Training Loss: 0.5456\n",
      "Epoch [10/10], Iteration [16300/20000], Training Loss: 0.6201\n",
      "Epoch [10/10], Iteration [16400/20000], Training Loss: 0.4321\n",
      "Epoch [10/10], Iteration [16500/20000], Training Loss: 0.6238\n",
      "Epoch [10/10], Iteration [16600/20000], Training Loss: 0.5486\n",
      "Epoch [10/10], Iteration [16700/20000], Training Loss: 0.5038\n",
      "Epoch [10/10], Iteration [16800/20000], Training Loss: 0.4681\n",
      "Epoch [10/10], Iteration [16900/20000], Training Loss: 0.5222\n",
      "Epoch [10/10], Iteration [17000/20000], Training Loss: 0.5494\n",
      "Epoch [10/10], Iteration [17100/20000], Training Loss: 0.6407\n",
      "Epoch [10/10], Iteration [17200/20000], Training Loss: 0.5440\n",
      "Epoch [10/10], Iteration [17300/20000], Training Loss: 0.3965\n",
      "Epoch [10/10], Iteration [17400/20000], Training Loss: 0.4926\n",
      "Epoch [10/10], Iteration [17500/20000], Training Loss: 0.4108\n",
      "Epoch [10/10], Iteration [17600/20000], Training Loss: 0.5073\n",
      "Epoch [10/10], Iteration [17700/20000], Training Loss: 0.4679\n",
      "Epoch [10/10], Iteration [17800/20000], Training Loss: 0.6742\n",
      "Epoch [10/10], Iteration [17900/20000], Training Loss: 0.4991\n",
      "Epoch [10/10], Iteration [18000/20000], Training Loss: 0.5652\n",
      "Epoch [10/10], Iteration [18100/20000], Training Loss: 0.4733\n",
      "Epoch [10/10], Iteration [18200/20000], Training Loss: 0.5138\n",
      "Epoch [10/10], Iteration [18300/20000], Training Loss: 0.4307\n",
      "Epoch [10/10], Iteration [18400/20000], Training Loss: 0.4964\n",
      "Epoch [10/10], Iteration [18500/20000], Training Loss: 0.5698\n",
      "Epoch [10/10], Iteration [18600/20000], Training Loss: 0.5334\n",
      "Epoch [10/10], Iteration [18700/20000], Training Loss: 0.4587\n",
      "Epoch [10/10], Iteration [18800/20000], Training Loss: 0.4556\n",
      "Epoch [10/10], Iteration [18900/20000], Training Loss: 0.4844\n",
      "Epoch [10/10], Iteration [19000/20000], Training Loss: 0.4817\n",
      "Epoch [10/10], Iteration [19100/20000], Training Loss: 0.4224\n",
      "Epoch [10/10], Iteration [19200/20000], Training Loss: 0.6041\n",
      "Epoch [10/10], Iteration [19300/20000], Training Loss: 0.5440\n",
      "Epoch [10/10], Iteration [19400/20000], Training Loss: 0.4467\n",
      "Epoch [10/10], Iteration [19500/20000], Training Loss: 0.5988\n",
      "Epoch [10/10], Iteration [19600/20000], Training Loss: 0.5021\n",
      "Epoch [10/10], Iteration [19700/20000], Training Loss: 0.4847\n",
      "Epoch [10/10], Iteration [19800/20000], Training Loss: 0.4683\n",
      "Epoch [10/10], Iteration [19900/20000], Training Loss: 0.4236\n",
      "Epoch [10/10], Iteration [20000/20000], Training Loss: 0.4268\n",
      "Epoch [10/10], Average Training Loss: 0.5089\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define your parameters\n",
    "input_size = 300  # Dimension for word embeddings\n",
    "hidden_size = 8\n",
    "output_size = 2  # Binary classification\n",
    "dropout_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(x)  # No embedding layer, directly use the input\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "#         print(\"lstm_out shape:\", lstm_out.shape)  # Print the shape of lstm_out\n",
    "#         lstm_out = lstm_out[:, -1, :]  # Take the last hidden state of the sequence\n",
    "#         lstm_out = lstm_out[:, -1, :]  # Remove the indexing for time steps\n",
    "\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Convert document_representations into PyTorch tensor\n",
    "document_tensor = torch.stack(document_representations)\n",
    "\n",
    "# Convert labels into PyTorch tensor\n",
    "labels_tensor = torch.tensor(y_train)\n",
    "\n",
    "# Create DataLoader object for training\n",
    "train_dataset = TensorDataset(document_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate your LSTM classifier model\n",
    "model = LSTMClassifier(input_size, hidden_size, output_size, dropout_prob)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Iteration [{i}/{len(train_loader)}], Training Loss: {loss.item():.4f}')\n",
    "    \n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {average_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:37:09.131478Z",
     "iopub.status.busy": "2024-04-27T13:37:09.130638Z",
     "iopub.status.idle": "2024-04-27T13:37:32.386696Z",
     "shell.execute_reply": "2024-04-27T13:37:32.384913Z",
     "shell.execute_reply.started": "2024-04-27T13:37:09.131428Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming 'documents' is a list of documents where each document is a list of word tokens\n",
    "# Assuming 'w2v_model' is your Word2Vec model\n",
    "# Assuming 'oov_vector' is your out-of-vocabulary vector\n",
    "\n",
    "# Create a list to store the document representations\n",
    "test_representations = []\n",
    "\n",
    "# Iterate over each document in the training data\n",
    "for doc in test_documents:\n",
    "    # Get word embeddings for the document's word tokens\n",
    "    word_embeddings = get_word_embeddings(doc, w2v_model, oov_vector)\n",
    "    \n",
    "    # Check if the list of word embeddings is empty\n",
    "    if len(word_embeddings) == 0:\n",
    "        # If the list is empty, assign the OOV vector as the document representation\n",
    "        doc_representation = torch.tensor(oov_vector, dtype=torch.float32)\n",
    "    else:\n",
    "        # Otherwise, aggregate word embeddings (e.g., by averaging)\n",
    "        doc_representation = torch.mean(word_embeddings, dim=0)  # Assuming average pooling\n",
    "    \n",
    "    # Append the document representation to the list\n",
    "    test_representations.append(doc_representation)\n",
    "\n",
    "# Convert the list of document representations into a tensor\n",
    "# document_tensor = torch.stack(document_representations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:37:32.389831Z",
     "iopub.status.busy": "2024-04-27T13:37:32.389408Z",
     "iopub.status.idle": "2024-04-27T13:37:32.397848Z",
     "shell.execute_reply": "2024-04-27T13:37:32.396148Z",
     "shell.execute_reply.started": "2024-04-27T13:37:32.389796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'lstm_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:37:43.111767Z",
     "iopub.status.busy": "2024-04-27T13:37:43.111225Z",
     "iopub.status.idle": "2024-04-27T13:37:43.576800Z",
     "shell.execute_reply": "2024-04-27T13:37:43.575559Z",
     "shell.execute_reply.started": "2024-04-27T13:37:43.111710Z"
    }
   },
   "outputs": [],
   "source": [
    "test_document_tensor = torch.stack(test_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:37:44.462217Z",
     "iopub.status.busy": "2024-04-27T13:37:44.461656Z",
     "iopub.status.idle": "2024-04-27T13:37:49.607995Z",
     "shell.execute_reply": "2024-04-27T13:37:49.606824Z",
     "shell.execute_reply.started": "2024-04-27T13:37:44.462165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 77.54%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess your test data to convert it into word embeddings using word2vec\n",
    "\n",
    "# Assuming test_data_word2vec is a list of word embeddings for each document in your test dataset\n",
    "\n",
    "# Step 2: Convert the word embeddings into PyTorch tensors\n",
    "\n",
    "\n",
    "# Step 3: Create DataLoader object for your test dataset\n",
    "test_dataset = TensorDataset(test_document_tensor, torch.tensor(y_test)) \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Step 4: Pass the test dataset through your trained LSTM model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Step 5: Calculate the accuracy of the model on the test dataset\n",
    "print(f'Accuracy on test data: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**got a zero text dont know why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacked lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T13:42:15.406405Z",
     "iopub.status.busy": "2024-04-27T13:42:15.405637Z",
     "iopub.status.idle": "2024-04-27T13:58:29.751188Z",
     "shell.execute_reply": "2024-04-27T13:58:29.749887Z",
     "shell.execute_reply.started": "2024-04-27T13:42:15.406373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Iteration [1000/20000], Training Loss: 0.5746\n",
      "Epoch [1/15], Iteration [2000/20000], Training Loss: 0.4172\n",
      "Epoch [1/15], Iteration [3000/20000], Training Loss: 0.5321\n",
      "Epoch [1/15], Iteration [4000/20000], Training Loss: 0.5623\n",
      "Epoch [1/15], Iteration [5000/20000], Training Loss: 0.5236\n",
      "Epoch [1/15], Iteration [6000/20000], Training Loss: 0.5114\n",
      "Epoch [1/15], Iteration [7000/20000], Training Loss: 0.3965\n",
      "Epoch [1/15], Iteration [8000/20000], Training Loss: 0.6051\n",
      "Epoch [1/15], Iteration [9000/20000], Training Loss: 0.5456\n",
      "Epoch [1/15], Iteration [10000/20000], Training Loss: 0.5383\n",
      "Epoch [1/15], Iteration [11000/20000], Training Loss: 0.6306\n",
      "Epoch [1/15], Iteration [12000/20000], Training Loss: 0.5273\n",
      "Epoch [1/15], Iteration [13000/20000], Training Loss: 0.5204\n",
      "Epoch [1/15], Iteration [14000/20000], Training Loss: 0.5263\n",
      "Epoch [1/15], Iteration [15000/20000], Training Loss: 0.5094\n",
      "Epoch [1/15], Iteration [16000/20000], Training Loss: 0.5450\n",
      "Epoch [1/15], Iteration [17000/20000], Training Loss: 0.5541\n",
      "Epoch [1/15], Iteration [18000/20000], Training Loss: 0.5514\n",
      "Epoch [1/15], Iteration [19000/20000], Training Loss: 0.5145\n",
      "Epoch [1/15], Iteration [20000/20000], Training Loss: 0.4964\n",
      "Epoch [1/15], Average Training Loss: 0.5308\n",
      "Epoch [2/15], Iteration [1000/20000], Training Loss: 0.4144\n",
      "Epoch [2/15], Iteration [2000/20000], Training Loss: 0.6496\n",
      "Epoch [2/15], Iteration [3000/20000], Training Loss: 0.4772\n",
      "Epoch [2/15], Iteration [4000/20000], Training Loss: 0.5545\n",
      "Epoch [2/15], Iteration [5000/20000], Training Loss: 0.4899\n",
      "Epoch [2/15], Iteration [6000/20000], Training Loss: 0.4904\n",
      "Epoch [2/15], Iteration [7000/20000], Training Loss: 0.4547\n",
      "Epoch [2/15], Iteration [8000/20000], Training Loss: 0.5679\n",
      "Epoch [2/15], Iteration [9000/20000], Training Loss: 0.5704\n",
      "Epoch [2/15], Iteration [10000/20000], Training Loss: 0.5503\n",
      "Epoch [2/15], Iteration [11000/20000], Training Loss: 0.4963\n",
      "Epoch [2/15], Iteration [12000/20000], Training Loss: 0.6175\n",
      "Epoch [2/15], Iteration [13000/20000], Training Loss: 0.5014\n",
      "Epoch [2/15], Iteration [14000/20000], Training Loss: 0.3707\n",
      "Epoch [2/15], Iteration [15000/20000], Training Loss: 0.5035\n",
      "Epoch [2/15], Iteration [16000/20000], Training Loss: 0.5481\n",
      "Epoch [2/15], Iteration [17000/20000], Training Loss: 0.4611\n",
      "Epoch [2/15], Iteration [18000/20000], Training Loss: 0.5325\n",
      "Epoch [2/15], Iteration [19000/20000], Training Loss: 0.6274\n",
      "Epoch [2/15], Iteration [20000/20000], Training Loss: 0.4311\n",
      "Epoch [2/15], Average Training Loss: 0.5164\n",
      "Epoch [3/15], Iteration [1000/20000], Training Loss: 0.5659\n",
      "Epoch [3/15], Iteration [2000/20000], Training Loss: 0.5100\n",
      "Epoch [3/15], Iteration [3000/20000], Training Loss: 0.5051\n",
      "Epoch [3/15], Iteration [4000/20000], Training Loss: 0.5349\n",
      "Epoch [3/15], Iteration [5000/20000], Training Loss: 0.5092\n",
      "Epoch [3/15], Iteration [6000/20000], Training Loss: 0.4214\n",
      "Epoch [3/15], Iteration [7000/20000], Training Loss: 0.4719\n",
      "Epoch [3/15], Iteration [8000/20000], Training Loss: 0.4367\n",
      "Epoch [3/15], Iteration [9000/20000], Training Loss: 0.6025\n",
      "Epoch [3/15], Iteration [10000/20000], Training Loss: 0.4263\n",
      "Epoch [3/15], Iteration [11000/20000], Training Loss: 0.4448\n",
      "Epoch [3/15], Iteration [12000/20000], Training Loss: 0.4486\n",
      "Epoch [3/15], Iteration [13000/20000], Training Loss: 0.5259\n",
      "Epoch [3/15], Iteration [14000/20000], Training Loss: 0.5598\n",
      "Epoch [3/15], Iteration [15000/20000], Training Loss: 0.6797\n",
      "Epoch [3/15], Iteration [16000/20000], Training Loss: 0.5079\n",
      "Epoch [3/15], Iteration [17000/20000], Training Loss: 0.5505\n",
      "Epoch [3/15], Iteration [18000/20000], Training Loss: 0.4584\n",
      "Epoch [3/15], Iteration [19000/20000], Training Loss: 0.6204\n",
      "Epoch [3/15], Iteration [20000/20000], Training Loss: 0.5145\n",
      "Epoch [3/15], Average Training Loss: 0.5127\n",
      "Epoch [4/15], Iteration [1000/20000], Training Loss: 0.4669\n",
      "Epoch [5/15], Iteration [9000/20000], Training Loss: 0.4609\n",
      "Epoch [5/15], Iteration [10000/20000], Training Loss: 0.7096\n",
      "Epoch [5/15], Iteration [11000/20000], Training Loss: 0.5298\n",
      "Epoch [5/15], Iteration [12000/20000], Training Loss: 0.5334\n",
      "Epoch [5/15], Iteration [13000/20000], Training Loss: 0.5712\n",
      "Epoch [5/15], Iteration [14000/20000], Training Loss: 0.4815\n",
      "Epoch [5/15], Iteration [15000/20000], Training Loss: 0.4951\n",
      "Epoch [5/15], Iteration [16000/20000], Training Loss: 0.4848\n",
      "Epoch [5/15], Iteration [17000/20000], Training Loss: 0.4693\n",
      "Epoch [5/15], Iteration [18000/20000], Training Loss: 0.4551\n",
      "Epoch [5/15], Iteration [19000/20000], Training Loss: 0.4893\n",
      "Epoch [5/15], Iteration [20000/20000], Training Loss: 0.4086\n",
      "Epoch [5/15], Average Training Loss: 0.5105\n",
      "Epoch [6/15], Iteration [1000/20000], Training Loss: 0.4197\n",
      "Epoch [6/15], Iteration [2000/20000], Training Loss: 0.4083\n",
      "Epoch [6/15], Iteration [3000/20000], Training Loss: 0.4336\n",
      "Epoch [6/15], Iteration [4000/20000], Training Loss: 0.5126\n",
      "Epoch [6/15], Iteration [5000/20000], Training Loss: 0.5245\n",
      "Epoch [6/15], Iteration [6000/20000], Training Loss: 0.4160\n",
      "Epoch [6/15], Iteration [7000/20000], Training Loss: 0.5215\n",
      "Epoch [6/15], Iteration [8000/20000], Training Loss: 0.5350\n",
      "Epoch [6/15], Iteration [9000/20000], Training Loss: 0.5857\n",
      "Epoch [6/15], Iteration [10000/20000], Training Loss: 0.4850\n",
      "Epoch [6/15], Iteration [11000/20000], Training Loss: 0.4884\n",
      "Epoch [6/15], Iteration [12000/20000], Training Loss: 0.4463\n",
      "Epoch [6/15], Iteration [13000/20000], Training Loss: 0.5306\n",
      "Epoch [6/15], Iteration [14000/20000], Training Loss: 0.5750\n",
      "Epoch [6/15], Iteration [15000/20000], Training Loss: 0.5461\n",
      "Epoch [6/15], Iteration [16000/20000], Training Loss: 0.4434\n",
      "Epoch [6/15], Iteration [17000/20000], Training Loss: 0.5174\n",
      "Epoch [6/15], Iteration [18000/20000], Training Loss: 0.5682\n",
      "Epoch [6/15], Iteration [19000/20000], Training Loss: 0.4541\n",
      "Epoch [6/15], Iteration [20000/20000], Training Loss: 0.6116\n",
      "Epoch [6/15], Average Training Loss: 0.5101\n",
      "Epoch [7/15], Iteration [1000/20000], Training Loss: 0.4912\n",
      "Epoch [7/15], Iteration [2000/20000], Training Loss: 0.4784\n",
      "Epoch [7/15], Iteration [3000/20000], Training Loss: 0.4843\n",
      "Epoch [7/15], Iteration [4000/20000], Training Loss: 0.4544\n",
      "Epoch [7/15], Iteration [5000/20000], Training Loss: 0.3967\n",
      "Epoch [7/15], Iteration [6000/20000], Training Loss: 0.5155\n",
      "Epoch [7/15], Iteration [7000/20000], Training Loss: 0.6222\n",
      "Epoch [7/15], Iteration [8000/20000], Training Loss: 0.3802\n",
      "Epoch [7/15], Iteration [9000/20000], Training Loss: 0.4725\n",
      "Epoch [7/15], Iteration [10000/20000], Training Loss: 0.4649\n",
      "Epoch [7/15], Iteration [11000/20000], Training Loss: 0.5317\n",
      "Epoch [7/15], Iteration [12000/20000], Training Loss: 0.4757\n",
      "Epoch [7/15], Iteration [13000/20000], Training Loss: 0.5653\n",
      "Epoch [7/15], Iteration [14000/20000], Training Loss: 0.4789\n",
      "Epoch [7/15], Iteration [15000/20000], Training Loss: 0.4972\n",
      "Epoch [7/15], Iteration [16000/20000], Training Loss: 0.4443\n",
      "Epoch [7/15], Iteration [17000/20000], Training Loss: 0.5480\n",
      "Epoch [7/15], Iteration [18000/20000], Training Loss: 0.5452\n",
      "Epoch [7/15], Iteration [19000/20000], Training Loss: 0.5737\n",
      "Epoch [7/15], Iteration [20000/20000], Training Loss: 0.4819\n",
      "Epoch [7/15], Average Training Loss: 0.5095\n",
      "Epoch [8/15], Iteration [1000/20000], Training Loss: 0.6494\n",
      "Epoch [8/15], Iteration [2000/20000], Training Loss: 0.5606\n",
      "Epoch [8/15], Iteration [3000/20000], Training Loss: 0.4513\n",
      "Epoch [8/15], Iteration [4000/20000], Training Loss: 0.4448\n",
      "Epoch [8/15], Iteration [5000/20000], Training Loss: 0.5466\n",
      "Epoch [8/15], Iteration [6000/20000], Training Loss: 0.4261\n",
      "Epoch [8/15], Iteration [7000/20000], Training Loss: 0.6630\n",
      "Epoch [8/15], Iteration [8000/20000], Training Loss: 0.4969\n",
      "Epoch [8/15], Iteration [9000/20000], Training Loss: 0.4713\n",
      "Epoch [8/15], Iteration [10000/20000], Training Loss: 0.4921\n",
      "Epoch [8/15], Iteration [11000/20000], Training Loss: 0.5287\n",
      "Epoch [8/15], Iteration [12000/20000], Training Loss: 0.5146\n",
      "Epoch [8/15], Iteration [13000/20000], Training Loss: 0.6443\n",
      "Epoch [8/15], Iteration [14000/20000], Training Loss: 0.5513\n",
      "Epoch [8/15], Iteration [15000/20000], Training Loss: 0.5157\n",
      "Epoch [8/15], Iteration [16000/20000], Training Loss: 0.4322\n",
      "Epoch [8/15], Iteration [17000/20000], Training Loss: 0.4729\n",
      "Epoch [8/15], Iteration [18000/20000], Training Loss: 0.4543\n",
      "Epoch [8/15], Iteration [19000/20000], Training Loss: 0.5008\n",
      "Epoch [8/15], Iteration [20000/20000], Training Loss: 0.5691\n",
      "Epoch [8/15], Average Training Loss: 0.5093\n",
      "Epoch [9/15], Iteration [1000/20000], Training Loss: 0.4933\n",
      "Epoch [9/15], Iteration [2000/20000], Training Loss: 0.5344\n",
      "Epoch [9/15], Iteration [3000/20000], Training Loss: 0.3800\n",
      "Epoch [9/15], Iteration [4000/20000], Training Loss: 0.4036\n",
      "Epoch [9/15], Iteration [5000/20000], Training Loss: 0.4723\n",
      "Epoch [9/15], Iteration [6000/20000], Training Loss: 0.5241\n",
      "Epoch [9/15], Iteration [7000/20000], Training Loss: 0.5034\n",
      "Epoch [9/15], Iteration [8000/20000], Training Loss: 0.5580\n",
      "Epoch [9/15], Iteration [9000/20000], Training Loss: 0.5805\n",
      "Epoch [9/15], Iteration [10000/20000], Training Loss: 0.5581\n",
      "Epoch [9/15], Iteration [11000/20000], Training Loss: 0.4772\n",
      "Epoch [9/15], Iteration [12000/20000], Training Loss: 0.6511\n",
      "Epoch [9/15], Iteration [13000/20000], Training Loss: 0.5016\n",
      "Epoch [9/15], Iteration [14000/20000], Training Loss: 0.4722\n",
      "Epoch [9/15], Iteration [15000/20000], Training Loss: 0.4769\n",
      "Epoch [9/15], Iteration [16000/20000], Training Loss: 0.5243\n",
      "Epoch [9/15], Iteration [17000/20000], Training Loss: 0.6018\n",
      "Epoch [9/15], Iteration [18000/20000], Training Loss: 0.5054\n",
      "Epoch [9/15], Iteration [19000/20000], Training Loss: 0.5701\n",
      "Epoch [9/15], Iteration [20000/20000], Training Loss: 0.4851\n",
      "Epoch [9/15], Average Training Loss: 0.5090\n",
      "Epoch [10/15], Iteration [1000/20000], Training Loss: 0.4419\n",
      "Epoch [10/15], Iteration [2000/20000], Training Loss: 0.5240\n",
      "Epoch [10/15], Iteration [3000/20000], Training Loss: 0.4289\n",
      "Epoch [10/15], Iteration [4000/20000], Training Loss: 0.4166\n",
      "Epoch [10/15], Iteration [5000/20000], Training Loss: 0.4691\n",
      "Epoch [10/15], Iteration [6000/20000], Training Loss: 0.5810\n",
      "Epoch [10/15], Iteration [7000/20000], Training Loss: 0.6596\n",
      "Epoch [10/15], Iteration [8000/20000], Training Loss: 0.5562\n",
      "Epoch [10/15], Iteration [9000/20000], Training Loss: 0.3730\n",
      "Epoch [10/15], Iteration [10000/20000], Training Loss: 0.5447\n",
      "Epoch [10/15], Iteration [11000/20000], Training Loss: 0.4716\n",
      "Epoch [10/15], Iteration [12000/20000], Training Loss: 0.5147\n",
      "Epoch [10/15], Iteration [13000/20000], Training Loss: 0.5377\n",
      "Epoch [10/15], Iteration [14000/20000], Training Loss: 0.6640\n",
      "Epoch [10/15], Iteration [15000/20000], Training Loss: 0.5971\n",
      "Epoch [10/15], Iteration [16000/20000], Training Loss: 0.5938\n",
      "Epoch [10/15], Iteration [17000/20000], Training Loss: 0.5402\n",
      "Epoch [10/15], Iteration [18000/20000], Training Loss: 0.5713\n",
      "Epoch [10/15], Iteration [19000/20000], Training Loss: 0.4098\n",
      "Epoch [10/15], Iteration [20000/20000], Training Loss: 0.5379\n",
      "Epoch [10/15], Average Training Loss: 0.5087\n",
      "Epoch [11/15], Iteration [1000/20000], Training Loss: 0.4923\n",
      "Epoch [11/15], Iteration [2000/20000], Training Loss: 0.5079\n",
      "Epoch [11/15], Iteration [3000/20000], Training Loss: 0.4184\n",
      "Epoch [11/15], Iteration [4000/20000], Training Loss: 0.4915\n",
      "Epoch [11/15], Iteration [5000/20000], Training Loss: 0.5681\n",
      "Epoch [11/15], Iteration [6000/20000], Training Loss: 0.3601\n",
      "Epoch [11/15], Iteration [7000/20000], Training Loss: 0.4005\n",
      "Epoch [11/15], Iteration [8000/20000], Training Loss: 0.5080\n",
      "Epoch [11/15], Iteration [9000/20000], Training Loss: 0.4872\n",
      "Epoch [11/15], Iteration [10000/20000], Training Loss: 0.5589\n",
      "Epoch [11/15], Iteration [11000/20000], Training Loss: 0.4621\n",
      "Epoch [11/15], Iteration [12000/20000], Training Loss: 0.4370\n",
      "Epoch [11/15], Iteration [13000/20000], Training Loss: 0.5899\n",
      "Epoch [11/15], Iteration [14000/20000], Training Loss: 0.5758\n",
      "Epoch [11/15], Iteration [15000/20000], Training Loss: 0.5582\n",
      "Epoch [11/15], Iteration [16000/20000], Training Loss: 0.4848\n",
      "Epoch [11/15], Iteration [17000/20000], Training Loss: 0.4602\n",
      "Epoch [11/15], Iteration [18000/20000], Training Loss: 0.4976\n",
      "Epoch [11/15], Iteration [19000/20000], Training Loss: 0.4755\n",
      "Epoch [11/15], Iteration [20000/20000], Training Loss: 0.5004\n",
      "Epoch [11/15], Average Training Loss: 0.5088\n",
      "Epoch [12/15], Iteration [1000/20000], Training Loss: 0.4728\n",
      "Epoch [12/15], Iteration [2000/20000], Training Loss: 0.5399\n",
      "Epoch [12/15], Iteration [3000/20000], Training Loss: 0.4930\n",
      "Epoch [12/15], Iteration [4000/20000], Training Loss: 0.5654\n",
      "Epoch [12/15], Iteration [5000/20000], Training Loss: 0.5334\n",
      "Epoch [12/15], Iteration [6000/20000], Training Loss: 0.5113\n",
      "Epoch [12/15], Iteration [7000/20000], Training Loss: 0.5251\n",
      "Epoch [12/15], Iteration [8000/20000], Training Loss: 0.5044\n",
      "Epoch [12/15], Iteration [9000/20000], Training Loss: 0.5054\n",
      "Epoch [12/15], Iteration [10000/20000], Training Loss: 0.5478\n",
      "Epoch [12/15], Iteration [11000/20000], Training Loss: 0.4493\n",
      "Epoch [12/15], Iteration [12000/20000], Training Loss: 0.4528\n",
      "Epoch [12/15], Iteration [13000/20000], Training Loss: 0.5862\n",
      "Epoch [12/15], Iteration [14000/20000], Training Loss: 0.5694\n",
      "Epoch [12/15], Iteration [15000/20000], Training Loss: 0.5853\n",
      "Epoch [12/15], Iteration [16000/20000], Training Loss: 0.5835\n",
      "Epoch [12/15], Iteration [17000/20000], Training Loss: 0.5024\n",
      "Epoch [12/15], Iteration [18000/20000], Training Loss: 0.5581\n",
      "Epoch [12/15], Iteration [19000/20000], Training Loss: 0.5811\n",
      "Epoch [12/15], Iteration [20000/20000], Training Loss: 0.5634\n",
      "Epoch [12/15], Average Training Loss: 0.5081\n",
      "Epoch [13/15], Iteration [1000/20000], Training Loss: 0.4705\n",
      "Epoch [13/15], Iteration [2000/20000], Training Loss: 0.4740\n",
      "Epoch [13/15], Iteration [3000/20000], Training Loss: 0.5440\n",
      "Epoch [13/15], Iteration [4000/20000], Training Loss: 0.4806\n",
      "Epoch [13/15], Iteration [5000/20000], Training Loss: 0.4884\n",
      "Epoch [13/15], Iteration [6000/20000], Training Loss: 0.6392\n",
      "Epoch [13/15], Iteration [7000/20000], Training Loss: 0.5547\n",
      "Epoch [13/15], Iteration [8000/20000], Training Loss: 0.5085\n",
      "Epoch [13/15], Iteration [9000/20000], Training Loss: 0.4895\n",
      "Epoch [13/15], Iteration [10000/20000], Training Loss: 0.5210\n",
      "Epoch [13/15], Iteration [11000/20000], Training Loss: 0.4822\n",
      "Epoch [13/15], Iteration [12000/20000], Training Loss: 0.5451\n",
      "Epoch [13/15], Iteration [13000/20000], Training Loss: 0.4312\n",
      "Epoch [13/15], Iteration [14000/20000], Training Loss: 0.4731\n",
      "Epoch [13/15], Iteration [15000/20000], Training Loss: 0.5406\n",
      "Epoch [13/15], Iteration [16000/20000], Training Loss: 0.5532\n",
      "Epoch [13/15], Iteration [17000/20000], Training Loss: 0.4280\n",
      "Epoch [13/15], Iteration [18000/20000], Training Loss: 0.4868\n",
      "Epoch [13/15], Iteration [19000/20000], Training Loss: 0.4764\n",
      "Epoch [13/15], Iteration [20000/20000], Training Loss: 0.4524\n",
      "Epoch [13/15], Average Training Loss: 0.5081\n",
      "Epoch [14/15], Iteration [1000/20000], Training Loss: 0.4135\n",
      "Epoch [14/15], Iteration [2000/20000], Training Loss: 0.4876\n",
      "Epoch [14/15], Iteration [3000/20000], Training Loss: 0.6275\n",
      "Epoch [14/15], Iteration [4000/20000], Training Loss: 0.5277\n",
      "Epoch [14/15], Iteration [5000/20000], Training Loss: 0.4312\n",
      "Epoch [14/15], Iteration [6000/20000], Training Loss: 0.5352\n",
      "Epoch [14/15], Iteration [7000/20000], Training Loss: 0.4976\n",
      "Epoch [14/15], Iteration [8000/20000], Training Loss: 0.4861\n",
      "Epoch [14/15], Iteration [9000/20000], Training Loss: 0.5180\n",
      "Epoch [14/15], Iteration [10000/20000], Training Loss: 0.5175\n",
      "Epoch [14/15], Iteration [11000/20000], Training Loss: 0.4871\n",
      "Epoch [14/15], Iteration [12000/20000], Training Loss: 0.5202\n",
      "Epoch [14/15], Iteration [13000/20000], Training Loss: 0.5740\n",
      "Epoch [14/15], Iteration [14000/20000], Training Loss: 0.5029\n",
      "Epoch [14/15], Iteration [15000/20000], Training Loss: 0.3320\n",
      "Epoch [14/15], Iteration [16000/20000], Training Loss: 0.4297\n",
      "Epoch [14/15], Iteration [17000/20000], Training Loss: 0.5659\n",
      "Epoch [14/15], Iteration [18000/20000], Training Loss: 0.5644\n",
      "Epoch [14/15], Iteration [19000/20000], Training Loss: 0.5619\n",
      "Epoch [14/15], Iteration [20000/20000], Training Loss: 0.4863\n",
      "Epoch [14/15], Average Training Loss: 0.5086\n",
      "Epoch [15/15], Iteration [1000/20000], Training Loss: 0.5227\n",
      "Epoch [15/15], Iteration [2000/20000], Training Loss: 0.4664\n",
      "Epoch [15/15], Iteration [3000/20000], Training Loss: 0.3606\n",
      "Epoch [15/15], Iteration [4000/20000], Training Loss: 0.4383\n",
      "Epoch [15/15], Iteration [5000/20000], Training Loss: 0.4856\n",
      "Epoch [15/15], Iteration [6000/20000], Training Loss: 0.5551\n",
      "Epoch [15/15], Iteration [7000/20000], Training Loss: 0.4877\n",
      "Epoch [15/15], Iteration [8000/20000], Training Loss: 0.5691\n",
      "Epoch [15/15], Iteration [9000/20000], Training Loss: 0.4945\n",
      "Epoch [15/15], Iteration [10000/20000], Training Loss: 0.4427\n",
      "Epoch [15/15], Iteration [11000/20000], Training Loss: 0.4956\n",
      "Epoch [15/15], Iteration [12000/20000], Training Loss: 0.4954\n",
      "Epoch [15/15], Iteration [13000/20000], Training Loss: 0.4209\n",
      "Epoch [15/15], Iteration [14000/20000], Training Loss: 0.5805\n",
      "Epoch [15/15], Iteration [15000/20000], Training Loss: 0.5632\n",
      "Epoch [15/15], Iteration [16000/20000], Training Loss: 0.4663\n",
      "Epoch [15/15], Iteration [17000/20000], Training Loss: 0.4198\n",
      "Epoch [15/15], Iteration [18000/20000], Training Loss: 0.5174\n",
      "Epoch [15/15], Iteration [19000/20000], Training Loss: 0.4913\n",
      "Epoch [15/15], Iteration [20000/20000], Training Loss: 0.5973\n",
      "Epoch [15/15], Average Training Loss: 0.5083\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define your parameters\n",
    "input_size = 300  # Dimension for word embeddings\n",
    "hidden_size = 8\n",
    "output_size = 2  # Binary classification\n",
    "dropout_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(x)  # No embedding layer, directly use the input\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)  # Take the last hidden state of the last layer\n",
    "        return output\n",
    "\n",
    "# Convert document_representations into PyTorch tensor\n",
    "document_tensor = torch.stack(document_representations)\n",
    "\n",
    "# Convert labels into PyTorch tensor\n",
    "labels_tensor = torch.tensor(y_train)\n",
    "\n",
    "# Create DataLoader object for training\n",
    "train_dataset = TensorDataset(document_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate your LSTM classifier model\n",
    "model = LSTMClassifier(input_size, hidden_size, output_size, num_layers, dropout_prob)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Iteration [{i}/{len(train_loader)}], Training Loss: {loss.item():.4f}')\n",
    "    \n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T14:52:40.990944Z",
     "iopub.status.busy": "2024-04-27T14:52:40.990358Z",
     "iopub.status.idle": "2024-04-27T14:52:40.998654Z",
     "shell.execute_reply": "2024-04-27T14:52:40.996456Z",
     "shell.execute_reply.started": "2024-04-27T14:52:40.990901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'stacked_lstm_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T14:53:40.317608Z",
     "iopub.status.busy": "2024-04-27T14:53:40.317210Z",
     "iopub.status.idle": "2024-04-27T14:53:47.090662Z",
     "shell.execute_reply": "2024-04-27T14:53:47.088954Z",
     "shell.execute_reply.started": "2024-04-27T14:53:40.317579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data for stacked lstm: 77.51%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Step 5: Calculate the accuracy of the model on the test dataset\n",
    "print(f'Accuracy on test data for stacked lstm: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T14:56:10.456068Z",
     "iopub.status.busy": "2024-04-27T14:56:10.455447Z",
     "iopub.status.idle": "2024-04-27T15:14:02.285267Z",
     "shell.execute_reply": "2024-04-27T15:14:02.284309Z",
     "shell.execute_reply.started": "2024-04-27T14:56:10.456026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Iteration [1000/20000], Training Loss: 0.5228\n",
      "Epoch [1/15], Iteration [2000/20000], Training Loss: 0.4955\n",
      "Epoch [1/15], Iteration [3000/20000], Training Loss: 0.5524\n",
      "Epoch [1/15], Iteration [4000/20000], Training Loss: 0.4869\n",
      "Epoch [1/15], Iteration [5000/20000], Training Loss: 0.4482\n",
      "Epoch [1/15], Iteration [6000/20000], Training Loss: 0.4667\n",
      "Epoch [1/15], Iteration [7000/20000], Training Loss: 0.5073\n",
      "Epoch [1/15], Iteration [8000/20000], Training Loss: 0.5122\n",
      "Epoch [1/15], Iteration [9000/20000], Training Loss: 0.5444\n",
      "Epoch [1/15], Iteration [10000/20000], Training Loss: 0.5255\n",
      "Epoch [1/15], Iteration [11000/20000], Training Loss: 0.6131\n",
      "Epoch [1/15], Iteration [12000/20000], Training Loss: 0.5082\n",
      "Epoch [1/15], Iteration [13000/20000], Training Loss: 0.5113\n",
      "Epoch [1/15], Iteration [14000/20000], Training Loss: 0.4812\n",
      "Epoch [1/15], Iteration [15000/20000], Training Loss: 0.4838\n",
      "Epoch [1/15], Iteration [16000/20000], Training Loss: 0.3793\n",
      "Epoch [1/15], Iteration [17000/20000], Training Loss: 0.5480\n",
      "Epoch [1/15], Iteration [18000/20000], Training Loss: 0.5495\n",
      "Epoch [1/15], Iteration [19000/20000], Training Loss: 0.5385\n",
      "Epoch [1/15], Iteration [20000/20000], Training Loss: 0.5665\n",
      "Epoch [1/15], Average Training Loss: 0.5240\n",
      "Epoch [2/15], Iteration [1000/20000], Training Loss: 0.5767\n",
      "Epoch [2/15], Iteration [2000/20000], Training Loss: 0.4074\n",
      "Epoch [2/15], Iteration [3000/20000], Training Loss: 0.5641\n",
      "Epoch [2/15], Iteration [4000/20000], Training Loss: 0.5491\n",
      "Epoch [2/15], Iteration [5000/20000], Training Loss: 0.4891\n",
      "Epoch [2/15], Iteration [6000/20000], Training Loss: 0.6305\n",
      "Epoch [2/15], Iteration [7000/20000], Training Loss: 0.4530\n",
      "Epoch [2/15], Iteration [8000/20000], Training Loss: 0.5081\n",
      "Epoch [2/15], Iteration [9000/20000], Training Loss: 0.5363\n",
      "Epoch [2/15], Iteration [10000/20000], Training Loss: 0.5755\n",
      "Epoch [2/15], Iteration [11000/20000], Training Loss: 0.6161\n",
      "Epoch [2/15], Iteration [12000/20000], Training Loss: 0.3695\n",
      "Epoch [2/15], Iteration [13000/20000], Training Loss: 0.5541\n",
      "Epoch [2/15], Iteration [14000/20000], Training Loss: 0.3834\n",
      "Epoch [2/15], Iteration [15000/20000], Training Loss: 0.5088\n",
      "Epoch [2/15], Iteration [16000/20000], Training Loss: 0.4229\n",
      "Epoch [2/15], Iteration [17000/20000], Training Loss: 0.6357\n",
      "Epoch [2/15], Iteration [18000/20000], Training Loss: 0.5194\n",
      "Epoch [2/15], Iteration [19000/20000], Training Loss: 0.4536\n",
      "Epoch [2/15], Iteration [20000/20000], Training Loss: 0.6237\n",
      "Epoch [2/15], Average Training Loss: 0.5094\n",
      "Epoch [3/15], Iteration [1000/20000], Training Loss: 0.3870\n",
      "Epoch [3/15], Iteration [2000/20000], Training Loss: 0.5180\n",
      "Epoch [3/15], Iteration [3000/20000], Training Loss: 0.5382\n",
      "Epoch [3/15], Iteration [4000/20000], Training Loss: 0.5688\n",
      "Epoch [3/15], Iteration [5000/20000], Training Loss: 0.4040\n",
      "Epoch [3/15], Iteration [6000/20000], Training Loss: 0.3458\n",
      "Epoch [3/15], Iteration [7000/20000], Training Loss: 0.4466\n",
      "Epoch [3/15], Iteration [8000/20000], Training Loss: 0.5434\n",
      "Epoch [3/15], Iteration [9000/20000], Training Loss: 0.5555\n",
      "Epoch [3/15], Iteration [10000/20000], Training Loss: 0.5052\n",
      "Epoch [3/15], Iteration [11000/20000], Training Loss: 0.4857\n",
      "Epoch [3/15], Iteration [12000/20000], Training Loss: 0.5661\n",
      "Epoch [3/15], Iteration [13000/20000], Training Loss: 0.5634\n",
      "Epoch [3/15], Iteration [14000/20000], Training Loss: 0.5948\n",
      "Epoch [3/15], Iteration [15000/20000], Training Loss: 0.5748\n",
      "Epoch [3/15], Iteration [16000/20000], Training Loss: 0.5551\n",
      "Epoch [3/15], Iteration [17000/20000], Training Loss: 0.4960\n",
      "Epoch [3/15], Iteration [18000/20000], Training Loss: 0.5428\n",
      "Epoch [3/15], Iteration [19000/20000], Training Loss: 0.4587\n",
      "Epoch [3/15], Iteration [20000/20000], Training Loss: 0.5473\n",
      "Epoch [3/15], Average Training Loss: 0.5061\n",
      "Epoch [4/15], Iteration [1000/20000], Training Loss: 0.5042\n",
      "Epoch [4/15], Iteration [2000/20000], Training Loss: 0.3978\n",
      "Epoch [4/15], Iteration [3000/20000], Training Loss: 0.6258\n",
      "Epoch [4/15], Iteration [4000/20000], Training Loss: 0.5216\n",
      "Epoch [4/15], Iteration [5000/20000], Training Loss: 0.5236\n",
      "Epoch [4/15], Iteration [6000/20000], Training Loss: 0.3649\n",
      "Epoch [4/15], Iteration [7000/20000], Training Loss: 0.4931\n",
      "Epoch [4/15], Iteration [8000/20000], Training Loss: 0.5636\n",
      "Epoch [4/15], Iteration [9000/20000], Training Loss: 0.5301\n",
      "Epoch [4/15], Iteration [10000/20000], Training Loss: 0.5716\n",
      "Epoch [4/15], Iteration [11000/20000], Training Loss: 0.5635\n",
      "Epoch [4/15], Iteration [12000/20000], Training Loss: 0.5988\n",
      "Epoch [4/15], Iteration [13000/20000], Training Loss: 0.4064\n",
      "Epoch [4/15], Iteration [14000/20000], Training Loss: 0.4208\n",
      "Epoch [4/15], Iteration [15000/20000], Training Loss: 0.4877\n",
      "Epoch [4/15], Iteration [16000/20000], Training Loss: 0.5048\n",
      "Epoch [4/15], Iteration [17000/20000], Training Loss: 0.6078\n",
      "Epoch [4/15], Iteration [18000/20000], Training Loss: 0.5483\n",
      "Epoch [4/15], Iteration [19000/20000], Training Loss: 0.3619\n",
      "Epoch [4/15], Iteration [20000/20000], Training Loss: 0.4677\n",
      "Epoch [4/15], Average Training Loss: 0.5048\n",
      "Epoch [5/15], Iteration [1000/20000], Training Loss: 0.4643\n",
      "Epoch [5/15], Iteration [2000/20000], Training Loss: 0.4129\n",
      "Epoch [5/15], Iteration [3000/20000], Training Loss: 0.4846\n",
      "Epoch [5/15], Iteration [4000/20000], Training Loss: 0.5559\n",
      "Epoch [5/15], Iteration [5000/20000], Training Loss: 0.4484\n",
      "Epoch [5/15], Iteration [6000/20000], Training Loss: 0.4949\n",
      "Epoch [5/15], Iteration [7000/20000], Training Loss: 0.5081\n",
      "Epoch [5/15], Iteration [8000/20000], Training Loss: 0.5671\n",
      "Epoch [5/15], Iteration [9000/20000], Training Loss: 0.5440\n",
      "Epoch [5/15], Iteration [10000/20000], Training Loss: 0.5233\n",
      "Epoch [5/15], Iteration [11000/20000], Training Loss: 0.4342\n",
      "Epoch [5/15], Iteration [12000/20000], Training Loss: 0.5022\n",
      "Epoch [5/15], Iteration [13000/20000], Training Loss: 0.4950\n",
      "Epoch [5/15], Iteration [14000/20000], Training Loss: 0.4920\n",
      "Epoch [5/15], Iteration [15000/20000], Training Loss: 0.5311\n",
      "Epoch [5/15], Iteration [16000/20000], Training Loss: 0.4780\n",
      "Epoch [5/15], Iteration [17000/20000], Training Loss: 0.6543\n",
      "Epoch [5/15], Iteration [18000/20000], Training Loss: 0.4642\n",
      "Epoch [5/15], Iteration [19000/20000], Training Loss: 0.5377\n",
      "Epoch [5/15], Iteration [20000/20000], Training Loss: 0.4470\n",
      "Epoch [5/15], Average Training Loss: 0.5039\n",
      "Epoch [6/15], Iteration [1000/20000], Training Loss: 0.4347\n",
      "Epoch [6/15], Iteration [2000/20000], Training Loss: 0.4835\n",
      "Epoch [6/15], Iteration [3000/20000], Training Loss: 0.4866\n",
      "Epoch [6/15], Iteration [4000/20000], Training Loss: 0.4719\n",
      "Epoch [6/15], Iteration [5000/20000], Training Loss: 0.4102\n",
      "Epoch [6/15], Iteration [6000/20000], Training Loss: 0.6305\n",
      "Epoch [6/15], Iteration [7000/20000], Training Loss: 0.5434\n",
      "Epoch [6/15], Iteration [8000/20000], Training Loss: 0.3629\n",
      "Epoch [6/15], Iteration [9000/20000], Training Loss: 0.5378\n",
      "Epoch [6/15], Iteration [10000/20000], Training Loss: 0.3956\n",
      "Epoch [6/15], Iteration [11000/20000], Training Loss: 0.4159\n",
      "Epoch [6/15], Iteration [12000/20000], Training Loss: 0.4512\n",
      "Epoch [6/15], Iteration [13000/20000], Training Loss: 0.5108\n",
      "Epoch [6/15], Iteration [14000/20000], Training Loss: 0.4756\n",
      "Epoch [6/15], Iteration [15000/20000], Training Loss: 0.4516\n",
      "Epoch [6/15], Iteration [16000/20000], Training Loss: 0.5981\n",
      "Epoch [6/15], Iteration [17000/20000], Training Loss: 0.5601\n",
      "Epoch [6/15], Iteration [18000/20000], Training Loss: 0.4487\n",
      "Epoch [6/15], Iteration [19000/20000], Training Loss: 0.4880\n",
      "Epoch [6/15], Iteration [20000/20000], Training Loss: 0.5599\n",
      "Epoch [6/15], Average Training Loss: 0.5031\n",
      "Epoch [7/15], Iteration [1000/20000], Training Loss: 0.4406\n",
      "Epoch [7/15], Iteration [2000/20000], Training Loss: 0.4088\n",
      "Epoch [7/15], Iteration [3000/20000], Training Loss: 0.3841\n",
      "Epoch [7/15], Iteration [4000/20000], Training Loss: 0.5392\n",
      "Epoch [7/15], Iteration [5000/20000], Training Loss: 0.4578\n",
      "Epoch [7/15], Iteration [6000/20000], Training Loss: 0.5488\n",
      "Epoch [7/15], Iteration [7000/20000], Training Loss: 0.5226\n",
      "Epoch [7/15], Iteration [8000/20000], Training Loss: 0.4117\n",
      "Epoch [7/15], Iteration [9000/20000], Training Loss: 0.4746\n",
      "Epoch [7/15], Iteration [10000/20000], Training Loss: 0.5226\n",
      "Epoch [7/15], Iteration [11000/20000], Training Loss: 0.5114\n",
      "Epoch [7/15], Iteration [12000/20000], Training Loss: 0.5038\n",
      "Epoch [7/15], Iteration [13000/20000], Training Loss: 0.5564\n",
      "Epoch [7/15], Iteration [14000/20000], Training Loss: 0.5776\n",
      "Epoch [7/15], Iteration [15000/20000], Training Loss: 0.5926\n",
      "Epoch [7/15], Iteration [16000/20000], Training Loss: 0.4174\n",
      "Epoch [7/15], Iteration [17000/20000], Training Loss: 0.5768\n",
      "Epoch [7/15], Iteration [18000/20000], Training Loss: 0.5134\n",
      "Epoch [7/15], Iteration [19000/20000], Training Loss: 0.3881\n",
      "Epoch [7/15], Iteration [20000/20000], Training Loss: 0.5584\n",
      "Epoch [7/15], Average Training Loss: 0.5032\n",
      "Epoch [8/15], Iteration [1000/20000], Training Loss: 0.4591\n",
      "Epoch [8/15], Iteration [2000/20000], Training Loss: 0.5289\n",
      "Epoch [8/15], Iteration [3000/20000], Training Loss: 0.5750\n",
      "Epoch [8/15], Iteration [4000/20000], Training Loss: 0.5153\n",
      "Epoch [8/15], Iteration [5000/20000], Training Loss: 0.5259\n",
      "Epoch [8/15], Iteration [6000/20000], Training Loss: 0.4746\n",
      "Epoch [8/15], Iteration [7000/20000], Training Loss: 0.4770\n",
      "Epoch [8/15], Iteration [8000/20000], Training Loss: 0.4859\n",
      "Epoch [8/15], Iteration [9000/20000], Training Loss: 0.4162\n",
      "Epoch [8/15], Iteration [10000/20000], Training Loss: 0.4832\n",
      "Epoch [8/15], Iteration [11000/20000], Training Loss: 0.5005\n",
      "Epoch [8/15], Iteration [12000/20000], Training Loss: 0.4172\n",
      "Epoch [8/15], Iteration [13000/20000], Training Loss: 0.5746\n",
      "Epoch [8/15], Iteration [14000/20000], Training Loss: 0.4117\n",
      "Epoch [8/15], Iteration [15000/20000], Training Loss: 0.4731\n",
      "Epoch [8/15], Iteration [16000/20000], Training Loss: 0.4547\n",
      "Epoch [8/15], Iteration [17000/20000], Training Loss: 0.4800\n",
      "Epoch [8/15], Iteration [18000/20000], Training Loss: 0.4256\n",
      "Epoch [8/15], Iteration [19000/20000], Training Loss: 0.5261\n",
      "Epoch [8/15], Iteration [20000/20000], Training Loss: 0.5163\n",
      "Epoch [8/15], Average Training Loss: 0.5021\n",
      "Epoch [9/15], Iteration [1000/20000], Training Loss: 0.4681\n",
      "Epoch [9/15], Iteration [2000/20000], Training Loss: 0.5534\n",
      "Epoch [9/15], Iteration [3000/20000], Training Loss: 0.5289\n",
      "Epoch [9/15], Iteration [4000/20000], Training Loss: 0.4716\n",
      "Epoch [9/15], Iteration [5000/20000], Training Loss: 0.4856\n",
      "Epoch [9/15], Iteration [6000/20000], Training Loss: 0.5074\n",
      "Epoch [9/15], Iteration [7000/20000], Training Loss: 0.5051\n",
      "Epoch [9/15], Iteration [8000/20000], Training Loss: 0.4578\n",
      "Epoch [9/15], Iteration [9000/20000], Training Loss: 0.4801\n",
      "Epoch [9/15], Iteration [10000/20000], Training Loss: 0.5220\n",
      "Epoch [9/15], Iteration [11000/20000], Training Loss: 0.5296\n",
      "Epoch [9/15], Iteration [12000/20000], Training Loss: 0.4759\n",
      "Epoch [9/15], Iteration [13000/20000], Training Loss: 0.5184\n",
      "Epoch [9/15], Iteration [14000/20000], Training Loss: 0.5277\n",
      "Epoch [9/15], Iteration [15000/20000], Training Loss: 0.4009\n",
      "Epoch [9/15], Iteration [16000/20000], Training Loss: 0.5051\n",
      "Epoch [9/15], Iteration [17000/20000], Training Loss: 0.5423\n",
      "Epoch [9/15], Iteration [18000/20000], Training Loss: 0.5346\n",
      "Epoch [9/15], Iteration [19000/20000], Training Loss: 0.4764\n",
      "Epoch [9/15], Iteration [20000/20000], Training Loss: 0.5825\n",
      "Epoch [9/15], Average Training Loss: 0.5019\n",
      "Epoch [10/15], Iteration [1000/20000], Training Loss: 0.5182\n",
      "Epoch [10/15], Iteration [2000/20000], Training Loss: 0.5155\n",
      "Epoch [10/15], Iteration [3000/20000], Training Loss: 0.4783\n",
      "Epoch [10/15], Iteration [4000/20000], Training Loss: 0.5775\n",
      "Epoch [10/15], Iteration [5000/20000], Training Loss: 0.4749\n",
      "Epoch [10/15], Iteration [6000/20000], Training Loss: 0.4707\n",
      "Epoch [10/15], Iteration [7000/20000], Training Loss: 0.5006\n",
      "Epoch [10/15], Iteration [8000/20000], Training Loss: 0.5487\n",
      "Epoch [10/15], Iteration [9000/20000], Training Loss: 0.4072\n",
      "Epoch [10/15], Iteration [10000/20000], Training Loss: 0.3634\n",
      "Epoch [10/15], Iteration [11000/20000], Training Loss: 0.4869\n",
      "Epoch [10/15], Iteration [12000/20000], Training Loss: 0.4348\n",
      "Epoch [10/15], Iteration [13000/20000], Training Loss: 0.5525\n",
      "Epoch [10/15], Iteration [14000/20000], Training Loss: 0.5751\n",
      "Epoch [10/15], Iteration [15000/20000], Training Loss: 0.5187\n",
      "Epoch [10/15], Iteration [16000/20000], Training Loss: 0.4564\n",
      "Epoch [10/15], Iteration [17000/20000], Training Loss: 0.4269\n",
      "Epoch [10/15], Iteration [18000/20000], Training Loss: 0.4397\n",
      "Epoch [10/15], Iteration [19000/20000], Training Loss: 0.3919\n",
      "Epoch [10/15], Iteration [20000/20000], Training Loss: 0.4787\n",
      "Epoch [10/15], Average Training Loss: 0.5018\n",
      "Epoch [11/15], Iteration [1000/20000], Training Loss: 0.5013\n",
      "Epoch [11/15], Iteration [2000/20000], Training Loss: 0.4533\n",
      "Epoch [11/15], Iteration [3000/20000], Training Loss: 0.6271\n",
      "Epoch [11/15], Iteration [4000/20000], Training Loss: 0.4748\n",
      "Epoch [11/15], Iteration [5000/20000], Training Loss: 0.4775\n",
      "Epoch [11/15], Iteration [6000/20000], Training Loss: 0.5360\n",
      "Epoch [11/15], Iteration [7000/20000], Training Loss: 0.4909\n",
      "Epoch [11/15], Iteration [8000/20000], Training Loss: 0.5012\n",
      "Epoch [11/15], Iteration [9000/20000], Training Loss: 0.5473\n",
      "Epoch [11/15], Iteration [10000/20000], Training Loss: 0.5030\n",
      "Epoch [11/15], Iteration [11000/20000], Training Loss: 0.5223\n",
      "Epoch [11/15], Iteration [12000/20000], Training Loss: 0.4458\n",
      "Epoch [11/15], Iteration [13000/20000], Training Loss: 0.5937\n",
      "Epoch [11/15], Iteration [14000/20000], Training Loss: 0.4204\n",
      "Epoch [11/15], Iteration [15000/20000], Training Loss: 0.4640\n",
      "Epoch [11/15], Iteration [16000/20000], Training Loss: 0.4081\n",
      "Epoch [11/15], Iteration [17000/20000], Training Loss: 0.4887\n",
      "Epoch [11/15], Iteration [18000/20000], Training Loss: 0.5455\n",
      "Epoch [11/15], Iteration [19000/20000], Training Loss: 0.4229\n",
      "Epoch [11/15], Iteration [20000/20000], Training Loss: 0.5626\n",
      "Epoch [11/15], Average Training Loss: 0.5019\n",
      "Epoch [12/15], Iteration [1000/20000], Training Loss: 0.5017\n",
      "Epoch [12/15], Iteration [2000/20000], Training Loss: 0.5083\n",
      "Epoch [12/15], Iteration [3000/20000], Training Loss: 0.6056\n",
      "Epoch [12/15], Iteration [4000/20000], Training Loss: 0.5142\n",
      "Epoch [12/15], Iteration [5000/20000], Training Loss: 0.5313\n",
      "Epoch [12/15], Iteration [6000/20000], Training Loss: 0.4409\n",
      "Epoch [12/15], Iteration [7000/20000], Training Loss: 0.3825\n",
      "Epoch [12/15], Iteration [8000/20000], Training Loss: 0.4356\n",
      "Epoch [12/15], Iteration [9000/20000], Training Loss: 0.3862\n",
      "Epoch [12/15], Iteration [10000/20000], Training Loss: 0.4785\n",
      "Epoch [12/15], Iteration [11000/20000], Training Loss: 0.4546\n",
      "Epoch [12/15], Iteration [12000/20000], Training Loss: 0.5971\n",
      "Epoch [12/15], Iteration [13000/20000], Training Loss: 0.5478\n",
      "Epoch [12/15], Iteration [14000/20000], Training Loss: 0.5669\n",
      "Epoch [12/15], Iteration [15000/20000], Training Loss: 0.4567\n",
      "Epoch [12/15], Iteration [16000/20000], Training Loss: 0.5121\n",
      "Epoch [12/15], Iteration [17000/20000], Training Loss: 0.5937\n",
      "Epoch [12/15], Iteration [18000/20000], Training Loss: 0.4448\n",
      "Epoch [12/15], Iteration [19000/20000], Training Loss: 0.4721\n",
      "Epoch [12/15], Iteration [20000/20000], Training Loss: 0.5871\n",
      "Epoch [12/15], Average Training Loss: 0.5019\n",
      "Epoch [13/15], Iteration [1000/20000], Training Loss: 0.4454\n",
      "Epoch [13/15], Iteration [2000/20000], Training Loss: 0.6027\n",
      "Epoch [13/15], Iteration [3000/20000], Training Loss: 0.4637\n",
      "Epoch [13/15], Iteration [4000/20000], Training Loss: 0.4294\n",
      "Epoch [13/15], Iteration [5000/20000], Training Loss: 0.5268\n",
      "Epoch [13/15], Iteration [6000/20000], Training Loss: 0.4392\n",
      "Epoch [13/15], Iteration [7000/20000], Training Loss: 0.4329\n",
      "Epoch [13/15], Iteration [8000/20000], Training Loss: 0.5097\n",
      "Epoch [13/15], Iteration [9000/20000], Training Loss: 0.4568\n",
      "Epoch [13/15], Iteration [10000/20000], Training Loss: 0.5108\n",
      "Epoch [13/15], Iteration [11000/20000], Training Loss: 0.4495\n",
      "Epoch [13/15], Iteration [12000/20000], Training Loss: 0.4434\n",
      "Epoch [13/15], Iteration [13000/20000], Training Loss: 0.4953\n",
      "Epoch [13/15], Iteration [14000/20000], Training Loss: 0.5315\n",
      "Epoch [13/15], Iteration [15000/20000], Training Loss: 0.5226\n",
      "Epoch [13/15], Iteration [16000/20000], Training Loss: 0.4931\n",
      "Epoch [13/15], Iteration [17000/20000], Training Loss: 0.3984\n",
      "Epoch [13/15], Iteration [18000/20000], Training Loss: 0.5954\n",
      "Epoch [13/15], Iteration [19000/20000], Training Loss: 0.4510\n",
      "Epoch [13/15], Iteration [20000/20000], Training Loss: 0.4972\n",
      "Epoch [13/15], Average Training Loss: 0.5016\n",
      "Epoch [14/15], Iteration [1000/20000], Training Loss: 0.6092\n",
      "Epoch [14/15], Iteration [2000/20000], Training Loss: 0.4467\n",
      "Epoch [14/15], Iteration [3000/20000], Training Loss: 0.4221\n",
      "Epoch [14/15], Iteration [4000/20000], Training Loss: 0.5368\n",
      "Epoch [14/15], Iteration [5000/20000], Training Loss: 0.5041\n",
      "Epoch [14/15], Iteration [6000/20000], Training Loss: 0.5066\n",
      "Epoch [14/15], Iteration [7000/20000], Training Loss: 0.5384\n",
      "Epoch [14/15], Iteration [8000/20000], Training Loss: 0.5743\n",
      "Epoch [14/15], Iteration [9000/20000], Training Loss: 0.4925\n",
      "Epoch [14/15], Iteration [10000/20000], Training Loss: 0.3546\n",
      "Epoch [14/15], Iteration [11000/20000], Training Loss: 0.5169\n",
      "Epoch [14/15], Iteration [12000/20000], Training Loss: 0.6790\n",
      "Epoch [14/15], Iteration [13000/20000], Training Loss: 0.6202\n",
      "Epoch [14/15], Iteration [14000/20000], Training Loss: 0.4904\n",
      "Epoch [14/15], Iteration [15000/20000], Training Loss: 0.4933\n",
      "Epoch [14/15], Iteration [16000/20000], Training Loss: 0.4445\n",
      "Epoch [14/15], Iteration [17000/20000], Training Loss: 0.5959\n",
      "Epoch [14/15], Iteration [18000/20000], Training Loss: 0.5786\n",
      "Epoch [14/15], Iteration [19000/20000], Training Loss: 0.7077\n",
      "Epoch [14/15], Iteration [20000/20000], Training Loss: 0.4059\n",
      "Epoch [14/15], Average Training Loss: 0.5017\n",
      "Epoch [15/15], Iteration [1000/20000], Training Loss: 0.5045\n",
      "Epoch [15/15], Iteration [2000/20000], Training Loss: 0.4823\n",
      "Epoch [15/15], Iteration [3000/20000], Training Loss: 0.4898\n",
      "Epoch [15/15], Iteration [4000/20000], Training Loss: 0.4999\n",
      "Epoch [15/15], Iteration [5000/20000], Training Loss: 0.5816\n",
      "Epoch [15/15], Iteration [6000/20000], Training Loss: 0.5046\n",
      "Epoch [15/15], Iteration [7000/20000], Training Loss: 0.4856\n",
      "Epoch [15/15], Iteration [8000/20000], Training Loss: 0.5373\n",
      "Epoch [15/15], Iteration [9000/20000], Training Loss: 0.5536\n",
      "Epoch [15/15], Iteration [10000/20000], Training Loss: 0.4669\n",
      "Epoch [15/15], Iteration [11000/20000], Training Loss: 0.5146\n",
      "Epoch [15/15], Iteration [12000/20000], Training Loss: 0.4597\n",
      "Epoch [15/15], Iteration [13000/20000], Training Loss: 0.4888\n",
      "Epoch [15/15], Iteration [14000/20000], Training Loss: 0.5190\n",
      "Epoch [15/15], Iteration [15000/20000], Training Loss: 0.4424\n",
      "Epoch [15/15], Iteration [16000/20000], Training Loss: 0.4507\n",
      "Epoch [15/15], Iteration [17000/20000], Training Loss: 0.5023\n",
      "Epoch [15/15], Iteration [18000/20000], Training Loss: 0.5189\n",
      "Epoch [15/15], Iteration [19000/20000], Training Loss: 0.5766\n",
      "Epoch [15/15], Iteration [20000/20000], Training Loss: 0.5698\n",
      "Epoch [15/15], Average Training Loss: 0.5015\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define your parameters\n",
    "input_size = 300  # Dimension for word embeddings\n",
    "hidden_size = 8\n",
    "output_size = 2  # Binary classification\n",
    "dropout_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "bidirectional = True  # Use bidirectional LSTM\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, bidirectional, dropout_prob=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)  # Multiply by 2 if bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(x)  # No embedding layer, directly use the input\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)  # Take the last hidden state\n",
    "        return output\n",
    "\n",
    "# Convert document_representations into PyTorch tensor\n",
    "document_tensor = torch.stack(document_representations)\n",
    "\n",
    "# Convert labels into PyTorch tensor\n",
    "labels_tensor = torch.tensor(y_train)\n",
    "\n",
    "# Create DataLoader object for training\n",
    "train_dataset = TensorDataset(document_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate your BiLSTM classifier model\n",
    "model = BiLSTMClassifier(input_size, hidden_size, output_size, num_layers, bidirectional, dropout_prob)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Iteration [{i}/{len(train_loader)}], Training Loss: {loss.item():.4f}')\n",
    "    \n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T15:14:29.037658Z",
     "iopub.status.busy": "2024-04-27T15:14:29.037314Z",
     "iopub.status.idle": "2024-04-27T15:14:29.047336Z",
     "shell.execute_reply": "2024-04-27T15:14:29.045514Z",
     "shell.execute_reply.started": "2024-04-27T15:14:29.037629Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'bi_lstm_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T15:14:54.521196Z",
     "iopub.status.busy": "2024-04-27T15:14:54.520639Z",
     "iopub.status.idle": "2024-04-27T15:15:01.358958Z",
     "shell.execute_reply": "2024-04-27T15:15:01.357570Z",
     "shell.execute_reply.started": "2024-04-27T15:14:54.521164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data for bilstm: 77.88%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Step 5: Calculate the accuracy of the model on the test dataset\n",
    "print(f'Accuracy on test data for bilstm: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T15:15:59.784139Z",
     "iopub.status.busy": "2024-04-27T15:15:59.783302Z",
     "iopub.status.idle": "2024-04-27T15:42:06.798063Z",
     "shell.execute_reply": "2024-04-27T15:42:06.796314Z",
     "shell.execute_reply.started": "2024-04-27T15:15:59.784097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Iteration [1000/20000], Training Loss: 0.4973\n",
      "Epoch [1/15], Iteration [2000/20000], Training Loss: 0.5126\n",
      "Epoch [1/15], Iteration [3000/20000], Training Loss: 0.5954\n",
      "Epoch [1/15], Iteration [4000/20000], Training Loss: 0.4584\n",
      "Epoch [1/15], Iteration [5000/20000], Training Loss: 0.5859\n",
      "Epoch [1/15], Iteration [6000/20000], Training Loss: 0.4023\n",
      "Epoch [1/15], Iteration [7000/20000], Training Loss: 0.4319\n",
      "Epoch [1/15], Iteration [8000/20000], Training Loss: 0.5530\n",
      "Epoch [1/15], Iteration [9000/20000], Training Loss: 0.5080\n",
      "Epoch [1/15], Iteration [10000/20000], Training Loss: 0.5964\n",
      "Epoch [1/15], Iteration [11000/20000], Training Loss: 0.4581\n",
      "Epoch [1/15], Iteration [12000/20000], Training Loss: 0.5891\n",
      "Epoch [1/15], Iteration [13000/20000], Training Loss: 0.5233\n",
      "Epoch [1/15], Iteration [14000/20000], Training Loss: 0.5418\n",
      "Epoch [1/15], Iteration [15000/20000], Training Loss: 0.4233\n",
      "Epoch [1/15], Iteration [16000/20000], Training Loss: 0.4837\n",
      "Epoch [1/15], Iteration [17000/20000], Training Loss: 0.5360\n",
      "Epoch [1/15], Iteration [18000/20000], Training Loss: 0.4386\n",
      "Epoch [1/15], Iteration [19000/20000], Training Loss: 0.4279\n",
      "Epoch [1/15], Iteration [20000/20000], Training Loss: 0.5578\n",
      "Epoch [1/15], Average Training Loss: 0.5257\n",
      "Epoch [2/15], Iteration [1000/20000], Training Loss: 0.6396\n",
      "Epoch [2/15], Iteration [2000/20000], Training Loss: 0.5245\n",
      "Epoch [2/15], Iteration [3000/20000], Training Loss: 0.5984\n",
      "Epoch [2/15], Iteration [4000/20000], Training Loss: 0.4636\n",
      "Epoch [2/15], Iteration [5000/20000], Training Loss: 0.4529\n",
      "Epoch [2/15], Iteration [6000/20000], Training Loss: 0.4992\n",
      "Epoch [2/15], Iteration [7000/20000], Training Loss: 0.5756\n",
      "Epoch [2/15], Iteration [8000/20000], Training Loss: 0.5608\n",
      "Epoch [2/15], Iteration [9000/20000], Training Loss: 0.5541\n",
      "Epoch [2/15], Iteration [10000/20000], Training Loss: 0.4785\n",
      "Epoch [2/15], Iteration [11000/20000], Training Loss: 0.4501\n",
      "Epoch [2/15], Iteration [12000/20000], Training Loss: 0.5254\n",
      "Epoch [2/15], Iteration [13000/20000], Training Loss: 0.4496\n",
      "Epoch [2/15], Iteration [14000/20000], Training Loss: 0.6213\n",
      "Epoch [2/15], Iteration [15000/20000], Training Loss: 0.5502\n",
      "Epoch [2/15], Iteration [16000/20000], Training Loss: 0.4784\n",
      "Epoch [2/15], Iteration [17000/20000], Training Loss: 0.3908\n",
      "Epoch [2/15], Iteration [18000/20000], Training Loss: 0.5284\n",
      "Epoch [2/15], Iteration [19000/20000], Training Loss: 0.4241\n",
      "Epoch [2/15], Iteration [20000/20000], Training Loss: 0.4864\n",
      "Epoch [2/15], Average Training Loss: 0.5107\n",
      "Epoch [3/15], Iteration [1000/20000], Training Loss: 0.4471\n",
      "Epoch [3/15], Iteration [2000/20000], Training Loss: 0.4734\n",
      "Epoch [3/15], Iteration [3000/20000], Training Loss: 0.4835\n",
      "Epoch [3/15], Iteration [4000/20000], Training Loss: 0.5035\n",
      "Epoch [3/15], Iteration [5000/20000], Training Loss: 0.4961\n",
      "Epoch [3/15], Iteration [6000/20000], Training Loss: 0.5022\n",
      "Epoch [3/15], Iteration [7000/20000], Training Loss: 0.5296\n",
      "Epoch [3/15], Iteration [8000/20000], Training Loss: 0.5365\n",
      "Epoch [3/15], Iteration [9000/20000], Training Loss: 0.4537\n",
      "Epoch [3/15], Iteration [10000/20000], Training Loss: 0.5561\n",
      "Epoch [3/15], Iteration [11000/20000], Training Loss: 0.3976\n",
      "Epoch [3/15], Iteration [12000/20000], Training Loss: 0.4308\n",
      "Epoch [3/15], Iteration [13000/20000], Training Loss: 0.5337\n",
      "Epoch [3/15], Iteration [14000/20000], Training Loss: 0.4818\n",
      "Epoch [3/15], Iteration [15000/20000], Training Loss: 0.4071\n",
      "Epoch [3/15], Iteration [16000/20000], Training Loss: 0.4310\n",
      "Epoch [3/15], Iteration [17000/20000], Training Loss: 0.4851\n",
      "Epoch [3/15], Iteration [18000/20000], Training Loss: 0.5516\n",
      "Epoch [3/15], Iteration [19000/20000], Training Loss: 0.4727\n",
      "Epoch [3/15], Iteration [20000/20000], Training Loss: 0.5042\n",
      "Epoch [3/15], Average Training Loss: 0.5069\n",
      "Epoch [4/15], Iteration [1000/20000], Training Loss: 0.5826\n",
      "Epoch [4/15], Iteration [2000/20000], Training Loss: 0.5728\n",
      "Epoch [4/15], Iteration [3000/20000], Training Loss: 0.5357\n",
      "Epoch [4/15], Iteration [4000/20000], Training Loss: 0.6604\n",
      "Epoch [4/15], Iteration [5000/20000], Training Loss: 0.6224\n",
      "Epoch [4/15], Iteration [6000/20000], Training Loss: 0.4989\n",
      "Epoch [4/15], Iteration [7000/20000], Training Loss: 0.4734\n",
      "Epoch [4/15], Iteration [8000/20000], Training Loss: 0.5145\n",
      "Epoch [4/15], Iteration [9000/20000], Training Loss: 0.4387\n",
      "Epoch [4/15], Iteration [10000/20000], Training Loss: 0.4865\n",
      "Epoch [4/15], Iteration [11000/20000], Training Loss: 0.5034\n",
      "Epoch [4/15], Iteration [12000/20000], Training Loss: 0.4633\n",
      "Epoch [4/15], Iteration [13000/20000], Training Loss: 0.5653\n",
      "Epoch [4/15], Iteration [14000/20000], Training Loss: 0.4495\n",
      "Epoch [4/15], Iteration [15000/20000], Training Loss: 0.5620\n",
      "Epoch [4/15], Iteration [16000/20000], Training Loss: 0.4623\n",
      "Epoch [4/15], Iteration [17000/20000], Training Loss: 0.5493\n",
      "Epoch [4/15], Iteration [18000/20000], Training Loss: 0.3970\n",
      "Epoch [4/15], Iteration [19000/20000], Training Loss: 0.5116\n",
      "Epoch [4/15], Iteration [20000/20000], Training Loss: 0.4687\n",
      "Epoch [4/15], Average Training Loss: 0.5050\n",
      "Epoch [5/15], Iteration [1000/20000], Training Loss: 0.5285\n",
      "Epoch [5/15], Iteration [2000/20000], Training Loss: 0.5432\n",
      "Epoch [5/15], Iteration [3000/20000], Training Loss: 0.4347\n",
      "Epoch [5/15], Iteration [4000/20000], Training Loss: 0.4592\n",
      "Epoch [5/15], Iteration [5000/20000], Training Loss: 0.4770\n",
      "Epoch [5/15], Iteration [6000/20000], Training Loss: 0.4844\n",
      "Epoch [5/15], Iteration [7000/20000], Training Loss: 0.4905\n",
      "Epoch [5/15], Iteration [8000/20000], Training Loss: 0.3584\n",
      "Epoch [5/15], Iteration [9000/20000], Training Loss: 0.3610\n",
      "Epoch [5/15], Iteration [10000/20000], Training Loss: 0.4540\n",
      "Epoch [5/15], Iteration [11000/20000], Training Loss: 0.4789\n",
      "Epoch [5/15], Iteration [12000/20000], Training Loss: 0.5482\n",
      "Epoch [5/15], Iteration [13000/20000], Training Loss: 0.4121\n",
      "Epoch [5/15], Iteration [14000/20000], Training Loss: 0.5514\n",
      "Epoch [5/15], Iteration [15000/20000], Training Loss: 0.4901\n",
      "Epoch [5/15], Iteration [16000/20000], Training Loss: 0.4870\n",
      "Epoch [5/15], Iteration [17000/20000], Training Loss: 0.5309\n",
      "Epoch [5/15], Iteration [18000/20000], Training Loss: 0.4131\n",
      "Epoch [5/15], Iteration [19000/20000], Training Loss: 0.4469\n",
      "Epoch [5/15], Iteration [20000/20000], Training Loss: 0.3947\n",
      "Epoch [5/15], Average Training Loss: 0.5039\n",
      "Epoch [6/15], Iteration [1000/20000], Training Loss: 0.5231\n",
      "Epoch [6/15], Iteration [2000/20000], Training Loss: 0.5043\n",
      "Epoch [6/15], Iteration [3000/20000], Training Loss: 0.6837\n",
      "Epoch [6/15], Iteration [4000/20000], Training Loss: 0.3503\n",
      "Epoch [6/15], Iteration [5000/20000], Training Loss: 0.5514\n",
      "Epoch [6/15], Iteration [6000/20000], Training Loss: 0.6016\n",
      "Epoch [6/15], Iteration [7000/20000], Training Loss: 0.3976\n",
      "Epoch [6/15], Iteration [8000/20000], Training Loss: 0.4658\n",
      "Epoch [6/15], Iteration [9000/20000], Training Loss: 0.4995\n",
      "Epoch [6/15], Iteration [10000/20000], Training Loss: 0.5451\n",
      "Epoch [6/15], Iteration [11000/20000], Training Loss: 0.5900\n",
      "Epoch [6/15], Iteration [12000/20000], Training Loss: 0.4029\n",
      "Epoch [6/15], Iteration [13000/20000], Training Loss: 0.4776\n",
      "Epoch [6/15], Iteration [14000/20000], Training Loss: 0.4652\n",
      "Epoch [6/15], Iteration [15000/20000], Training Loss: 0.5741\n",
      "Epoch [6/15], Iteration [16000/20000], Training Loss: 0.4994\n",
      "Epoch [6/15], Iteration [17000/20000], Training Loss: 0.6568\n",
      "Epoch [6/15], Iteration [18000/20000], Training Loss: 0.6341\n",
      "Epoch [6/15], Iteration [19000/20000], Training Loss: 0.5357\n",
      "Epoch [6/15], Iteration [20000/20000], Training Loss: 0.4607\n",
      "Epoch [6/15], Average Training Loss: 0.5031\n",
      "Epoch [7/15], Iteration [1000/20000], Training Loss: 0.6127\n",
      "Epoch [7/15], Iteration [2000/20000], Training Loss: 0.5801\n",
      "Epoch [7/15], Iteration [3000/20000], Training Loss: 0.5006\n",
      "Epoch [7/15], Iteration [4000/20000], Training Loss: 0.4142\n",
      "Epoch [7/15], Iteration [5000/20000], Training Loss: 0.6129\n",
      "Epoch [7/15], Iteration [6000/20000], Training Loss: 0.6732\n",
      "Epoch [7/15], Iteration [7000/20000], Training Loss: 0.6032\n",
      "Epoch [7/15], Iteration [8000/20000], Training Loss: 0.5658\n",
      "Epoch [7/15], Iteration [9000/20000], Training Loss: 0.6012\n",
      "Epoch [7/15], Iteration [10000/20000], Training Loss: 0.5033\n",
      "Epoch [7/15], Iteration [11000/20000], Training Loss: 0.4907\n",
      "Epoch [7/15], Iteration [12000/20000], Training Loss: 0.4356\n",
      "Epoch [7/15], Iteration [13000/20000], Training Loss: 0.5660\n",
      "Epoch [7/15], Iteration [14000/20000], Training Loss: 0.4513\n",
      "Epoch [7/15], Iteration [15000/20000], Training Loss: 0.5431\n",
      "Epoch [7/15], Iteration [16000/20000], Training Loss: 0.5373\n",
      "Epoch [7/15], Iteration [17000/20000], Training Loss: 0.5229\n",
      "Epoch [7/15], Iteration [18000/20000], Training Loss: 0.4790\n",
      "Epoch [7/15], Iteration [19000/20000], Training Loss: 0.4772\n",
      "Epoch [7/15], Iteration [20000/20000], Training Loss: 0.3872\n",
      "Epoch [7/15], Average Training Loss: 0.5028\n",
      "Epoch [8/15], Iteration [1000/20000], Training Loss: 0.4729\n",
      "Epoch [8/15], Iteration [2000/20000], Training Loss: 0.6494\n",
      "Epoch [8/15], Iteration [3000/20000], Training Loss: 0.5646\n",
      "Epoch [8/15], Iteration [4000/20000], Training Loss: 0.5409\n",
      "Epoch [8/15], Iteration [5000/20000], Training Loss: 0.4519\n",
      "Epoch [8/15], Iteration [6000/20000], Training Loss: 0.4779\n",
      "Epoch [8/15], Iteration [7000/20000], Training Loss: 0.5865\n",
      "Epoch [8/15], Iteration [8000/20000], Training Loss: 0.5601\n",
      "Epoch [8/15], Iteration [9000/20000], Training Loss: 0.5174\n",
      "Epoch [8/15], Iteration [10000/20000], Training Loss: 0.5128\n",
      "Epoch [8/15], Iteration [11000/20000], Training Loss: 0.4733\n",
      "Epoch [8/15], Iteration [12000/20000], Training Loss: 0.4627\n",
      "Epoch [8/15], Iteration [13000/20000], Training Loss: 0.4105\n",
      "Epoch [8/15], Iteration [14000/20000], Training Loss: 0.4834\n",
      "Epoch [8/15], Iteration [15000/20000], Training Loss: 0.5301\n",
      "Epoch [8/15], Iteration [16000/20000], Training Loss: 0.4484\n",
      "Epoch [8/15], Iteration [17000/20000], Training Loss: 0.4489\n",
      "Epoch [8/15], Iteration [18000/20000], Training Loss: 0.5366\n",
      "Epoch [8/15], Iteration [19000/20000], Training Loss: 0.6145\n",
      "Epoch [8/15], Iteration [20000/20000], Training Loss: 0.5087\n",
      "Epoch [8/15], Average Training Loss: 0.5023\n",
      "Epoch [9/15], Iteration [1000/20000], Training Loss: 0.4065\n",
      "Epoch [9/15], Iteration [2000/20000], Training Loss: 0.5650\n",
      "Epoch [9/15], Iteration [3000/20000], Training Loss: 0.4645\n",
      "Epoch [9/15], Iteration [4000/20000], Training Loss: 0.4821\n",
      "Epoch [9/15], Iteration [5000/20000], Training Loss: 0.3915\n",
      "Epoch [9/15], Iteration [6000/20000], Training Loss: 0.5006\n",
      "Epoch [9/15], Iteration [7000/20000], Training Loss: 0.4589\n",
      "Epoch [9/15], Iteration [8000/20000], Training Loss: 0.5725\n",
      "Epoch [9/15], Iteration [9000/20000], Training Loss: 0.5070\n",
      "Epoch [9/15], Iteration [10000/20000], Training Loss: 0.4823\n",
      "Epoch [9/15], Iteration [11000/20000], Training Loss: 0.4691\n",
      "Epoch [9/15], Iteration [12000/20000], Training Loss: 0.5764\n",
      "Epoch [9/15], Iteration [13000/20000], Training Loss: 0.6197\n",
      "Epoch [9/15], Iteration [14000/20000], Training Loss: 0.5860\n",
      "Epoch [9/15], Iteration [15000/20000], Training Loss: 0.5822\n",
      "Epoch [9/15], Iteration [16000/20000], Training Loss: 0.4694\n",
      "Epoch [9/15], Iteration [17000/20000], Training Loss: 0.4593\n",
      "Epoch [9/15], Iteration [18000/20000], Training Loss: 0.5352\n",
      "Epoch [9/15], Iteration [19000/20000], Training Loss: 0.5209\n",
      "Epoch [9/15], Iteration [20000/20000], Training Loss: 0.4744\n",
      "Epoch [9/15], Average Training Loss: 0.5019\n",
      "Epoch [10/15], Iteration [1000/20000], Training Loss: 0.4945\n",
      "Epoch [10/15], Iteration [2000/20000], Training Loss: 0.6316\n",
      "Epoch [10/15], Iteration [3000/20000], Training Loss: 0.4932\n",
      "Epoch [10/15], Iteration [4000/20000], Training Loss: 0.4317\n",
      "Epoch [10/15], Iteration [5000/20000], Training Loss: 0.3447\n",
      "Epoch [10/15], Iteration [6000/20000], Training Loss: 0.3963\n",
      "Epoch [10/15], Iteration [7000/20000], Training Loss: 0.3981\n",
      "Epoch [10/15], Iteration [8000/20000], Training Loss: 0.5556\n",
      "Epoch [10/15], Iteration [9000/20000], Training Loss: 0.4125\n",
      "Epoch [10/15], Iteration [10000/20000], Training Loss: 0.4839\n",
      "Epoch [10/15], Iteration [11000/20000], Training Loss: 0.6212\n",
      "Epoch [10/15], Iteration [12000/20000], Training Loss: 0.4959\n",
      "Epoch [10/15], Iteration [13000/20000], Training Loss: 0.5084\n",
      "Epoch [10/15], Iteration [14000/20000], Training Loss: 0.5197\n",
      "Epoch [10/15], Iteration [15000/20000], Training Loss: 0.5196\n",
      "Epoch [10/15], Iteration [16000/20000], Training Loss: 0.3655\n",
      "Epoch [10/15], Iteration [17000/20000], Training Loss: 0.5243\n",
      "Epoch [10/15], Iteration [18000/20000], Training Loss: 0.4208\n",
      "Epoch [10/15], Iteration [19000/20000], Training Loss: 0.4294\n",
      "Epoch [10/15], Iteration [20000/20000], Training Loss: 0.5193\n",
      "Epoch [10/15], Average Training Loss: 0.5022\n",
      "Epoch [11/15], Iteration [1000/20000], Training Loss: 0.4848\n",
      "Epoch [11/15], Iteration [2000/20000], Training Loss: 0.5251\n",
      "Epoch [11/15], Iteration [3000/20000], Training Loss: 0.5881\n",
      "Epoch [11/15], Iteration [4000/20000], Training Loss: 0.3922\n",
      "Epoch [11/15], Iteration [5000/20000], Training Loss: 0.5009\n",
      "Epoch [11/15], Iteration [6000/20000], Training Loss: 0.5677\n",
      "Epoch [11/15], Iteration [7000/20000], Training Loss: 0.5140\n",
      "Epoch [11/15], Iteration [8000/20000], Training Loss: 0.5571\n",
      "Epoch [11/15], Iteration [9000/20000], Training Loss: 0.4629\n",
      "Epoch [11/15], Iteration [10000/20000], Training Loss: 0.4047\n",
      "Epoch [11/15], Iteration [11000/20000], Training Loss: 0.5477\n",
      "Epoch [11/15], Iteration [12000/20000], Training Loss: 0.5446\n",
      "Epoch [11/15], Iteration [13000/20000], Training Loss: 0.4587\n",
      "Epoch [11/15], Iteration [14000/20000], Training Loss: 0.5515\n",
      "Epoch [11/15], Iteration [15000/20000], Training Loss: 0.6524\n",
      "Epoch [11/15], Iteration [16000/20000], Training Loss: 0.4695\n",
      "Epoch [11/15], Iteration [17000/20000], Training Loss: 0.5284\n",
      "Epoch [11/15], Iteration [18000/20000], Training Loss: 0.4835\n",
      "Epoch [11/15], Iteration [19000/20000], Training Loss: 0.6104\n",
      "Epoch [11/15], Iteration [20000/20000], Training Loss: 0.4210\n",
      "Epoch [11/15], Average Training Loss: 0.5016\n",
      "Epoch [12/15], Iteration [1000/20000], Training Loss: 0.4849\n",
      "Epoch [12/15], Iteration [2000/20000], Training Loss: 0.3701\n",
      "Epoch [12/15], Iteration [3000/20000], Training Loss: 0.5359\n",
      "Epoch [12/15], Iteration [4000/20000], Training Loss: 0.5391\n",
      "Epoch [12/15], Iteration [5000/20000], Training Loss: 0.5103\n",
      "Epoch [12/15], Iteration [6000/20000], Training Loss: 0.4987\n",
      "Epoch [12/15], Iteration [7000/20000], Training Loss: 0.4529\n",
      "Epoch [12/15], Iteration [8000/20000], Training Loss: 0.4737\n",
      "Epoch [12/15], Iteration [9000/20000], Training Loss: 0.4897\n",
      "Epoch [12/15], Iteration [10000/20000], Training Loss: 0.5411\n",
      "Epoch [12/15], Iteration [11000/20000], Training Loss: 0.4367\n",
      "Epoch [12/15], Iteration [12000/20000], Training Loss: 0.6123\n",
      "Epoch [12/15], Iteration [13000/20000], Training Loss: 0.5020\n",
      "Epoch [12/15], Iteration [14000/20000], Training Loss: 0.5633\n",
      "Epoch [12/15], Iteration [15000/20000], Training Loss: 0.5228\n",
      "Epoch [12/15], Iteration [16000/20000], Training Loss: 0.4826\n",
      "Epoch [12/15], Iteration [17000/20000], Training Loss: 0.5646\n",
      "Epoch [12/15], Iteration [18000/20000], Training Loss: 0.6270\n",
      "Epoch [12/15], Iteration [19000/20000], Training Loss: 0.6137\n",
      "Epoch [12/15], Iteration [20000/20000], Training Loss: 0.4553\n",
      "Epoch [12/15], Average Training Loss: 0.5015\n",
      "Epoch [13/15], Iteration [1000/20000], Training Loss: 0.5408\n",
      "Epoch [13/15], Iteration [2000/20000], Training Loss: 0.3734\n",
      "Epoch [13/15], Iteration [3000/20000], Training Loss: 0.4801\n",
      "Epoch [13/15], Iteration [4000/20000], Training Loss: 0.4094\n",
      "Epoch [13/15], Iteration [5000/20000], Training Loss: 0.4213\n",
      "Epoch [13/15], Iteration [6000/20000], Training Loss: 0.4144\n",
      "Epoch [13/15], Iteration [7000/20000], Training Loss: 0.7002\n",
      "Epoch [13/15], Iteration [8000/20000], Training Loss: 0.4069\n",
      "Epoch [13/15], Iteration [9000/20000], Training Loss: 0.3650\n",
      "Epoch [13/15], Iteration [10000/20000], Training Loss: 0.5535\n",
      "Epoch [13/15], Iteration [11000/20000], Training Loss: 0.5847\n",
      "Epoch [13/15], Iteration [12000/20000], Training Loss: 0.5318\n",
      "Epoch [13/15], Iteration [13000/20000], Training Loss: 0.6089\n",
      "Epoch [13/15], Iteration [14000/20000], Training Loss: 0.5007\n",
      "Epoch [13/15], Iteration [15000/20000], Training Loss: 0.4774\n",
      "Epoch [13/15], Iteration [16000/20000], Training Loss: 0.5209\n",
      "Epoch [13/15], Iteration [17000/20000], Training Loss: 0.6082\n",
      "Epoch [13/15], Iteration [18000/20000], Training Loss: 0.4264\n",
      "Epoch [13/15], Iteration [19000/20000], Training Loss: 0.4566\n",
      "Epoch [13/15], Iteration [20000/20000], Training Loss: 0.3198\n",
      "Epoch [13/15], Average Training Loss: 0.5014\n",
      "Epoch [14/15], Iteration [1000/20000], Training Loss: 0.5588\n",
      "Epoch [14/15], Iteration [2000/20000], Training Loss: 0.4486\n",
      "Epoch [14/15], Iteration [3000/20000], Training Loss: 0.4717\n",
      "Epoch [14/15], Iteration [4000/20000], Training Loss: 0.4801\n",
      "Epoch [14/15], Iteration [5000/20000], Training Loss: 0.4866\n",
      "Epoch [14/15], Iteration [6000/20000], Training Loss: 0.4507\n",
      "Epoch [14/15], Iteration [7000/20000], Training Loss: 0.5806\n",
      "Epoch [14/15], Iteration [8000/20000], Training Loss: 0.5797\n",
      "Epoch [14/15], Iteration [9000/20000], Training Loss: 0.4362\n",
      "Epoch [14/15], Iteration [10000/20000], Training Loss: 0.4900\n",
      "Epoch [14/15], Iteration [11000/20000], Training Loss: 0.5373\n",
      "Epoch [14/15], Iteration [12000/20000], Training Loss: 0.5111\n",
      "Epoch [14/15], Iteration [13000/20000], Training Loss: 0.5182\n",
      "Epoch [14/15], Iteration [14000/20000], Training Loss: 0.5020\n",
      "Epoch [14/15], Iteration [15000/20000], Training Loss: 0.4666\n",
      "Epoch [14/15], Iteration [16000/20000], Training Loss: 0.5561\n",
      "Epoch [14/15], Iteration [17000/20000], Training Loss: 0.4458\n",
      "Epoch [14/15], Iteration [18000/20000], Training Loss: 0.5014\n",
      "Epoch [14/15], Iteration [19000/20000], Training Loss: 0.5375\n",
      "Epoch [14/15], Iteration [20000/20000], Training Loss: 0.5328\n",
      "Epoch [14/15], Average Training Loss: 0.5014\n",
      "Epoch [15/15], Iteration [1000/20000], Training Loss: 0.4516\n",
      "Epoch [15/15], Iteration [2000/20000], Training Loss: 0.5372\n",
      "Epoch [15/15], Iteration [3000/20000], Training Loss: 0.5878\n",
      "Epoch [15/15], Iteration [4000/20000], Training Loss: 0.4225\n",
      "Epoch [15/15], Iteration [5000/20000], Training Loss: 0.4771\n",
      "Epoch [15/15], Iteration [6000/20000], Training Loss: 0.5528\n",
      "Epoch [15/15], Iteration [7000/20000], Training Loss: 0.5260\n",
      "Epoch [15/15], Iteration [8000/20000], Training Loss: 0.3917\n",
      "Epoch [15/15], Iteration [9000/20000], Training Loss: 0.4915\n",
      "Epoch [15/15], Iteration [10000/20000], Training Loss: 0.5435\n",
      "Epoch [15/15], Iteration [11000/20000], Training Loss: 0.5088\n",
      "Epoch [15/15], Iteration [12000/20000], Training Loss: 0.5003\n",
      "Epoch [15/15], Iteration [13000/20000], Training Loss: 0.5588\n",
      "Epoch [15/15], Iteration [14000/20000], Training Loss: 0.4661\n",
      "Epoch [15/15], Iteration [15000/20000], Training Loss: 0.5714\n",
      "Epoch [15/15], Iteration [16000/20000], Training Loss: 0.3932\n",
      "Epoch [15/15], Iteration [17000/20000], Training Loss: 0.5514\n",
      "Epoch [15/15], Iteration [18000/20000], Training Loss: 0.5053\n",
      "Epoch [15/15], Iteration [19000/20000], Training Loss: 0.4417\n",
      "Epoch [15/15], Iteration [20000/20000], Training Loss: 0.4731\n",
      "Epoch [15/15], Average Training Loss: 0.5010\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define your parameters\n",
    "input_size = 300  # Dimension for word embeddings\n",
    "hidden_size = 8\n",
    "output_size = 2  # Binary classification\n",
    "dropout_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "bidirectional = True  # Use bidirectional LSTM\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, bidirectional, dropout_prob=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)  # Multiply by 2 if bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(x)  # No embedding layer, directly use the input\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)  # Take the last hidden state\n",
    "        return output\n",
    "\n",
    "# Convert document_representations into PyTorch tensor\n",
    "document_tensor = torch.stack(document_representations)\n",
    "\n",
    "# Convert labels into PyTorch tensor\n",
    "labels_tensor = torch.tensor(y_train)\n",
    "\n",
    "# Create DataLoader object for training\n",
    "train_dataset = TensorDataset(document_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate your BiLSTM classifier model\n",
    "model = BiLSTMClassifier(input_size, hidden_size, output_size, num_layers, bidirectional, dropout_prob)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Iteration [{i}/{len(train_loader)}], Training Loss: {loss.item():.4f}')\n",
    "    \n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T15:43:06.593676Z",
     "iopub.status.busy": "2024-04-27T15:43:06.593260Z",
     "iopub.status.idle": "2024-04-27T15:43:15.660438Z",
     "shell.execute_reply": "2024-04-27T15:43:15.659452Z",
     "shell.execute_reply.started": "2024-04-27T15:43:06.593647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 78.01%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Step 5: Calculate the accuracy of the model on the test dataset\n",
    "print(f'Accuracy on test data: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T15:43:56.852546Z",
     "iopub.status.busy": "2024-04-27T15:43:56.851978Z",
     "iopub.status.idle": "2024-04-27T15:43:56.857912Z",
     "shell.execute_reply": "2024-04-27T15:43:56.857066Z",
     "shell.execute_reply.started": "2024-04-27T15:43:56.852518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'stacked_bilstm_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-27T15:42:07.297756Z",
     "iopub.status.idle": "2024-04-27T15:42:07.298442Z",
     "shell.execute_reply": "2024-04-27T15:42:07.298193Z",
     "shell.execute_reply.started": "2024-04-27T15:42:07.298166Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2477,
     "sourceId": 4140,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
